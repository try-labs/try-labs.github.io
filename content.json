{"meta":{"title":"Sam的个人博客","subtitle":"一个程序员的成长记录","description":"一个程序员的成长记录","author":"Sam","url":"https://ydstudios.gitee.io","root":"/"},"pages":[{"title":"","date":"2019-12-16T07:19:54.000Z","updated":"2021-02-28T12:49:30.000Z","comments":true,"path":"about/index.html","permalink":"https://ydstudios.gitee.io/about/index.html","excerpt":"","text":"我的个人信息 学历专业：本科 / 计算机科学与技术专业 英语水平：CET-4 其他证书：全国计算机等级考试四级网络工程师 微信：LY123456520 (加微信时请注明来意) 掘金：https://juejin.cn/user/465848660925805 个人爱好 爱看电影和美食。 喜欢阅读技术和历史方面的书籍。 技术视野开阔喜欢钻研，学习能力不错。 良好的设计和编码品味，对优雅的代码有一定的追求。 具有一定的沟通能力和高度的团队合作意识，能快速的融入团队。 折腾&#8195;从大三开始折腾一些免费的空间，搭建一个个人主页就开心的不得了。后来毕业了，市面上有很多开源的博客框架我没有用，自己用PHP从头做了一个博客程序，在godday上买域名、在Bandwagon、ConoHa等上买服务器，博客搞了一年多，最后由于工作太忙了就放弃了。&#8195;再后来大概17年的时候，看到同事的小网站搞的风生水起的，我又开始折腾了！那段时间我一连搞了一个电影网站、一个图片网站还有一个磁力链搜索网站，对应的也买了好几个域名！（现在都过期啦）那个时候很长一段时间都是在买服务器、爬数据或者就是在配置环境，因为买的都是国外的vps，很容易被墙！就这样搞了一年多，网站也被百度搜录了不少，可那个时候版权意识也越来越强，又由于工作上的变动，这些就又就放弃了！&#8195;可是17年的双十一，我在阿里云上趁着活动买了一个服务器，这样又搞起了我的博客。这回可是用了心💕了，重新买了一个ydstudio.net的域名，自己又把之前的暂住证找回来了，南京这边网站备案外地户口必须要暂住证，去拿暂住证又是请了几天假，在此不得不吐槽一下有些社区办事真混乱😄😄，博客用了PHP写的typecho框架，在centos上装好Nginx、MySQL、PHP、git等必备软件，把没有丢的文章发布上去，这么一弄博客还挺不错！😁😄😄阿里云续费真是贵呀，后面就只续费了一年。兜兜转转进入2019年，每日工作也是繁琐，突然觉得一个作为程序员，没有一个属于自己的博客还是差点意思！自己买域名服务器搞太费事了，没有以前那种折腾劲了，然后就发点时间找到了git page，最后又在gitee上也搞了一份，我想这个又不用自己发时间精力去维护，想起来的话就用markdown写写文章，两条命令就部署上去了，很easy呀！！！ &#8195;从开始自己写，到现在用git page,也总算是安定下来了，希望能一直坚持吧😄"},{"title":"友情链接","date":"2020-01-11T17:54:54.000Z","updated":"2020-01-11T10:25:22.000Z","comments":true,"path":"links/index.html","permalink":"https://ydstudios.gitee.io/links/index.html","excerpt":"","text":"heyangli.com 排名不分先后，欢迎访问(～￣▽￣)～ 友链申请要求： 博客可以被稳定访问，原则上只与原创技术类博客进行友链互换 页面无繁杂广告推广"},{"title":"文章标签","date":"2020-10-11T12:18:00.000Z","updated":"2020-10-11T04:27:26.000Z","comments":true,"path":"tags/index.html","permalink":"https://ydstudios.gitee.io/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"如何保证RocketMQ消息不丢失","slug":"如何保证RocketMQ消息不丢失","date":"2021-03-12T10:31:13.000Z","updated":"2021-03-14T15:26:59.000Z","comments":true,"path":"post/b6f085c7.html","link":"","permalink":"https://ydstudios.gitee.io/post/b6f085c7.html","excerpt":"前言随着业务的发展，系统功能也越来也复杂。渐渐的我们会遇到这样的问题。例如，系统和多个外部系统交互对接耗时耗力、系统某个模块流程处理时间过长业务越来越接受不了、某个模块在某个时间点流量太大系统时刻有宕机的风险等等。对于这些问题要想解决，系统此时引入消息队列或许是一个不错的选择。我们都知道消息队列有三大技能：解耦、异步、削峰。看起来可以完美的解决上面提到的问题，但是你们的系统真的适合消息队列么？","text":"前言随着业务的发展，系统功能也越来也复杂。渐渐的我们会遇到这样的问题。例如，系统和多个外部系统交互对接耗时耗力、系统某个模块流程处理时间过长业务越来越接受不了、某个模块在某个时间点流量太大系统时刻有宕机的风险等等。对于这些问题要想解决，系统此时引入消息队列或许是一个不错的选择。我们都知道消息队列有三大技能：解耦、异步、削峰。看起来可以完美的解决上面提到的问题，但是你们的系统真的适合消息队列么？ 系统在引入消息队列之后到底会带来哪些不好的影响，我们今天来说一说： 系统可用性降低，增加运维难度 系统引入的外部依赖越多，系统稳定性越差。一旦MQ宕机，就会对业务造成影响。以前A、B、C三个系统正常，服务就没有问题。现在A系统引入消息队列，B、C消费消息，现在消费队列宕机，业务就执行不下去了。如何保证MQ的高可用？ 系统复杂度提高 MQ的加入大大增加了系统的复杂度，以前系统间是同步的远程调用，现在是通过MQ进行异步调用。如何保证消息没有被重复消费？怎么处理消息丢失情况？如何保证顺序消息能被正确的消费？ 一致性问题 A系统处理完业务，通过MQ给B、C系统发消息数据，如果B系统、C系统处理失败。这样的情况要怎么处理，如何保证消息数据处理的一致性？ 看到了吧引入一个消息队列，就会带来这么的问题！所以大家在使用消息队列的时候，千万要想清楚，自己的系统适不适合、有没有那个运维能力、业务能不能接受！本文这次只谈如何让消息队列不丢失消息，且以RocketMQ为例。 一个消息到开始到结束会经历这么三个阶段：生产阶段、消息队列Broker存储阶段和消费阶段。一个消息在三个阶段中的任何一个阶段都有可能丢失，知道这个之后，我们只要保证这三个阶段不出现问题，消息自然就不会出现丢失了。接下来我们来细说一下如何保证这三个阶段不出现问题。 生产阶段生产阶段的使命就是将消息发送到队列之中。生产者（Producer）通过网络请求将消息发送给消息队列，消息队列接受到之后返回响应给生产者。RocketMQ有两种常用的消息发送方式：同步发送、异步发送。 同步发送123456789101112131415DefaultMQProducer producer = new DefaultMQProducer(\"unique_group_name\", true); producer.setNamesrvAddr(\"127.0.0.1:9876;127.0.0.1:9870\"); SimpleDateFormat format = new SimpleDateFormat(\"HH:mm:ss\"); String content = \"测试消息\" + format.format(new Date()); Message msg = new Message(\"TopicTest\", \"TagA\", UUID.randomUUID().toString(), content.getBytes(StandardCharsets.UTF_8)); try &#123; producer.start(); SendResult sendResult = producer.send(msg); log.info(\"MsgId= &#123;&#125;,结果= &#123;&#125; \", sendResult.getMsgId(), sendResult.getSendStatus()); &#125; catch (MQClientException | RemotingException | InterruptedException | MQBrokerException e) &#123; log.error(\"消息发送发生了错误[&#123;&#125;]\", msg, e); &#125; 同步发送时只要send()方法没有抛出异常，就可以认为消息发送成功，即消息队列Broker成功接受到了消息。 既然是同步发送肯定就比较耗费一些时间，如果你的业务比较注重RT那就可以使用异步发送的方式。 异步发送异步发送消息的方式可以降低消息发送的RT，我比较喜欢这种方式。 1234567891011121314151617181920212223DefaultMQProducer producer = new DefaultMQProducer(\"unique_group_name\", true); producer.setNamesrvAddr(\"127.0.0.1:9876;127.0.0.1:9870\"); // 消息自定义的唯一标识 String key = UUID.randomUUID().toString(); String content = \"消息发送测试\"; Message msg = new Message(\"TopicTest\", \"TagA\", key, content.getBytes(StandardCharsets.UTF_8)); try &#123; producer.start(); producer.send(msg, new SendCallback() &#123; @Override public void onSuccess(SendResult sendResult) &#123; log.info(\"根据消息[&#123;&#125;]的key[&#123;&#125;]更新消息[&#123;&#125;]的发送状态[&#123;&#125;]\", msg.getProperty(MessageConst.PROPERTY_KEYS), key, sendResult.getMsgId(), sendResult.getSendStatus()); &#125; @Override public void onException(Throwable e) &#123; log.error(\"发送出现错误[&#123;&#125;]\", msg, e); &#125; &#125;); &#125; catch (RemotingException | InterruptedException e) &#123; log.info(\"消息发送发生异常[&#123;&#125;]\", msg, e); &#125; 使用异步发送方式时记得重写SendCallback类的两个方法，在onSuccess()方法中更新消息的发送状态为发送成功,只要不发生异常且回调了onSuccess()方法也可以认为成功发送到了Broker。 SendStatus问题发送消息时，将获得包含SendStatus的SendResult。以下是每个状态的说明列表： SEND_OKSEND_OK并不意味着它是可靠的。要确保不会丢失任何消息，还应启用SYNC_MASTER或SYNC_FLUSH。 FLUSH_DISK_TIMEOUT如果Broker设置MessageStoreConfig的FlushDiskType = SYNC_FLUSH（默认为ASYNC_FLUSH），并且Broker没有在MessageStoreConfig的syncFlushTimeout（默认为5秒）内完成刷新磁盘，您将获得此状态。 FLUSH_SLAVE_TIMEOUT如果Broker的角色是SYNC_MASTER（默认为ASYNC_MASTER），并且从属Broker未在MessageStoreConfig的syncFlushTimeout（默认为5秒）内完成与主服务器的同步，则您将获得此状态。 SLAVE_NOT_AVAILABLE如果Broker的角色是SYNC_MASTER（默认为ASYNC_MASTER），但没有配置slave Broker，您将获得此状态。 对于SendStatus有多种情况的问题，因此无论使用同步还是异步的发送方式，都需要判断SendStatus是不是SEND_OK，如果不是则需要针对不同的情况进行分别处理。 FLUSH_DISK_TIMEOUT，FLUSH_SLAVE_TIMEOUT这两种情况说明消息落盘出现了异常，为了不丢失消息，我们可以稍等时间后重发消息。 SLAVE_NOT_AVAILABLE这种情况说明集群中的Slave不可用，重新发送是无用的，需要人工介入处理。 其实你查看RocketMQ的源码就会发现，不论是同步发送还是异步发送，都是可以针对不同的场景自定义重试次数的，而且很多方法还有内部重试机制。 Warn: this method has internal retry-mechanism, that is, internal implementation will retry{@link #retryTimesWhenSendFailed} times before claiming failure. As a result, multiple messages may potentiallydelivered to broker(s). It’s up to the application developers to resolve potential duplication issue. 源码默认的处理方式 123456789101112131415161718/** * Maximum number of retry to perform internally before claiming sending failure in synchronous mode. &lt;/p&gt; * * This may potentially cause message duplication which is up to application developers to resolve. */private int retryTimesWhenSendFailed = 2;/** * Maximum number of retry to perform internally before claiming sending failure in asynchronous mode. &lt;/p&gt; * * This may potentially cause message duplication which is up to application developers to resolve. */private int retryTimesWhenSendAsyncFailed = 2;/** * Indicate whether to retry another broker on sending failure internally. */private boolean retryAnotherBrokerWhenNotStoreOK = false; 自定义重试机制： 123producer.setRetryTimesWhenSendFailed(5);producer.setRetryAnotherBrokerWhenNotStoreOK(true);producer.setRetryTimesWhenSendAsyncFailed(5); 这里就要提下消息投递语义（message delivery semantic），简单的来说就是消息传递过程中的传递保证。主要分为三种： at most once：最多一次。消息可能丢失也可能被处理，但最多只会被处理一次。 at least once：至少一次。消息不会丢失，但可能被处理多次,可能重复，不会丢失。 exactly once：精确传递一次。消息被处理且只会被处理一次,不丢失不重复就一次。 有些异常情况的出现，可能是因为网络的偶尔波动导致，其实已经发送到了Broker，只不过是返回ACK给生产者的时候出现了超时，这个时候生产者重试就会导致消息重复投递。毕竟生产者为了保证消息一定成功投递到Broker中，就无法保证只进行一次精确投递。为了防止消息重复消费，那就需要消费者自身保证业务处理的幂等性。另外对于发送状态SendStatus 不是SEND_OK的消息要使用定时任务进行补偿发送。还要提到一点的就是，重试也是需要做好限制的，设定最大重试次数，也要保证重试的时间间隔，毕竟经验告诉我们，有些异常情况下短时间内的重试是没有意义的。具体的设计可以参考我之前文章中的本地消息表方案，本地事务+定时任务补偿保证消息一定投递成功。 消息队列Broker存储阶段默认的情况下，消息队列为了快速响应，在接受到生产者的请求，将消息保存在内存成功之后，就会立刻返回ACK响应给生产者。 你以为人家的架构是这样的： 消息刷盘方式 同步刷盘在返回写成功状态时，消息已经被写入磁盘。具体流程是，消息写入内存的PAGECACHE后，立刻通知刷盘线程刷盘， 然后等待刷盘完成，刷盘线程执行完成后唤醒等待的线程，返回消息写 成功的状态。 异步刷盘在返回写成功状态时，消息可能只是被写入了内存的PAGECACHE，写操作的返回快，吞吐量大；当内存里的消息量积累到一定程度时，统一触发写磁盘动作，快速写入。 将默认的异步刷盘修改成同步刷盘 1flushDiskType=SYNC_FLUSH 异步刷盘方式在遇到消息队列宕机、机器异常断电或者内存硬盘损坏的情况，消息就无法成功持久化到硬盘中，那这个消息就永久丢失了。对于这种情况，我们就需要改变RocketMQ的刷盘机制，将默认的异步刷盘，修改成同步刷盘。即消息成功保存到硬盘上时才返回给生产者ACK响应。 其实人家的架构是这样的： 同步刷盘的缺点很明显，那就是降低了吞吐量，加大了消息发送的响应RT时间，但是为了不丢失宝贵的消息这一点损耗是值得的。 集群部署上面讲的是单个消息队列Broker对于可靠保存消息的处理方式，但是生产环境肯定是采用的集群部署。目前RocketMQ支持单Master模式、多Master模式、多Master多Slave模式（异步）和多Master多Slave模式（同步）4种集群方式。 我这里申明一下，生产环境下的消息队列一定是采用集群的方式进行部署，不会有单机部署的情况。自己在本地搞搞单机部署玩玩肯定是可以的，生产环境也这么搞，你肯定是在逗我！ 单Master模式这种方式风险较大，一旦Broker重启或者宕机时，会导致整个服务不可用。不建议线上环境使用,可以用于本地测试。 多Master模式一个集群无Slave，全是Master，例如2个Master或者3个Master，这种模式的优缺点如下： 优点：配置简单，单个Master宕机或重启维护对应用无影响，在磁盘配置为RAID10时，即使机器宕机不可恢复情况下，由于RAID10磁盘非常可靠，消息也不会丢（异步刷盘丢失少量消息，同步刷盘一条不丢），性能最高； 缺点：单台机器宕机期间，这台机器上未被消费的消息在机器恢复之前不可订阅，消息实时性会受到影响。 多Master多Slave模式异步刷盘多Master多Slave模式每个Master配置一个Slave，有多对Master-Slave，HA采用异步复制方式，主备有短暂消息延迟（毫秒级），这种模式的优缺点如下： 优点：即使磁盘损坏，消息丢失的非常少，且消息实时性不会受影响，同时Master宕机后，消费者仍然可以从Slave消费，而且此过程对应用透明，不需要人工干预，性能同多Master模式几乎一样； 缺点：Master宕机，磁盘损坏情况下会丢失少量消息。 同步多Master多Slave模式每个Master配置一个Slave，有多对Master-Slave，HA采用同步双写方式，即只有主备都写成功，才向应用返回成功，这种模式的优缺点如下： 优点：数据与服务都无单点故障，Master宕机情况下，消息无延迟，服务可用性与数据可用性都非常高； 缺点：性能比异步复制模式略低（大约低10%左右），发送单个消息的RT会略高，且目前版本在主节点宕机后，备机不能自动切换为主机。 即使消息成功保存到了Master的硬盘上，然后在Master将消息同步给Slave的时候，这个期间Master挂了，而且是那种硬盘修不好的那种，不要说这种情况不可能，支付宝的专用电缆都能被挖断，还有啥不可能的。哈哈哈，也是够倒霉的！ 其实人家真正的架构是这样的: 四种集群方式优缺点都列出来了，很明显为了保证消息一定不会在Broker这个阶段丢失，生产环境一定要使用第四种集群方式：同步复制多Master多Slave模式。具体配置如下： 1234567## master 节点配置brokerRole=SYNC_MASTERflushDiskType=SYNC_FLUSH## slave 节点配置brokerRole=SLAVEflushDiskType=ASYNC_FLUSH 加上前面的同步刷盘的配置，这样生产者发送消息给Broker，Master使用同步刷盘方式将消息保存到硬盘上，保存成功之后使用同步复制的方式将消息复制到Slave上，slave保存成功之后，Broker才返回给生产者ACK。 消息堆积对于并发比较高的系统，如果下游的消费者宕机，则会导致大量的消息堆积在消息队列里，这样很容易会把服务器的硬盘撑爆，新的消息发送到消息队列，硬盘拒绝写入，这时消息很容易就会丢失。所以，部署消息队列的机器硬盘空间要比较充裕，且要有一定的监控，防止这种情况发生。 消费阶段终于到了最后一个阶段，但是大家也不能大意。消费者拉取消息进行本地业务处理，业务处理完成才能提交ACK ConsumeConcurrentlyStatus.CONSUME_SUCCESS，切不可先提交ACK再进行业务处理。如果业务处理出现异常情况，可以先返回ConsumeConcurrentlyStatus.RECONSUME_LATER等待消息队列的下次重试。 1234567891011121314151617DefaultMQPushConsumer consumer = new DefaultMQPushConsumer(\"unique_group_name\");consumer.subscribe(\"TopicTest\", \"*\");consumer.setConsumeFromWhere(ConsumeFromWhere.CONSUME_FROM_FIRST_OFFSET);consumer.setNamesrvAddr(\"127.0.0.1:9876;127.0.0.1:9870\");consumer.setConsumeMessageBatchMaxSize(1);consumer.registerMessageListener((MessageListenerConcurrently) (msgs, context) -&gt; &#123; MessageExt messageExt = msgs.get(0); // 进行业务处理 // 处理失败返回ConsumeConcurrentlyStatus.RECONSUME_LATER // 处理成功返回ConsumeConcurrentlyStatus.CONSUME_SUCCESS return ConsumeConcurrentlyStatus.CONSUME_SUCCESS;&#125;);consumer.start(); 还有一点要注意的是，消息队列 RocketMQ 默认允许每条消息最多重试 16 次，每次重试的间隔时间如下： 第几次重试 与上次重试的间隔时间 第几次重试 与上次重试的间隔时间 1 10 秒 9 7 分钟 2 30 秒 10 8 分钟 3 1 分钟 11 9 分钟 4 2 分钟 12 10 分钟 5 3 分钟 13 20 分钟 6 4 分钟 14 30 分钟 7 5 分钟 15 1 小时 8 6 分钟 16 2 小时 如果消息重试 16 次后仍然失败，消息将不再投递。如果严格按照上述重试时间间隔计算，某条消息在一直消费失败的前提下，将会在接下来的 4 小时 46 分钟之内进行 16 次重试，超过这个时间范围消息将不再重试投递。此时，消息队列 RocketMQ 不会立刻将消息丢弃，而是将其发送到该消费者对应的特殊队列中。在消息队列 RocketMQ 中，这种正常情况下无法被消费的消息称为死信消息（Dead-Letter Message），存储死信消息的特殊队列称为死信队列（Dead-Letter Queue）。死信队列里的消息有效期与正常消息相同，均为3天。3天后会被自动删除。针对这种情况，为了不丢失消息我们需要处理死信队列里的消息。 有消息进入死信队列，意味着某些问题导致消费者无法正常消费消息，因此，通常需要人工介入对其进行特殊处理。排查可疑因素并解决问题后，可以在消息队列 RocketMQ 控制台重新发送该消息让消费者重新消费一次，或者直接让专门的消费者订阅死信队列进行消费。 死信队列名称一般是 %DLQ% + ConsumerGroupName组成，还有个重试队列名称一般是 %RETRY% + ConsumerGroupName组成，这些都是RocketMQ自动创建的。 总结一个消息从新生到终结经历了生产、存储、消费三个阶段，针对不同的阶段可能会出现丢失消息的地方，我们给出不同的解决方案。最终，RocketMQ丢失消息的概率被大大的降低了。我们将视角拔高一点，你就会发现，解决不同的消息队列不丢失消息，只有消息队列的配置稍有不同，其他地方都是类似的。好像我们已经形成了解决消息不丢失的方法论了，再遇到其他的消息队列我们就不慌了。","categories":[],"tags":[{"name":"RocketMQ","slug":"RocketMQ","permalink":"https://ydstudios.gitee.io/tags/RocketMQ/"},{"name":"消息队列","slug":"消息队列","permalink":"https://ydstudios.gitee.io/tags/%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/"}]},{"title":"分布式锁实现方案到底有哪些","slug":"分布式锁实现方案到底有哪些","date":"2021-02-28T10:19:27.000Z","updated":"2021-03-13T07:09:52.000Z","comments":true,"path":"post/3c363844.html","link":"","permalink":"https://ydstudios.gitee.io/post/3c363844.html","excerpt":"前言前面几篇文章从系统架构方式的演进，谈到了单体架构、分布式架构的优缺点，其中特意说到了分布式架构由于功能的拆分，导致系统出现了不能保证数据一致性的问题，即分布式事务问题，从而引出了几种分布式事务的解决方案。文章的篇幅大都在说分布式事务，其实分布式系统中还有一个重要角色–分布式锁，一直没有提及。今天这篇文章我们就将光环移到分布式锁的身上，来说说分布式锁的那些事吧！","text":"前言前面几篇文章从系统架构方式的演进，谈到了单体架构、分布式架构的优缺点，其中特意说到了分布式架构由于功能的拆分，导致系统出现了不能保证数据一致性的问题，即分布式事务问题，从而引出了几种分布式事务的解决方案。文章的篇幅大都在说分布式事务，其实分布式系统中还有一个重要角色–分布式锁，一直没有提及。今天这篇文章我们就将光环移到分布式锁的身上，来说说分布式锁的那些事吧！ 什么是锁说分布式锁之前，我们先说一说什么是锁、锁有什么作用？一般来说当代码中会有多个线程任务访问同一共享资源时,就会出现冲突或者错误，目前解决这种问题都是采用序列化访问共享资源的解决方案,即将共享资源放在某一代码块中并加锁,某一时刻只能有一个线程访问该代码块。从而我们可以得出结论，锁是一种方案，一种解决多线程访问共享资源不出问题的解决方案。 为什么需要分布式锁(distributed lock)Java中提供了很多种类丰富的锁，每种锁因其特性的不同，可以在适当的场景下展现出非常高的效率，具体有哪些锁此处暂时也按下不表。Java中提供的锁，例如synchronized、ReentrantLock之类的锁，只能在单机JVM中使用，可以称之为单机锁。可在分布式系统环境下，一个功能可能会涉及到多个服务，不同的服务又部署在不同的服务器不同的JVM中，这样Java提供的单机锁就无能为力了。可是在分布式环境下，我们仍然需要有一个类似synchronized的锁，获得到锁的服务可以正确的访问共享资源。在分布式环境下能提供这样服务的我们称之为分布式锁。 锁的种类很丰富，例如乐观锁、悲观锁；公平锁、非公平锁；可重入锁、不可重入锁；共享锁、排他锁；本文只考虑分布式情况下的排他锁。 分布式系统专家Martin在他的文章中写了他对分布式锁的理解，以下便是节选（原文为英文此处翻译成了中文）： 锁的目的是确保在可能尝试执行同一工作的多个节点中，只有一个节点实际执行了该工作（一次至少只有一个节点）。这项工作可能是将一些数据写入共享存储系统，执行一些计算，调用一些外部API，等等。在较高的层次上，您可能希望在分布式应用程序中使用锁的原因有两个：一是为了效率，二是为了正确性。为了区分这些情况，您可以询问如果锁定失败会发生什么： 效率：使用锁可以避免不必要地重复两次相同的工作（例如，一些昂贵的计算）。如果锁定失败，并且两个节点最终执行相同的工作，结果是成本略有增加（您最终向AWS支付的费用比其他情况下多5美分）或带来轻微不便（例如，用户最终两次收到相同的电子邮件通知）。 正确性：使用锁可以防止并发进程互相攻击，从而破坏系统的状态。如果锁定失败，并且两个节点同时处理同一数据段，则会导致文件损坏、数据丢失、永久性不一致、患者服用的药物剂量错误或其他严重问题。 两者都是想要锁定的有效情况，但是您需要非常清楚要处理的是哪一种。 分布式锁有什么要求前面我们说到了分布式锁的使用场景，从中我们可以得出一些分布式锁的基础要求，我总结大概有以下三点： 互斥性在同一时间，锁只能被一个客户端的一个线程获得。 安全性只有获得锁的线程才能释放锁，并且要有超时机制，客户端崩溃之后要能自动释放锁，不能出现死锁。 可用性高性能的获得锁和释放锁，且不能有单点故障。 分布式锁的简单实现根据锁资源本身的安全性，我们可以将分布式锁分为两大类： 基于异步复制的中间件，如MySQL、Redis等等 基于paxos协议的分布式一致性的中间件，如zookeeper、etcd等等 数据库实现分布式锁首先说下结论，不建议使用基于数据库实现分布式锁。 实现思路使用MySQL的唯一索引加上行锁机制,由线程名threadName、类名className、方法名methodName三个字段创建唯一索引。 在要获得锁的时候向DB中写一条记录，这条记录主要包含主键、当前占用锁的线程名threadName、类名className、方法名methodName、锁的有效截止时间deadline和创建时间createTime等，如果插入成功表示当前线程获取到了锁，如果插入失败那么证明锁被其他客户端的线程获取。释放分布式锁的时候，客户端直接删除对应的记录即可。如果客户端崩溃了，不能自行删除对应的记录，则需要一个定时任务补偿，查询当前时间已经大于锁的有效截止时间deadline的记录，删除即可。 优缺点 优点 使用MySQL实现简单，MySQL的唯一索引加上行锁机制，可以保证在同一时刻只会有一个客户端的一个线程获得锁。 缺点 MySQL存在单点故障，并发量大的时候吞吐量急剧下降。如果要实现高可用的话，则需要搭建MySQL集群，如果采用异步复制的方式同步数据到slave，存在数据延迟或者数据丢失的风险，这样就会出现客户端已经在master数据库上获得到了锁，但数据未同步到slave时master宕机，集群高可用重新选举出新的master之后，客户端又可以获得同一把锁，这样的情况就不满足分布式锁的互斥性要求了。但是，如果MySQL集群采用半同步复制或者全同步复制到slave，则锁的性能、吞吐量又降低了很多。 锁失效机制实现的不够优雅，而且补偿任务删除数据会存在一定时间的延迟，延迟这段时间锁不可用。 Redis实现分布式锁Redis是开发中常用的中间件，后端的开发也基本上都使用过。Redis基本上都是内存操作，性能好、速度快。接下来我们来用Redis实现一个简版分布式锁。 Redis如何实现分布式锁我们用Redis的set命令实现加锁，用lua脚本来释放锁，用key的过期机制来保障分布式锁不会死锁。 可能有些头铁的老哥使用set加锁；使用expire命令设置key的有效期；使用del命令释放锁。别不相信，我还看过相关的文章，文章作者对不正确使用Redis实现分布式锁，导致出现商品出现超卖进行复盘。 实现加锁1set key value [EX seconds] [PX milliseconds] [NX|XX] 从 Redis 2.6.12 版本开始，Redis对set命令进行了一系列的升级。 SET 命令的行为可以通过一系列参数来修改： EX seconds：将键的过期时间设置为 seconds 秒。执行 SET key value EX seconds 的效果等同于执行 SETEX key seconds value 。 PX milliseconds：将键的过期时间设置为 milliseconds 毫秒。 执行 SET key value PX milliseconds 的效果等同于执行 PSETEX key milliseconds value 。 NX：只在键不存在时，才对键进行设置操作。执行 SET key value NX 的效果等同于执行 SETNX key value 。 XX：只在键已经存在时，才对键进行设置操作。 对于返回值的升级，从 Redis 2.6.12 版本开始， SET 命令只在设置操作成功完成时才返回 OK；如果命令使用了 NX 或者 XX 选项；但是因为条件没达到而造成设置操作未执行；那么命令将返回空(nil)。 我们来验证一下，执行两次以下命令： 1set bizLock randomValue EX 20 NX randomValue是由客户端生成的一个随机字符串（可以使用UUID生成），相当于是客户端持有锁的标志。NX 表示只有当 bizLock 对应的key值不存在的时候才能SET成功，相当于只有第一个请求的客户端才能获得锁。EX 20 表示这个锁有效期20秒。 确实如Redis的文档所说，SET 命令只在设置操作成功完成时才返回 OK；如果命令使用了 NX 或者 XX 选项；但是因为条件没达到而造成设置操作未执行；那么命令将返回空(nil)。这样一个命令就可以保证分布式锁的互斥性和安全性两个要求。 实现解锁前面说了有人直接使用del命令删除对应的key来实现解锁，这个是有问题的，很有可能误解锁。下面是一个分布式锁服务的流程图： 由于客户端A在成功获得分布式锁之后，自身发生GC导致出现STW(stop the world)现象或者是发生了网络问题，客户端的业务操作还没有完成，分布式锁的有效期已经到了，锁已经被释放，可是这个情况客户端A并不知晓。然后客户端B向分布式锁服务请求锁，获得锁成功之后，处理自己的业务逻辑。客户端B业务处理完成释放分布式锁，客户端A从GC停顿或者网络问题中恢复过来，继续自己的业务处理。这个过程存在两个问题： 假设客户端A在步骤5和6中间恢复过来，处理完业务使用del命令释放分布式锁，很明显他释放的是客户端B的锁，这个不符合分布式锁的安全性要求。 假设客户端A按照图中的流程一样，这样就意味着在同一时刻分布式系统中有两把一样的锁，导致业务处理重复出现错误数据。 问题1，那是因为判断是否拥有锁和解锁是两个步骤，不具有原子性。对于这个问题，我们可以使用lua脚本来解锁。Redis 2.6 版本通过内嵌支持 Lua 环境，会单线程原子性执行 Lua 脚本，保证 Lua 脚本在处理的过程中不会被任意其它请求打断。 lua解锁脚本如下： 12345if redis.call(\"get\",KEYS[1]) == ARGV[1] then return redis.call(\"del\",KEYS[1])else return 0end 问题2，客户端发生GC导致停顿过久获得的分布式锁失效，且造成系统中同一时间拥有两把分布式锁。对于这种问题，建议在最终提交业务逻辑之前，再检查一下自身是否还拥有锁，拥有的话提交不拥有的业务回滚。当然，这种方案只是尽可能的减少出错的可能性，并不能完全避免问题发生。另外就是建议，保证业务处理接口的幂等性。例如在数据库层面添加唯一索引，使用数据库进行兜底。 Redis实现分布式锁优缺点 优点 代码实现起来简单，加锁解锁效率高。 客户端崩溃或者网络延迟也可以自动清除锁，保证不会阻塞所有流程。 Redis的优异的性能，全内存操作，可以支持很高的并发。 缺点 使用Redis实现分布式锁自然是不能使用单个Redis，这样必然有单点故障，起码要使用集群的Redis，例如主从架构的Redis集群。Redis是使用异步复制的方式同步数据到slave，存在数据延迟或者数据丢失的风险，这样就会出现客户端已经在master上获得到了锁，但数据未同步到slave时master宕机，集群高可用重新选举出新的master之后，客户端又可以获得同一把锁，打破分布式锁的互斥性要求。 Redis服务器时钟漂移问题。简单的说，就是Redis服务器上的时间比正常的时钟快，导致锁提前过期释放。但客户端仍然认为自己持有锁，打破分布式锁的互斥性要求。 RedLock算法实现分布式锁假设我们有N个master节点，官方文档里将N设置成5，其实大等于3就行。 获取当前服务器时间。 轮流用相同的key和随机值在N个节点上请求锁，在这一步里，客户端在每个master上请求锁时，会有一个和总的锁释放时间相比小的多的超时时间。比如如果锁自动释放时间是10秒钟，那每个节点锁请求的超时时间可能是5-50毫秒的范围，这个可以防止一个客户端在某个宕掉的master节点上阻塞过长时间，如果一个master节点不可用了，我们应该尽快尝试下一个master节点。 客户端计算第二步中获取锁所花的时间，只有当客户端在大多数master节点上成功获取了锁（在这里是3个），而且总共消耗的时间不超过锁释放时间，这个锁就认为是获取成功了。 如果锁获取成功了，那现在锁自动释放时间就是最初的锁释放时间减去之前获取锁所消耗的时间。 如果锁获取失败了，不管是因为获取成功的锁不超过一半（N/2+1)还是因为总消耗时间超过了锁释放时间，客户端都会到每个master节点上释放锁，即便是那些他认为没有获取成功的锁。 这个地方说说关于集群相关知识点，节点发生宕机，剩余多少节点集群仍然可以正确使用？计算公式： 当前剩余节点数量 &gt; n/2 注意公式里大于 &gt; 号，不带等于 = 号，这个主要是为了最大程度上防止选举投票时出现票数相同，提高投票效率。 问题1: 多少个节点可以搭建成集群 1个节点能否搭建集群 1-1 ?&gt;0.5 (1是最少宕机1个节点,条件不成立),1个节点不能搭建集群 2个节点能否搭建集群 2-1 ?&gt;1 (条件不成立),2个节点不能搭建集群 3个节点能否搭建集群 3-1 ?&gt;1.5 (条件成立),3个节点是搭建集群的最小单位 4个节点能否搭建集群 4-1 ?&gt;2 (条件成立),4个节点能搭建集群 问题2: 为什么集群节点都是奇数,而不是偶数? 3个节点允许宕机的最大的数量是几个节点? 3-2 &gt;1.5条件不成立，3-1 &gt;1.5条件成立，因此3个节点最多允许宕机1个节点. 4个节点允许宕机的最大的数量是几个节点? 4-2 &gt;2条件不成立，4-1 &gt;2条件成立，因此4个节点最多允许宕机1个节点。 结论: 5个节点与6个节点搭建的集群最大都是容忍2个节点宕机,所以从成本的角度考虑,5个节点更节省资源。 Zookeeper实现分布式锁ZooKeeper是开放源码的分布式应用程序协调服务，是Google的Chubby一个开源的实现。基于它可以实现诸如数据发布/订阅、负载均衡、命名服务、分布式协调/通知、集群管理、Master 选举、分布式锁和分布式队列等功能。在这里我们就用它来实现以下分布式锁。 Zookeeper如何实现分布式锁Zookeeper实现分布式锁优缺点如何抉择[1]分布式系统专家Martin的《How to do distributed locking》","categories":[],"tags":[]},{"title":"孔乙己“茴”字四种写法引起我对策略模式实现的思考","slug":"孔乙己“茴”字四种写法引起我对策略模式实现的思考","date":"2021-02-23T20:29:36.000Z","updated":"2021-02-23T12:59:30.000Z","comments":true,"path":"post/d06ff354.html","link":"","permalink":"https://ydstudios.gitee.io/post/d06ff354.html","excerpt":"关于策略模式的文章，其实网上实在是太多了，自己的又没有啥文采，肯定也是写不出来什么花！但是我突然想起了孔乙己，对没错就是那个知道“茴”字有四种写法的孔乙己。当时，语文老师说这个场景是为了表现孔乙己的一种迂腐封建的性格。可是我对孔乙己的“茴”字有四种写法，一直有着不同的看法。我倒是觉得从某种意思上知道四种写法反倒是好事，比如你老板安排你做一件事情，你有四种不同的方案可以解决，这难道不是一件好事情嘛！于是，我还是觉得这篇文章还是有写的必要，让大家看看“茴”字有不同的写法。","text":"关于策略模式的文章，其实网上实在是太多了，自己的又没有啥文采，肯定也是写不出来什么花！但是我突然想起了孔乙己，对没错就是那个知道“茴”字有四种写法的孔乙己。当时，语文老师说这个场景是为了表现孔乙己的一种迂腐封建的性格。可是我对孔乙己的“茴”字有四种写法，一直有着不同的看法。我倒是觉得从某种意思上知道四种写法反倒是好事，比如你老板安排你做一件事情，你有四种不同的方案可以解决，这难道不是一件好事情嘛！于是，我还是觉得这篇文章还是有写的必要，让大家看看“茴”字有不同的写法。 假设目前有个抽奖的业务场景，奖品共有现金、优惠券、积分和谢谢参与四类，后续很大可能增加新的奖品类型如赠送抽奖次数。用户抽中则实时发送给用户。一般的小伙伴如何实现这个奖品发放的逻辑，我来写一下伪代码： 1234567891011public void sendReward(Byte rewardType, String reward) &#123; if (rewardType.equals(\"积分\")) &#123; log.info(\"发送积分奖励[&#123;&#125;]\", reward); &#125; else if (rewardType.equals(\"优惠券\")) &#123; log.info(\"发送优惠券奖励[&#123;&#125;]\", reward); &#125; else if (rewardType.equals(\"现金\")) &#123; log.info(\"发送现金奖励[&#123;&#125;]\", reward); &#125; else if (rewardType.equals(\"谢谢参与\")) &#123; log.info(\"对不起，谢谢参与[&#123;&#125;]\", reward); &#125; &#125; 大家看一看这种代码，是不是感觉似曾相似，心里是不是在想“这不就是我写的么”！写过这种代码也没有什么不好意思的，毕竟大部分人都是普通人不是那种一个顶十个的大牛，笔者之前也是经常写这种代码，毕竟那个时候也不了解什么是好代码、如何写出好代码，此处必须提大牛Martin Fowler(马丁.福勒)，他的著作《重构》大家可以看看。在此，我想强调的是过去写不出优秀的代码不重要，重要的是现在你要知道什么是优秀的代码和如何写出优秀的代码，不然你多年工作经验的沉淀在哪里呢？ 任何一个傻瓜都能写出计算机可以理解的代码。唯有写出人类容易理解的代码，才是优秀的程序员。—— Martin Fowler(马丁.福勒) 问题在哪里现在我们来说说这段伪代码到底有哪些不好的地方： NPE的问题。 在没有对入参进行校验的情况下，直接使用 rewardType.equals(&quot;积分&quot;)进行判断，若rewardType值是null，这个地方就会出现空指针异常。 硬编码问题。奖励类型的魔法值后期多处拷贝，有可能出现拼写错误，也无法很好展示到底有多少奖励类型。 过多的if else会导致嵌套过深和逻辑表达式复杂。 违反开闭原则（OCP）和单一职责原则（SRP）。需求明确提出后续可能会增加“赠送抽奖次数”的奖品类型，按照目前代码的结构要增加发送新型的奖品，则势必需要修改sendReward()代码，增加一个新的分支，这样就修改了原来的代码，就会需要对此处的代码进行回归测试。 如何改进根据上面提到的问题我们给出针对性的优化意见。 调换equals双方的位置 &quot;积分&quot;.equals(rewardType)或者使用java.util.Objects.equals()等等方法替代。 定义一个奖励类型枚举。 使用卫语句优化判断。 使用上面三个优化意见我们来修改伪代码，最终伪代码呈现如下： 定义一个奖励类型枚举 1234567891011121314151617181920212223242526272829@Getterpublic enum RewardTypeEnum &#123; /** * 现金奖励 */ CASH(\"1\",\"现金\"), /** * 积分 */ POINT(\"2\",\"积分\"), /** * 优惠券 */ COUPON(\"3\",\"优惠券\"), /** * 谢谢参与 */ THANK_YOU(\"4\",\"谢谢参与\"), ; private String code; private String desc; RewardTypeEnum(String code, String desc) &#123; this.code = code; this.desc = desc; &#125;&#125; 修改后的发送奖励伪代码： 123456789101112131415161718public void sendReward(String rewardType, String reward) &#123; if (RewardTypeEnum.POINT.getCode().equals(rewardType)) &#123; log.info(\"发送积分奖励[&#123;&#125;]\", reward); return; &#125; if (RewardTypeEnum.COUPON.getCode().equals(rewardType)) &#123; log.info(\"发送优惠券奖励[&#123;&#125;]\", reward); return; &#125; if (RewardTypeEnum.CASH.getCode().equals(rewardType)) &#123; log.info(\"发送现金奖励[&#123;&#125;]\", reward); return; &#125; if (RewardTypeEnum.THANK_YOU.getCode().equals(rewardType)) &#123; log.info(\"对不起，谢谢参与[&#123;&#125;]\", reward); return; &#125; &#125; 对于问题4的解决，我们先需要思考一下。奖励的种类目前有四种，后续很有可能增加奖励种类，不同的奖励发放给用户肯定是涉及了不同的模块，调用不同的发放接口，最终的数据肯定也是写到不同的数据表中，这种情况明显是不同奖励有着不同的发放策略，很明显大家都能想到符合策略模式，本文的主角终于出场了！ 策略模式属于设计模式中行为模式的一种，它主要是将算法封装起来，并且可以相互的替换。在看一些关于设计模式的书时，策略模式首先会定义一个接口，然后具体的策略类去实现策略接口，最后再定义一个Context上下文类来持有一个具体的策略以供调用者使用。这里其实有个问题好多书籍或者博客并没有说明，如何更优雅的让上下文类来持有一个具体的策略，这里我先搭建一下代码的框架，这个问题后面再细说。 定义策略接口123456789public interface RewardSendStrategy &#123; /** * 发送奖励 * @param memberId 会员id */ void sendReward(Long memberId);&#125; 发送优惠券的策略123456789@Slf4j@Servicepublic class CouponRewardSendStrategy implements RewardSendStrategy &#123; @Override public void sendReward(Long memberId) &#123; log.info(\"给[&#123;&#125;]发送优惠券奖品[&#123;&#125;]\", memberId); &#125;&#125; 发送积分的策略123456789@Slf4j@Servicepublic class PointRewardSendStrategy implements RewardSendStrategy &#123; @Override public void sendReward(Long memberId) &#123; log.info(\"给[&#123;&#125;]发送积分奖品[&#123;&#125;]\", memberId); &#125;&#125; 谢谢参与的策略123456789@Slf4j@Servicepublic class ThankYouRewardSendStrategy implements RewardSendStrategy &#123; @Override public void sendReward(Long memberId) &#123; log.info(\"[&#123;&#125;]，对不起，谢谢参与\", memberId); &#125;&#125; 发送现金策略123456789@Slf4j@Servicepublic class CashRewardSendStrategy implements RewardSendStrategy &#123; @Override public void sendReward(Long memberId) &#123; log.info(\"给[&#123;&#125;]发送现金奖品\", memberId); &#125;&#125; 定义Context上下文类123456789101112131415public class RewardSendStrategyContext &#123; /** * Context持有具体策略 */ private RewardSendStrategy sendStrategy; public void setSendStrategy(RewardSendStrategy sendStrategy) &#123; this.sendStrategy = sendStrategy; &#125; public void excute(Long memberId) &#123; sendStrategy.sendReward(memberId); &#125;&#125; 单元测试，试试如何使用1234567891011@ContextConfiguration(locations = &#123;\"classpath:spring/spring-dao.xml\", \"classpath:spring/spring-service.xml\"&#125;)@RunWith(value = SpringJUnit4ClassRunner.class)public class StrategyTest &#123; @Test public void test() &#123; RewardSendStrategyContext context = new RewardSendStrategyContext(); context.setSendStrategy(new CouponRewardSendStrategy()); context.excute(11L); &#125;&#125; 单元测试的执行结果不用怀疑,肯定打印出了 给[11]发送优惠券奖品，但是这种调用方式我感觉并没有解决掉if else多级逻辑判断，为什么这么说，因为这个地方我知道我调用的是发放优惠券的策略，但是在实际业务调用中，我并不知道要发送什么类型奖品，我还是要根据传入的奖品类型去判断使用不同的奖品发送策略。 123456789101112131415161718192021222324public void test2() &#123; String rewardType = RewardTypeEnum.THANK_YOU.getCode(); RewardSendStrategyContext strategyContext = new RewardSendStrategyContext(); if (RewardTypeEnum.POINT.getCode().equals(rewardType)) &#123; strategyContext.setSendStrategy(new PointRewardSendStrategy()); strategyContext.excute(11L); return; &#125; if (RewardTypeEnum.COUPON.getCode().equals(rewardType)) &#123; strategyContext.setSendStrategy(new CouponRewardSendStrategy()); strategyContext.excute(11L); return; &#125; if (RewardTypeEnum.CASH.getCode().equals(rewardType)) &#123; strategyContext.setSendStrategy(new CashRewardSendStrategy()); strategyContext.excute(11L); return; &#125; if (RewardTypeEnum.THANK_YOU.getCode().equals(rewardType)) &#123; strategyContext.setSendStrategy(new ThankYouRewardSendStrategy()); strategyContext.excute(11L); return; &#125; &#125; 按照我们前面给的建议进行优化，代码貌似又回到了起点。上面的代码明显还是不符合开闭原则和单一原则，看来我们还是需要进一步的优化。 使用Spring框架的特性改进前面我们已经把发送不同奖品的逻辑分散到了各个单独的类中，这些操作符合单一职责原则。但是没有统一入口，导致在调用策略的地方还是出现了大片的if判断，使得代码还是没有满足开闭原则。所以接下来我们要做的就是干掉这些if判断，提供一个易用的调用接口。 那怎么样我们才能减少或者直接消除上述代码中存在的 if 判断呢？回想之前的一篇文章船新版本的策略模式，你一定没有见过 ，在这个文章中我用注解标识每个策略方法，以注解的值为key，策略方法为value保存在Map中，然后通过Map的 get()获取具体的策略方法，省去了 if 判断。这个其实就是表驱动法，是一种很经典的编程技巧。那在这里就继续用这种方式来解决我们目前遇到的问题。当然此时我们不需要自定义注解，也不需要反射调用，不需要那么大费周章。Spring是一个很优秀的Java框架，一般做Java开发都是基于Spring框架的，Spring框架里有很多特性、技巧能帮助我们写出更优秀的代码。接下来我就展示一下如何使用Spring框架让我们的代码质量更上一个层次。 在此之前我需要修改一下之前的接口，我们在接口中再定义两个方法，其中一个方法是标识当前类的奖励类型。 1234567891011121314151617181920public interface RewardSendStrategy &#123; /** * 发放奖励的类型,通过这个方法来标示不同的策略 * @return */ String type(); /** * 是否匹配 * @param type 奖励类型 * @return */ boolean isTypeMatch(String type); /** * 发送奖励 * @param memberId 会员id */ void sendReward(Long memberId);&#125; 按照这种接口定义，我们在相应的策略类里实现相关方法，这里只给出策略类一个作为示范。 12345678910111213141516171819@Slf4j@Servicepublic class CouponRewardSendStrategy implements RewardSendStrategy &#123; @Override public String type() &#123; return RewardTypeEnum.COUPON.getCode(); &#125; @Override public boolean isTypeMatch(String type) &#123; return Objects.equals(type, type()); &#125; @Override public void sendReward(Long memberId) &#123; log.info(\"给[&#123;&#125;]发送优惠券奖品\", memberId); &#125;&#125; 好了，准备工作做的差不多了，接下来来显示Spring框架的威力吧！让它来帮组装我们需要的Map！ 方案一@Autowired注解这个注解大家肯定用过很多次了，我们常用它来注入其他的Bean，但是大家可能还有人不知道它有这种技能： 1234* &lt;p&gt;In case of a &#123;@link java.util.Collection&#125; or &#123;@link java.util.Map&#125;* dependency type, the container will autowire all beans matching the* declared value type. In case of a Map, the keys must be declared as* type String and will be resolved to the corresponding bean names. 这个是@Autowired注解源码上的注释，大致意思我举例说明 1234567# 这样定义属性的时候，Spring 会自动的将这个接口的实现类bean全都自动添加到这个 rewardSendStrategyList 中@Autowiredprivate List&lt;RewardSendStrategy&gt; rewardSendStrategyList;# 这样定义属性的时候，Spring 会自动的以实现类 beanName 作为 key，bean 作为 value 添加到这个 Map 中@Autowiredprivate Map&lt;String, RewardSendStrategy&gt; strategyMap; 666，就差一点我就要给你打一百分了，借此特性我们可以很容易的实现我们的Map，之前的Context类我们稍加修改,再借助一下Lamda表达式，我们得到了如下的代码： 12345678910111213141516171819@Slf4j@Componentpublic class RewardSendStrategyFactory &#123; @Autowired private List&lt;RewardSendStrategy&gt; strategyList; /** * 获取策略实例 * @param type * @return */ public RewardSendStrategy getImpl(String type) &#123; return strategyList.stream() .filter(strategy -&gt; strategy.isTypeMatch(type)) .findAny() .orElseThrow(() -&gt; new RuntimeException(\"没有找到策略实现\")); &#125; &#125; 这样一来我们调用就方便多了，而且很丝滑哦！ 12345678910111213@ContextConfiguration(locations = &#123;\"classpath:spring/spring-dao.xml\", \"classpath:spring/spring-service.xml\"&#125;)@RunWith(value = SpringJUnit4ClassRunner.class)public class StrategyTest &#123; @Autowired private RewardSendStrategyFactory factory; @Test public void test() &#123; RewardSendStrategy strategy = factory.getImpl(RewardTypeEnum.POINT.getCode()); strategy.sendReward(11L); &#125;&#125; 看着单元测试输出的结果，我感觉终于找到了完美实现策略模式的方案了。哈哈，其实还有好几种，这种方式我觉得是最简便的，也是我最常用的。所以我写在最前面，后面两种让我娓娓道来。 方案二使用Spring框架的ApplicationContextAware扩展点实现org.springframework.context.ApplicationContextAware接口，可以拿到ApplicationContext也就意味着可以拿到Spring容器中的所有Bean，既然拿到了所有的Bean，那我们就可以遍历其中的Bean，将是我们需要的放入到Map中。Context类进行修改，类名我也改成了RewardSendStrategyFactory,具体代码如下： 123456789101112131415161718192021222324252627282930@Slf4j@Componentpublic class RewardSendStrategyFactory implements ApplicationContextAware &#123; /** * 保存所有策略 */ private final static Map&lt;String, RewardSendStrategy&gt; STRATEGY_MAP = new LinkedHashMap&lt;&gt;(); @Override public void setApplicationContext(ApplicationContext applicationContext) throws BeansException &#123; Map&lt;String, RewardSendStrategy&gt; beans = applicationContext.getBeansOfType(RewardSendStrategy.class); if (MapUtils.isEmpty(beans)) &#123; return; &#125; beans.values().forEach(bean -&gt; STRATEGY_MAP.put(bean.type(), bean)); &#125; /** * 获取策略实例 * * @param type 奖励类型 * @return */ public static RewardSendStrategy getStrategyInstance(String type) &#123; log.info(\"策略实例[&#123;&#125;]\", STRATEGY_MAP); return STRATEGY_MAP.get(type); &#125;&#125; 调用方如何使用： 123456789101112@ContextConfiguration(locations = &#123;\"classpath:spring/spring-dao.xml\",\"classpath:spring/spring-service.xml\"&#125;)@RunWith(value = SpringJUnit4ClassRunner.class)public class StrategyTest &#123; @Test public void test()&#123; String code = RewardTypeEnum.CASH.getCode(); RewardSendStrategy strategyInstance = RewardSendStrategyFactory2.getStrategyInstance(code); strategyInstance.sendReward(12L); &#125;&#125; 方案三使用Spring框架的InitializingBean扩展点具体策略要实现org.springframework.beans.factory.InitializingBean接口。凡是将类交给Spring管理并且实现该接口，Spring在初始化bean的时候会自动执行afterPropertiesSet()方法，此时我们将这个Bean放入一个Map即可。这里我就只展示一个具体策略了，其他都是差不多的，具体的代码如下： 1234567891011121314151617181920212223@Slf4j@Servicepublic class PointRewardSendStrategy implements RewardSendStrategy, InitializingBean &#123; @Override public String type() &#123; return RewardTypeEnum.POINT.getCode(); &#125; @Override public boolean isTypeMatch(String type) &#123; return Objects.equals(type, type()); &#125; @Override public void sendReward(Long memberId) &#123; log.info(\"给[&#123;&#125;]发送积分奖品\", memberId); &#125; @Override public void afterPropertiesSet() throws Exception &#123; RewardSendStrategyFactory.registerStrategyInstance(type(), this); &#125;&#125; 细心的小伙伴可能发现了一些问题，如果每个具体策略类都实现 InitializingBean 接口，那么重写的afterPropertiesSet()方法明显是重复代码块，当然这个问题也是可以解决的，这里先按下不表！（不然下篇水文我怎么写） 相应的Context上下文类代码修改如下： 123456789101112131415161718192021222324252627282930@Slf4j@Componentpublic class RewardSendStrategyFactory &#123; /** * 保存策略集合 */ private final static Map&lt;String, RewardSendStrategy&gt; STRATEGY_MAP = new ConcurrentHashMap&lt;&gt;(16); /** * 添加策略实例 * * @param type * @param strategy */ public static void registerStrategyInstance(String type, RewardSendStrategy strategy) &#123; STRATEGY_MAP.put(type, strategy); &#125; /** * 获取策略实例 * @param type * @return */ public static RewardSendStrategy getStrategyInstance(String type) &#123; log.info(\"策略实例[&#123;&#125;]\", STRATEGY_MAP); return STRATEGY_MAP.get(type); &#125;&#125; 调用方式如单元测试所示： 123456@Test public void test() &#123; String rewardType = RewardTypeEnum.POINT.getCode(); RewardSendStrategy strategy = RewardSendStrategyFactory.getStrategyInstance(rewardType); strategy.sendReward(12L); &#125; 其他方案方案其实还有很多，例如前文中也说到了基于注解的形式，Spring容器启动的时候扫描指定的注解。这个看自己的想法了，只要符合优质代码的标准，而且能实现功能又没有bug，那都是顶呱呱的方案。 总结一千个观众眼中有一千个哈姆雷特。每个人对待任何事物都有自己的看法，一千人就有可能有一千种想法。有些事情看上去是件坏事，可换个角度去解读，反而可能会得到一种不一样思想。 善恶本有人相、我相、众生相,即是文化。","categories":[],"tags":[{"name":"设计模式","slug":"设计模式","permalink":"https://ydstudios.gitee.io/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"},{"name":"思考","slug":"思考","permalink":"https://ydstudios.gitee.io/tags/%E6%80%9D%E8%80%83/"},{"name":"策略模式","slug":"策略模式","permalink":"https://ydstudios.gitee.io/tags/%E7%AD%96%E7%95%A5%E6%A8%A1%E5%BC%8F/"}]},{"title":"分布式事务解决方案到底有哪些","slug":"分布式事务解决方案到底有哪些","date":"2021-02-16T19:58:33.000Z","updated":"2021-02-21T14:51:43.000Z","comments":true,"path":"post/9d74cc19.html","link":"","permalink":"https://ydstudios.gitee.io/post/9d74cc19.html","excerpt":"在系统是单体架构时，系统是和单个数据库进行交互，所以如果有多表操作的时候，可以使用数据库的事务实现数据的一致性，这种事务可以称之为本地事务。随着业务的发展，系统的压力越来越大，单体数据库的性能也达到了瓶颈，不可避免的进行数据库的拆分，还有系统模块的拆分，跨服务、跨数据库的事务场景就越来越多，这样解决分布式的事务的需求就出现了。","text":"在系统是单体架构时，系统是和单个数据库进行交互，所以如果有多表操作的时候，可以使用数据库的事务实现数据的一致性，这种事务可以称之为本地事务。随着业务的发展，系统的压力越来越大，单体数据库的性能也达到了瓶颈，不可避免的进行数据库的拆分，还有系统模块的拆分，跨服务、跨数据库的事务场景就越来越多，这样解决分布式的事务的需求就出现了。 有需求就要解决啊！我们程序员就是用来解决问题，实现需求的。之前的文章已经说了，目前已经存在好多种解决分布式事务的方案了，今天我们来说说其中几种比较有代表性的方案。 两阶段提交协议（2PC）两阶段提交协议（2PC，two phase commit protcol),是基于数据库资源层面的。把分布式事务分为两个阶段，一个是准备阶段，另外一个是提交阶段准备阶段和提交阶段都是由事务管理器（也称作协调者）发起，还有一个角色就是参与者。两阶段提交协议的流程如下： 准备阶段：协调者向参与者发起指令，参与者评估自己的资源，如果参与者评估指令能完成，则会写redo、undo日志，然后锁定资源、执行操作但是不提交。 提交阶段：如果每个参与者都明确的返回成功，也就是意味着资源锁定、执行操作成功，则协调者向各个参与者发起提交指令，参与者提交操作、释放锁定资源；如果有参与者在上述的两个步骤中有明确返回失败，也就是说资源锁定或者执行操作失败，则协调者向各个参与者发布中止指令，参与者执行undo日志，释放锁定的资源。 在最开始的准备阶段就锁定资源，这是一个重量级的操作，可以保证强一致性。但是实现起来却有很多的缺点： 阻塞，没有超时机制。如果在整个流程中，任何一个参与者或者协调者由于可能的网络延迟问题，导致协调者的指令不能发出或者参与者接受不到指令，整个流程也不能继续下去，且资源一直被锁定。 协调者有单点问题。协调者发出指令之后宕机，整个流程无法继续进行。 会有数据不一致的问题。协调者发出提交指令，部分参与者接受到了指令，执行了所有操作，但是有参与者宕机了，无法执行提交操作。 两阶段提交协议成功场景的示意图 三阶段提交协议（3PC）针对两阶段提交协议（2PC）的缺点进行改进提出三阶段提交协议（3PC，three phase commit protcol），也是基于数据库资源层面的。增加了一个询问的阶段，增加协调者、参与者超时机制，一旦发生超时则默认提交事务，其他的步骤就和2PC相同。 询问阶段：协调者想参与者询问能否完成指令，参与者只需要回答是和否，无需做其他的操作。 准备阶段：协调者向参与者发起指令，参与者评估自己的资源，如果参与者评估指令能完成，则会写redo、undo日志，然后锁定资源、执行操作但是不提交。如果有参与者在询问阶段回答否，则协调者向参与者发送中止请求。 提交阶段：如果每个参与者都明确的返回成功，也就是意味着资源锁定、执行操作成功，则协调者向各个参与者发起提交指令，参与者提交操作、释放锁定资源；如果有参与者在上述的两个步骤中有明确返回失败，也就是说资源锁定或者执行操作失败，则协调者向各个参与者发布中止指令，参与者执行undo日志，释放锁定的资源。 三阶段提交协议增加了询问阶段，这样可以确保尽可能早的发现参与者无法进行准备操作，但是不能完全避免这种情况，增加了超时机制可以减少资源的锁定时间。但是仍然会有数据不一致的问题。假设在提交阶段，协调者发出中止命令，由于发生网络分区等问题，部分参与者没有接受到命令则按照超时默认提交事务的规则，导致部分参与者回滚了事务，部分参与者提交了事务，数据一致性被破坏。（其实你就是超时默认中止操作还是会发生数据不一致的情况，真是太难啦😄） 三阶段提交协议成功场景流程图 TCC（Try-Confirm-Cancel）其实大家看了2PC和3PC的执行流程，就可以感觉到他们的实现，会出现资源阻塞、数据不一致的问题，性能效率也不高。在实际项目中也少有使用2PC和3PC实现分布式事务。后来又有大神提出TCC（Try-Confirm-Cancel）协议，协议是基于业务层面实现。这个协议将任务拆分成Try、Confirm、Cancel三个阶段,每个阶段都要保证各自操作幂等。 Try（预处理阶段）：参与者完成所有业务检查（一致性），预留业务资源(准隔离性)，所有参与者预留都成功，try阶段才算成功。此阶段仅是一个初步操作，它和后续的Confirm 一起才能真正构成一个完整的业务逻辑。 这个预留就是说用户在下订单使用了50积分抵扣金额，我们给积分增加一种冻结的状态，直接把使用的50积分状态置为冻结状态，在订单未完成支付之前用户查看自己的总积分没有减少，但是可用来支付的积分少了50，这样就不会一直占用资源，更新完50积分的状态就是释放了资源。 Confirm（确认阶段）：确认执行业务操作，不做任何业务检查，只使用Try阶段预留的业务资源。通常情况下，采用TCC则认为 Confirm阶段是不会出错的。即：只要Try成功，Confirm一定成功。若Confirm阶段真的出错了，需引入重试机制或人工处理。 Cancel（取消阶段）：取消Try阶段预留的业务资源。如果某个业务资源没有预留成功，则取消所有业务资源预留请求。通常情况下，采用TCC则认为Cancel阶段也是一定成功的。若Cancel阶段真的出错了，需引入重试机制或人工处理。 TCC示意图 2PC(3PC)是追求的数据的强一致性，是一种强一致性事务，而TCC在Confirm、Cancel阶段允许重试，这就意味着数据在一段时间内一致性被破坏，TCC符合BASE理论则可称作一种柔性事务。 如果拿TCC事务的处理流程与2PC两阶段提交做比较，2PC通常都是在跨库的DB层面实现，而TCC在应用层处理，通过业务逻辑来实现。这种分布式事务的实现方式的优势可以让应用自己定义数据操作的粒度，使得降低资源锁冲突、提高系统吞吐量成为可能。TCC的不足之处则在于对应用的代码侵入性非常强，业务逻辑的每个分支都需要实现try、confirm、cancel三个操作，增大了开发的成本。 目前业界已经有很多开源的TCC协议的分布式事务框架，例如Hmily、ByteTCC、TCC-transaction。使用这些框架就可以很大程度上节约时间，将更多的时间和注意力放到到具体业务中。 可靠消息最终一致性方案可靠消息方案通过消息生产、消息存储、消息投递三个阶段的可靠性，实现最终数据一致性，这种方案又有两种实现方式，一种是基于本地消息表实现，另外一种是基于事务消息实现。其实这种本地消息表和消息队列方案不冲突，因为消息队列的消息能够100%投递不丢失也可以用本地消息表实现。 本地消息表方案通过本地消息表实现分布式事务，我觉得是一种最简单、最简便的实现方式。它的核心思想就是讲分布式事务拆分成本地事务进行处理，用数据库的事务ACID特性保证数据一致性。还有再提一句这种实现思想是eBay里的大牛提出来的，前文说到的BASE理论也是同一家公司的人提出的，不得不赞叹eBay公司里大牛可真是多啊！ 简版本地消息表方案在这里我解释我所说的本地消息表方案中，是可以没有消息队列中间件的，这个可能和网上很多的说法不一样。因为考虑实际的情况，两个系统之间交互如果不存在高并发、大流量，以后也不会出现太多的业务耦合，引入消息中间件就会太过了，不仅提升了系统的复杂度，也增加了额外的中间件维护成本，降低了系统的可用性。所以解决这种情况最简单的方式就是使用http通信、异常用定时任务补偿即可。 这里我先举例说明我上面说的最简单的方式，之前我们的系统要从飞猪上进行会员引流，其中有这样一个需求，如果一个用户是我们系统的会员同时也是飞猪的会员，若这个用户在我们系统的等级发生了改变，则通知飞猪此用户的等级发生变动，然后用户在飞猪APP上查看会员信息的时候，飞猪重新调用一下我们系统的会员信息查询接口。你说对于这种需求用消息队列实现是不是有点大材小用了，最终的开发方案如下： 业务本地消息表设计 1234567891011121314151617181920CREATE TABLE `biz_local_message` ( `id` bigint(20) unsigned NOT NULL COMMENT '主键', `biz_module` tinyint(4) unsigned NOT NULL COMMENT '业务模块 1订单 2支付 4等等', `biz_no` char(64) NOT NULL COMMENT '业务单号，唯一标识', `biz_type` tinyint(4) unsigned NOT NULL COMMENT '业务类型', `msg` varchar(512) NOT NULL DEFAULT '' COMMENT '消息内容', `msg_desc` varchar(64) DEFAULT NULL COMMENT '消息描述', `backoff_second` int(10) unsigned NOT NULL DEFAULT '180' COMMENT '退避秒数，默认3分钟，用来计算next_handle_time时间', `handle_count` tinyint(4) NOT NULL DEFAULT '0' COMMENT '已经尝试处理次数', `max_handle_count` tinyint(4) NOT NULL DEFAULT '5' COMMENT '最大尝试处理次数', `next_handle_time` datetime DEFAULT NULL COMMENT '下一次处理时间', `handle_status` tinyint(4) unsigned NOT NULL DEFAULT '1' COMMENT '记录处理状态 1待处理 2处理中 4处理成功 8处理失败', `create_time` datetime NOT NULL COMMENT '创建时间', `create_user` varchar(32) NOT NULL DEFAULT 'admin' COMMENT '创建人', `update_time` datetime NOT NULL COMMENT '更新时间', `update_user` varchar(32) NOT NULL DEFAULT 'admin' COMMENT '更新人', PRIMARY KEY (`id`), UNIQUE KEY `uk_bizNoBizMoudleBizType` (`biz_no`,`biz_module`,`biz_type`) USING BTREE, KEY `idx_nextHandleTime` (`next_handle_time`)) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COMMENT='业务本地消息' 保证biz_local_message表数据跟业务流程在一个事务中，一起成功写入数据库。 这里的业务模块biz_module就是会员，业务单号biz_no可以用uuid，消息内容就是要推送的消息json保存。开启一个异步线程进行消息推送，推送前将消息的状态handle_status从1待处理设置成2处理中、处理次数 handle_count 值加1、由backoff_second字段和当前时间计算出下次推送时间设置next_handle_time字段，SQL更新记录影响行数affectRow 返回1才继续处理，更新失败则直接return。 定时任务补偿，处理出现推送异常的消息。定时任务的执行频率可以根据自己的业务需要自行设定,我这里当时设定的是每5分钟执行一次。 总体上是先进行异常处理，然后再处理异步线程可能没有推送的消息。 第一步将 where handle_status=2 AND handle_count = max_handle_count AND next_handle_time &lt;= now() 的记录状态handle_status更新为8处理失败 、next_handle_time字段清空，然后进入人工处理流程。 第二步将 where handle_status=2 AND handle_count &lt; max_handle_count AND next_handle_time &lt;= now() 则直接把这些记录的 handle_status 状态更新成 1待处理 、next_handle_time字段清空。 第三步将 where handle_status= 1 的记录分页处理。 处理前将消息的状态从1待处理 更新成 2处理中、处理次数 handle_count 值加1、由backoff_second字段和当前时间计算出下次推送时间设置next_handle_time字段、更新update_time字段，SQL更新记录影响行数affectRow 返回1才继续处理，更新失败则处理下一条记录。 前一步更新成功则继续处理；请求返回成功将记录的的状态由2处理中更新成4处理成功、清空 next_handle_time 字段，更新 update_time字段 。如果请求返回失败则更新将满足 handle_status=&#39;2&#39; AND handle_count &lt; max_handle_count 的记录更新 handle_status=1 、update_time字段，记录达到最大处理次数的将记录handle_status更新成 8处理失败，清空 next_handle_time 字段，更新update_time字段，然后进入人工处理流程。 在被调用的服务发生异常或者网络问题，短时间内的频繁重试所得到的结果也大致都是失败，这样的重试不仅没有效果，反而还会增服务的负担。所以在计算下一次处理时间next_handle_time除了加上backoff_second退避秒数之后也可以加上随机数，或者更高大上一点使用退避算法来计算。 简版本地消息表方案流程图 实际本地消息表方案上面那种是简版的本地消息表方案，但是采用分布式架构的系统由于业务解耦、高并发大流量异步削峰等需要会引入消息队列，则消息就会直接发送到消息队列MQ中，不再通过http请求的方式进行通讯，所以方案的流程就会和上面的不一样，具体流程如下图： 由于消息队列（我这里以RocketMQ为例，若使用的是其他的消息队列，请按照需要自行修改）的引入，业务本地消息表的设计稍稍改动了一下： 1234567891011121314151617181920212223CREATE TABLE `biz_local_message` ( `id` bigint(20) unsigned NOT NULL COMMENT '主键', `biz_module` tinyint(4) unsigned NOT NULL COMMENT '业务模块 1订单 2支付 4等等', `biz_no` char(64) NOT NULL COMMENT '业务单号，唯一标识', `biz_type` tinyint(4) unsigned NOT NULL COMMENT '业务类型', `topic_name` varchar(64) NOT NULL COMMENT '主题名称', `group_name` varchar(128) NOT NULL COMMENT '分组名称', `message_tag` varchar(128) NOT NULL COMMENT '消息tag', `msg` varchar(512) NOT NULL DEFAULT '' COMMENT '消息内容', `msg_desc` varchar(64) DEFAULT NULL COMMENT '消息描述', `backoff_second` int(10) unsigned NOT NULL DEFAULT '180' COMMENT '退避秒数，默认3分钟，用来计算next_handle_time时间', `handle_count` tinyint(4) NOT NULL DEFAULT '0' COMMENT '已经尝试处理次数', `max_handle_count` tinyint(4) NOT NULL DEFAULT '5' COMMENT '最大尝试处理次数', `next_handle_time` datetime DEFAULT NULL COMMENT '下一次处理时间', `handle_status` tinyint(4) unsigned NOT NULL DEFAULT '1' COMMENT '记录处理状态 1待处理 2处理中 4处理成功 8处理失败', `create_time` datetime NOT NULL COMMENT '创建时间', `create_user` varchar(32) NOT NULL DEFAULT 'admin' COMMENT '创建人', `update_time` datetime NOT NULL COMMENT '更新时间', `update_user` varchar(32) NOT NULL DEFAULT 'admin' COMMENT '更新人', PRIMARY KEY (`id`), UNIQUE KEY `uk_bizNoBizMoudleBizType` (`biz_no`,`biz_module`,`biz_type`) USING BTREE, KEY `idx_nextHandleTime` (`next_handle_time`)) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COMMENT='业务本地消息' 如果你的消息内容msg内容比较大使用的是text类型，为了提高数据库的效率，或许你可以将这个消息内容msg字段拿出来单独存一张表； 12345CREATE TABLE `biz_local_message_content` ( `id` bigint(20) unsigned NOT NULL COMMENT '主键', `local_message_id` bigint(20) unsigned NOT NULL COMMENT '消息主键', `msg` text NOT NULL DEFAULT '' COMMENT '消息内容') ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COMMENT='业务本地消息内容表' biz_local_message表中处理成功的消息可以直接统一删除，或者挪到一个新的表中备份。 这种和上面的方案相比就是把http通信的方式换成了把消息发送到消息队列中，然后下游系统从消息队列中消费消息，其他的流程差不多。 基于RocketMQ事务消息方案数据库的本地事务无法解决业务逻辑和消息发送的一致性，因为消息发送是一个网络通信过程，发送消息可能出现发送失败或者超时情况。超时的情况也有可能消息已经发送成功了，也有可能发送失败，但消息发送方是无法确定的，所以这时消息发送方是提交事务还是回滚事务，都是会有可能出现数据不一致的地方。 要解决这个问题，可以采用上面的本地消息表方案，业务逻辑和消息记录在一个事务中一起提交，然后再发送消息。或者采用MQ事务消息（half消息）的办法，事务消息和普通消息的区别在于，事务消息发送到消息队列中后处于prepared状态，是对消费者不可见的，等到事务消息的状态更改为可消费状态后，下游系统的消费者才可以消费到消息。 事务消息方案的处理流程如下： 事务发起者先发送一个事务消息到MQ中。 MQ系统接收到事务消息后，将事务消息持久化，此时事务消息的状态是“已准备”，并给消息发送者返回一个ACK响应。 事务消息发送者接收到了步骤2中MQ的ACK响应，则执行本地事务成功则提交事务，失败则回滚本地事务，然后给MQ系统返回一个响应，通知MQ本地事务执行的情况。如果事务消息发送者没有接收到步骤2中ACK响应，则无须或者取消执行本地事务。 MQ系统接收到事务消息发送者的反馈消息后，根据反馈的消息更改事务消息的状态。若反馈的消息是本地事务执行成功，则将事务消息的状态从“已准备”修改为“可消费”，并将消息下发给相应的消费者；若反馈的消息是本地事务执行失败，则直接删除该条事务消息。 步骤4中的反馈消息，有可能在发送给MQ的过程中丢失。所以MQ系统有定时任务进行补偿，扫描系统中那些状态仍然为“已准备”的事务消息，并向事务消息发送者询问消息的实际状态，并根据反馈的情况更新事务消息的状态。因此，事务消息发送者要实现一个查询事务消息状态的接口，以供MQ系统使用。 下游服务接收到MQ系统推送的消息之后，进行消费执行本地事务，如果执行成功，则给MQ系统返回ACK消息。反之，不返回ACK消息。MQ系统会进行重试投递，下游服务要注意实现接口幂等。 事务消息方案流程图 以上面提到的下单场景为例，伪代码实现如下：发送事务消息 123456789101112131415161718192021222324252627public Result buildOrder(OrderDto orderDto) &#123; TransactionListener transactionListener = new OrderTransactionListenerImpl(); TransactionMQProducer producer = new TransactionMQProducer(\"\"); producer.setTransactionListener(transactionListener); TransactionListener transactionListener = new OrderTransactionListenerImpl(); TransactionMQProducer producer = new TransactionMQProducer(\"\"); producer.setTransactionListener(transactionListener); // 1. 生成唯一业务bizId，将bizId和其他相关信息组装到消息体中 Message message = new Message(); orderDto.setBizId(\"\"); String orderString = JSON.toJSONString(orderDto); message.setBody(orderString.getBytes(StandardCharsets.UTF_8)); // 2. 调用消息客户端API，发送事务prepare消息 SendResult sendResult = producer.send(message); // 3. 根据不同情况返回不同的结果 if (SendStatus.SEND_OK.equals(sendResult.getSendStatus())) &#123; // 消息发送成功，可以返回前端告知用户订单处理中 return new Result(); &#125; // 反之失败，返回前端告知用户下单失败 return new Result(\"\",\"下单失败\");&#125; 执行本地事务和RocketMQ检查接口实现 1234567891011121314151617181920212223242526272829303132333435/** * 基于RocketMQ事务消息实现分布式事务 * * @author Sam * @date 2021/1/21 * @since 1.0.0 */@Slf4j@Componentpublic class OrderTransactionListenerImpl implements TransactionListener &#123; @Override public LocalTransactionState executeLocalTransaction(Message message, Object o) &#123; // 1. 从消息中取出业务唯一ID和相关信息 byte[] body = message.getBody(); // 2. 执行本地事务,成功返回LocalTransactionState.COMMIT_MESSAGE,失败返回LocalTransactionState.ROLLBACK_MESSAGE return LocalTransactionState.COMMIT_MESSAGE; &#125; /** * 消息队列定时补偿调用的接口,RocketMQ会调用多次 * @param messageExt * @return */ @Override public LocalTransactionState checkLocalTransaction(MessageExt messageExt) &#123; // 1. 从消息中取出业务唯一ID和相关信息 // 2. 检查业务,若执行成功则返回LocalTransactionState.COMMIT_MESSAGE,失败返回LocalTransactionState.ROLLBACK_MESSAGE return LocalTransactionState.UNKNOW; &#125;&#125; 其实，MQ事务消息和本地消息表两种方案其实是一样的，本质上就是MQ实现了那个日志记录的功能。还有这两种方案也是基于消息队列可靠性，对于如何提高消息队列的可靠性，恐怕还是可以写一篇长文。另外，基于事务消息的方案对MQ系统要求较高，目前并不是所有的MQ系统支持事务消息功能，貌似只有RocketMQ支持。如果你目前的系统使用的MQ系统不支持事务消息的功能，采用本地消息表方案实现分布式事务功能也算是一个不错的选择。 上述两种方式实现过程中可以发现，都是将分布式事务拆分成本地事务实现，流程中可能出现消息传递失败需要定时任务补偿，明显是符合BASE理论的，都不是强一致性的而都是柔性事务，实现数据的最终一致性。还有一点要提出来的就是，这些方案流程方向都是不可逆的，原则上上游系统事务成功提交，下游系统的事务则也一定要成功，业务不能回滚，如果要是出现消息消费失败，则只能进行不断的重试，直到成功为止。所以大家使用的时候要注意，如果要避免这种情况，可以加入前面提到预锁机制，例如，一个电商下单扣库存的业务场景，用户下单时不扣库存，等用户支付完成才扣库存，这种实现方案是有可能出现库存不足的问题，要解决这种情况，可以在用户下单的时候，直接使用RPC同步请求方式锁定库存进行预留，这样用户后面订单支付成功则将锁定的库存才真正的扣除，如果用户最终没有完成支付，则直接取消订单，是否锁定的库存即可。当然这种情况也有可能出现用户恶意下单，导致真正的用户无法购买的问题，还有库存锁定占用时间过长的问题等等，当然这些问题都是可以解决的，看业务的接受情况自行选择吧！ 方案总结世界上唯一不变的，就是变化本身。单体架构系统通过数据库事务的ACID特性很容易实现数据一致性，而且是强一致性。为了提高系统的高可用性、吞吐量，提出了分布式架构，结果导致出现了分布式事务问题。果然验证了那句话“软件行业是没有银弹的”！最终引出了CAP理论、BASE理论，各路大牛也给出了相应的解决方案。 在分布式架构的情况下，将之前的关系型数据库提供的事务称之为本地事务，解决分布式系统中存在事务方案进而称之为分布式事务（重新定义下概念）。2PC、3PC方案实现的事务能保证强一致性，我们称之为刚性事务；TCC和可靠消息方案实现的事务只能保证最终一致性，那么我们称之为柔性事务。 由于CAP理论太过于乐观导致对于构建分布式系统没有全面的指导作用，大牛们对CAP理论的缺点进行改进提出了BASE理论。相比之下BASE理论更符合实际的情况，更加的接地气。BASE理论的指导我们在发生消息丢失或者网络分区的问题情况下，分布式系统可以根据自身业务特点，采取是保大（Consistency）还是保小（Availability）😄😄，然后使得系统在一定时间内达到最终一致性。 刚性事务可以实现强一致性，但性能不高；柔性事务性能较高，但是只能实现弱一致性。所以大家在构建自己的分布式系统遇到分布式事务问题时，根据自己的需求场景和业务承受情况，自行选择吧！ [参考资料：《分布式服务架构 原理、设计与实践》 李艳鹏 杨彪 著 ]","categories":[],"tags":[{"name":"分布式事务","slug":"分布式事务","permalink":"https://ydstudios.gitee.io/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1/"},{"name":"2pc","slug":"2pc","permalink":"https://ydstudios.gitee.io/tags/2pc/"},{"name":"3pc","slug":"3pc","permalink":"https://ydstudios.gitee.io/tags/3pc/"},{"name":"tcc","slug":"tcc","permalink":"https://ydstudios.gitee.io/tags/tcc/"},{"name":"本地消息表","slug":"本地消息表","permalink":"https://ydstudios.gitee.io/tags/%E6%9C%AC%E5%9C%B0%E6%B6%88%E6%81%AF%E8%A1%A8/"},{"name":"rocketmq事务消息","slug":"rocketmq事务消息","permalink":"https://ydstudios.gitee.io/tags/rocketmq%E4%BA%8B%E5%8A%A1%E6%B6%88%E6%81%AF/"},{"name":"可靠消息最终一致性方案","slug":"可靠消息最终一致性方案","permalink":"https://ydstudios.gitee.io/tags/%E5%8F%AF%E9%9D%A0%E6%B6%88%E6%81%AF%E6%9C%80%E7%BB%88%E4%B8%80%E8%87%B4%E6%80%A7%E6%96%B9%E6%A1%88/"}]},{"title":"解耦神器之观察者模式","slug":"解耦神器之观察者模式","date":"2021-02-04T23:49:32.000Z","updated":"2021-02-06T09:07:26.000Z","comments":true,"path":"post/5b3d1989.html","link":"","permalink":"https://ydstudios.gitee.io/post/5b3d1989.html","excerpt":"大家是不是会经常听到有人说写代码要“高内聚、低耦合”，但是你要是问他什么是高内聚低耦合（waht）？为什么要高内聚、低耦合(why)？怎么做才能高内聚、低耦合(how)? 这些问题就不一定都能回答的了！","text":"大家是不是会经常听到有人说写代码要“高内聚、低耦合”，但是你要是问他什么是高内聚低耦合（waht）？为什么要高内聚、低耦合(why)？怎么做才能高内聚、低耦合(how)? 这些问题就不一定都能回答的了！ 百度百科：在软件设计中通常用耦合度和内聚度作为衡量模块独立程度的标准。划分模块的一个准则是高内聚低耦合。从模块粒度来看，高内聚：尽可能类的每个成员方法只完成一件事（最大限度的聚合）； 低耦合：减少类内部，一个成员方法调用另一个成员方法。从类角度来看， 高内聚低耦合：减少类内部，对其他类的调用；从功能块来看，高内聚低耦合：减少模块之间的交互复杂度（接口数量，参数数据）即横向：类与类之间、模块与模块之间；纵向：层次之间；尽可能，内容内聚，数据耦合。 我本人特别喜欢李连杰演的这一版霍元甲，他年少轻狂，好勇斗狠，勇争津门第一，中年家破人亡，流落他乡悟出了侠的真谛，成为真正的大侠霍元甲。当然我不太希望我的一生这么波荡起伏，因为我没有他那么厉害。但是我特别喜欢剧中的一句台词，就是“功夫是需要时间去磨练出来的，两三年的猫脚功夫，这一拳20年的功夫，你们挡得住么？。作为一位程序员，我们对业务的理解、系统的设计、代码实现这些能力也是需要磨练的，而且也要日复一日的练习，不然你怎么挡得住面试官20年的功夫。😄😄 本文对于什么是高内聚低耦合（waht）？为什么要高内聚、低耦合(why)？按下不表，就先说说一种能够让你的代码高内聚低耦合的方法： 观察者模式,这种设计模式我认为就和策略模式一样的常用，这两种设计模式不仅实现起来简单，而且效果还好！其中的观察者模式简直是降低模块耦合性提高我们代码逼格的神奇啊！ 那接下来就先说说观察者模式组成角色： 抽象主题(Subject)它把所有观察者对象的引用保存到一个容器里，每个主题都可以有任何数量的观察者。抽象主题提供一个接口，可以增加和删除观察者对象。 具体主题(ConcreteSubject)将有关状态存入具体观察者对象；在具体主题内部状态改变时，给所有登记过的观察者发出通知。 抽象抽象观察者(Observer)为所有的具体观察者定义一个接口，在得到主题通知时更新自己。 具体观察者(ConcreteObserver)实现抽象观察者角色所要求的更新接口，以便使本身的状态与主题状态协调。 Java原生实现Java从JDK1.0开始就自带了 java.util.Observer 和 java.util.Observable ,相关接口、类来实现观察者模式，由此可见Java对于观察者模式的重视，也从另一方面说明观察者模式是多么重要。那么接下来我们开始用Java自带的来实现观察者模式的编码。 在这里我们以一个下单的场景来举例描述，用户创建完订单，要发送一个消息通知用户订单创建成功，还要发送一个消息给仓库，检查对应的订单商品。具体代码如下。 1. 继承java.util.Observable定义主题12345678910111213141516171819public class OrderSubject extends Observable &#123; /** * 订单状态发生变化通知其他， */ private String status; public String getStatus() &#123; return status; &#125; public void setStatus(String status) &#123; this.status = status; // 通知其他 this.setChanged(); // 这里是可以携带一些额外的参数 this.notifyObservers(); &#125;&#125; 2. 实现java.util.Observer接口定义观察者java.util.Observer 是一个接口，接口中只有一个 update 方法。每当更改被观察对象时，都会调用此方法。 应用程序调用Observable对象的 notifyObservers方法，以将更改通知给所有对象的观察者。在这个例子中通知模块、仓库模块就是扮演观察者的角色，具体的代码实现如下： 通知模块实现java.util.Observer接口 12345678@Slf4jpublic class NoticeObserver implements Observer &#123; @Override public void update(Observable o, Object arg) &#123; OrderSubject orderSubject = (OrderSubject) o; log.info(\"通知中心收到了订单的变化[&#123;&#125;],arg = [&#123;&#125;]\", orderSubject.getStatus(),arg); &#125;&#125; 仓库模块实现java.util.Observer接口 123456789@Slf4jpublic class WareHouseObserver implements Observer &#123; @Override public void update(Observable observable, Object arg) &#123; OrderSubject orderSubject = (OrderSubject) observable; log.info(\"仓库观察者收到了订单的变化[&#123;&#125;],arg = [&#123;&#125;]\", orderSubject.getStatus(), arg); &#125;&#125; 3. 代码编写完成，测试一把具体测试代码如下： 1234567891011121314151617181920public class ObserverTest &#123; @Test public void test()&#123; OrderSubject orderSubject = new OrderSubject(); // 通知系统监听者 NoticeObserver noticeObserver = new NoticeObserver(); orderSubject.addObserver(noticeObserver); // 仓库监听者 WareHouseObserver wareHouseObserver = new WareHouseObserver(); orderSubject.addObserver(wareHouseObserver); // 订单创建 orderSubject.setStatus(\"订单创建啦\"); // 订单付款 orderSubject.setStatus(\"订单付款啦\"); &#125;&#125; 订单的状态发生了变化，对应的观察者就接受到了通知。JDK提供的实现很简单，但是缺陷也是很明显。 Observable 是一个类，不是一个接口，也不是一个抽象类，不继承该类无法使用权限类型为 protected 的setChanged()和 clearChanged()方法。 用Vector&lt;Observer&gt;保存观察者。Vector效率低下，JDK已经不建议使用了。 主题变化通知观察者的顺序无法自定义。通知的顺序是固定为按照设置观察者的时间倒叙通知，无法自定义。 整体代码执行是同步的，不能设置为异步。且观察者的执行出现异常，程序处理不到位会导致事务回滚。 既然JDK中实现不够完美，我们就看看有没有其他的实现方式。Google的Guava工具包提供了EventBus可以实现、Spring框架中也提供了相应的支持，由于我们使用Spring框架比较多，今天我们就先来说说Spring框架中是如何实现对观察者模式的支持吧！ Spring中的实现Spring框架中提供了相当多的组件可以实现观察者模式，文章后面会一一向大家介绍。这里我引入一个新的业务场景，用户注册之后发送短信通知、积分还有优惠券。这种业务场景非常的常见，我展示一下小伙伴们一般是怎么样实现这样的功能的。因为篇幅有限，这里就展示一下伪代码吧！ 123456789101112131415161718192021222324252627282930public class MemberService&#123; @Autowired private MemberDao memberDao; @Autowired private CouponService couponService; @Autowired private SmsService smsService; /** * 完成用户注册，成功之后发送优惠券、短信通知 * @param memberName 查询条件 * @param phone 分页信息 * @return */ public Result normalRegister(String memberName,String phone)&#123; // 1、执行注册逻辑 log.info(\"[register][执行用户(&#123;&#125;) 的注册逻辑]\", phone); // 保存用户 Long memberId = memberDao.saveMember(); // 2、发送优惠券 couponService.sendCoupon(memberId); // 3、发送短信 smsService.sendSms(phone); return new Result(); &#125;&#125; 大家对这部分代码怎么看？ 对于用户注册功能来说，保障用户成功注册，这是个业务的核心诉求，其他的都是锦上添花，所以这个地方不应该掺杂其他的业务。用户注册与发送短信、优惠券功能应该分隔开，这样也符合解耦的设计规则。不然后续业务增加需求，需要用户注册完成之后给用户发送积分，你是不是还要在这个 normalRegister方法中增加发送积分的逻辑，那这个岂不是又违背了开闭原则。 一件事做容易，想做好不容易。写程序也是如此，能够把功能编码实现bug又少，其实是很耗费时间和精力的。我常跟同事们开玩笑说“写代码不要糊弄，该写的代码一定要写，不然你最后肯定会被代码糊弄”。 接下来我来介绍一下如何使用Spring的事件发布机制来实现业务解耦的 由于后面的实现方法中用户注册事件类的代码都是类似的只是类名不同，那我就把用户注册事件类的代码写在前面。 1234567891011121314151617181920212223242526import org.springframework.context.ApplicationEvent;@Slf4j@Datapublic class UserRegisterEvent extends ApplicationEvent &#123; /** * 可以自定义些自己需要的属性 * 用户的id */ private Long memberId; /** * 用户手机号 */ private String memberPhone; public UserRegisterEvent(Object source) &#123; super(source); &#125; public UserRegisterEvent(Object source, Long memberId, String memberPhone) &#123; super(source); this.memberId = memberId; this.memberPhone = memberPhone; &#125;&#125; 利用继承ApplicationEvent类、实现ApplicationListener接口或者 @EventListener注解来实现org.springframework.context.ApplicationListener是Spring提供的一个泛型接口。由应用程序事件侦听器实现的接口,从Spring 3.0开始，ApplicationListener可以一般性地声明其感兴趣的事件类型。在Spring ApplicationContext中注册后，将相应地过滤事件，并且仅针对匹配事件对象调用侦听器。观察者可以实现ApplicationListener接口或者使用@EventListener注解，这个注解和ApplicationListener接口有着相同的功能，相比之下使用注解比使用接口更方便一点。 短信Service实现ApplicationListener接口123456789@Slf4j@Servicepublic class SmsService &#123; @EventListener public void sendSms(UserRegisterEvent event) &#123; log.info(\"[EventListener][给用户(&#123;&#125;) 发送短信]\", event); &#125;&#125; 优惠券Service直接在addCoupon 方法上添加注解 @EventListener注意addCoupon 方法的入参为 UserRegisterEvent 123456789@Slf4j@Servicepublic class CouponService implements ApplicationListener&lt;UserRegisterEvent&gt; &#123; @Override public void onApplicationEvent(UserRegisterEvent event) &#123; log.info(\"[addCoupon][给用户(&#123;&#125;) 发放优惠劵]\", event); &#125;&#125; 积分Service中的代码与上面的类似就不贴出来了 注册业务完成发布UserRegisterEvent事件这个地方MemberService要实现org.springframework.beans.factory.Aware.ApplicationEventPublisherAware接口，才有能力使用 ApplicationEventPublisher 发布事件。 1234567891011121314151617181920212223242526272829@Slf4j@Servicepublic class MemberService implements ApplicationEventPublisherAware &#123; private ApplicationEventPublisher applicationEventPublisher; @Override public void setApplicationEventPublisher(ApplicationEventPublisher applicationEventPublisher) &#123; this.applicationEventPublisher = applicationEventPublisher; &#125; /** * 最简单的观察者实现 * @param phone 用户手机号 * @return */ public Result register(String memberPhone) &#123; // 1、 ... 执行注册逻辑 log.info(\"[register][执行用户(&#123;&#125;) 的注册逻辑]\", memberPhone); // 2、用户数据保存到数据库 Long memberId = 1000L; // 3、发布事件通知 applicationEventPublisher.publishEvent(new UserRegisterEvent(this, memberId, memberPhone)); return new Result(); &#125;&#125; 单元测试123456789101112@ContextConfiguration(locations = &#123;\"classpath:spring/spring-dao.xml\", \"classpath:spring/spring-service.xml\"&#125;)@RunWith(value = SpringJUnit4ClassRunner.class)public class ObserverTest &#123; @Autowired private MemberService memberService; @Test public void register() &#123; memberService.register(\"13911111111\"); &#125;&#125; 执行结果 代码执行成功，打印出了给用户发送短信、优惠券日志，功能完美实现。但是稍微有点问题，我想调换一下两者的顺序，先执行发送优惠券，再发送短信。这个要求还是蛮常见，那怎么实现呢？ 使用SmartApplicationListener或者GenericApplicationListener接口实现指定观察者自定义顺序执行org.springframework.context.event.SmartApplicationListener是Spring3.0版本提供的接口，org.springframework.context.event.GenericApplicationListener则是Spring4.2版本提供的接口，如果在你的项目中使用不了，那就是说明你项目中使用的Spring版本过低，这个问题自己自行处理。在查SmartApplicationListener源码时发现有以下注释： 123456Extended variant of the standard &#123;@link ApplicationListener&#125; interface,exposing further metadata such as the supported event type.&lt;p&gt;Users are &lt;bold&gt;strongly advised&lt;/bold&gt; to use the &#123;@link GenericApplicationListener&#125;interface instead as it provides an improved detection of generics-basedevent types. 翻译之后大概的意思就是，SmartApplicationListener是标准ApplicationListener接口的扩展变体，公开了进一步的元数据，例如受支持的事件类型。但强烈建议改用GenericApplicationListener接口，因为它提供了对基于泛型的事件类型的改进检测，那我此处就只以GenericApplicationListener为例了。 优惠券服务Service同样实现接口 GenericApplicationListener 123456789101112131415161718192021222324252627282930313233343536373839404142434445@Slf4j@Servicepublic class SecondVersionCouponService implements GenericApplicationListener &#123; /** * 确定此侦听器是否实际上支持给定的事件类型。 * @param eventType * @return */ @Override public boolean supportsEventType(ResolvableType eventType) &#123; return SecondVersionUserRegisterEvent.class.equals(eventType.getRawClass()); &#125; /** * 确定此侦听器是否实际上支持给定的源类型 * @param sourceType * @return */ @Override public boolean supportsSourceType(Class&lt;?&gt; sourceType) &#123; return MemberService.class == sourceType; &#125; /** * 具体的业务逻辑 * @param event */ @Override public void onApplicationEvent(ApplicationEvent event) &#123; SecondVersionUserRegisterEvent userRegisterEvent = (SecondVersionUserRegisterEvent) event; Long memberId = userRegisterEvent.getMemberId(); log.info(\"[Second addCoupon][给用户(&#123;&#125;) 发放优惠劵]\", memberId); &#125; /** * 设定执行的顺序，数字越小优先级越高就越先被执行 * @return */ @Override public int getOrder() &#123; return 0; &#125;&#125; 短信服务Service同样实现接口 GenericApplicationListener 12345678910111213141516171819202122232425262728@Slf4j@Servicepublic class SecondVersionSmsService implements GenericApplicationListener &#123; @Override public boolean supportsEventType(ResolvableType eventType) &#123; return SecondVersionUserRegisterEvent.class == eventType.getRawClass(); &#125; @Override public boolean supportsSourceType(Class&lt;?&gt; sourceType) &#123; return MemberService.class == sourceType; &#125; @Override public void onApplicationEvent(ApplicationEvent event) &#123; SecondVersionUserRegisterEvent userRegisterEvent = (SecondVersionUserRegisterEvent) event; String memberPhone = userRegisterEvent.getMemberPhone(); log.info(\"[Second onApplicationEvent][给用户(&#123;&#125;) 发送短信]\", memberPhone); &#125; @Override public int getOrder() &#123; return 10; &#125;&#125; 其实使用org.springframework.context.ApplicationListener + org.springframework.core.annotation.Order注解也可以实现控制观察者的执行顺序，而且更简洁！ 注册业务完成发布UserRegisterEvent事件这个地方MemberService要实现org.springframework.beans.factory.Aware.ApplicationEventPublisherAware接口，才有能力使用 ApplicationEventPublisher 发布事件。 1234567891011121314151617181920212223242526272829303132333435363738394041@Slf4j@Servicepublic class MemberService implements ApplicationEventPublisherAware &#123; private ApplicationEventPublisher applicationEventPublisher; @Override public void setApplicationEventPublisher(ApplicationEventPublisher applicationEventPublisher) &#123; this.applicationEventPublisher = applicationEventPublisher; &#125; /** * 最简单的观察者实现 * @param phone 用户手机号 * @return */ @Transactional(rollbackFor = Exception.class) public Result register(String phone) &#123; // 1、 ... 执行注册逻辑 log.info(\"[Second register][执行用户(&#123;&#125;) 的注册逻辑]\", phone); // 2、 保存用户到数据库（假设这个是） BizLocalMessageEntity entity = new BizLocalMessageEntity(); ThreadLocalRandom random = ThreadLocalRandom.current(); entity.setBizModule((byte) 0); entity.setBizNo(\"\" + random.nextInt(100000)); entity.setBizType((byte) 0); entity.setSendStatus((byte) 0); entity.setCreateTime(new Date()); entity.setUpdateTime(new Date()); int affectRow = bizLocalMessageMapper.insert(entity); Long memberId = entity.getId(); log.info(\"插入的affectRow= [&#123;&#125;],主键[&#123;&#125;]\", affectRow, memberId); // 3、发布事件通知 applicationEventPublisher.publishEvent(new SecondVersionUserRegisterEvent(this, memberId, phone)); return new Result(); &#125;&#125; 单元测试123456789101112@ContextConfiguration(locations = &#123;\"classpath:spring/spring-dao.xml\", \"classpath:spring/spring-service.xml\"&#125;)@RunWith(value = SpringJUnit4ClassRunner.class)public class ObserverTest &#123; @Autowired private MemberService memberService; @Test public void register() &#123; memberService.register(\"13911111111\"); &#125;&#125; 执行结果 修改两个Service中order的数值可以轻松两个观察者的执行顺序，完美的解决了自定义观察者执行顺序的要求。细心的小伙伴会发现，假设在用户注册的方法中存在事务，观察者的执行出现异常，会导致整个事务回滚，明明用户的数据已经保存到数据库，现在却没有了！还是前面那句话，在这个业务场景，用户注册成功是核心诉求，发送短信、优惠券重要性都是比不上的。那要解决这种情况，就需要用户注册成功的事务提交之后，再发布注册成功事件，避免观察者执行结果的影响。 短信服务Service有有异常抛出导致事务回滚 12345678910@Override public void onApplicationEvent(ApplicationEvent event) &#123; SecondVersionUserRegisterEvent userRegisterEvent = (SecondVersionUserRegisterEvent) event; String memberPhone = userRegisterEvent.getMemberPhone(); if (StringUtils.isNotBlank(memberPhone)) &#123; throw new RuntimeException(\"测试事件中发生异常对原有业务的影响,短信发送失败！！！！\"); &#125; log.info(\"[Second onApplicationEvent][给用户(&#123;&#125;) 发送短信]\", memberPhone); &#125; 可能有的小伙伴想，观察者执行异常导致事务回滚，本质上来说这种方式实现的观察者模式，其实还是在一个线程中同步执行的。那我就在观察者中捕获所有的异常或者用户注册成功之后异步发布事件。 捕获所有异常 123456789101112131415public void onApplicationEvent(ApplicationEvent event) &#123; try &#123; SecondVersionUserRegisterEvent userRegisterEvent = (SecondVersionUserRegisterEvent) event; Long memberId = userRegisterEvent.getMemberId(); if (memberId != null) &#123; throw new RuntimeException(\"测试事件中发生异常对原有业务的影响,短信发送失败！！！！\"); &#125; log.info(\"[Second addCoupon][给用户(&#123;&#125;) 发放优惠劵]\", memberId); &#125; catch (Exception e) &#123; log.error(\"发放优惠劵发生异常\", e); &#125; &#125; 异步发布事件 123456789// 3、异步发布事件通知,这样的有可能事务还没有提交，则异步线程读取不到数据库中的数据CompletableFuture&lt;Void&gt; voidCompletableFuture = CompletableFuture.runAsync(() -&gt; &#123; applicationEventPublisher.publishEvent(new UserRegisterEvent(this, memberId, phone));&#125;);voidCompletableFuture.exceptionally(exception -&gt; &#123; log.error(\"发生了异常\", exception); return null;&#125;); 第一种方式可以解决事务回滚的问题，但是呢感觉会有点low！ 第二种方式假如观察者中查询会员的信息，可能出现查不到，因为可能事务还没有提交。 使用 @TransactionalEventListener 实现事务提交之后才执行观察者使用起来很简单，直接在对应的观察者方法上添加该注解即可。@TransactionalEventListener(phase = TransactionPhase.AFTER_COMMIT) 12345678910111213public class ThreeCouponService &#123; @TransactionalEventListener(phase = TransactionPhase.AFTER_COMMIT) public void addCoupon(ThreeUserRegisterEvent event) &#123; log.info(\"测试事件中发生异常对原有业务的影响\"); Long memberId = event.getMemberId(); if (memberId != null) &#123; throw new RuntimeException(\"测试事件中发生异常对原有业务的影响,优惠券发送失败！！！！\"); &#125; log.info(\" Version=3 [addCoupon][给用户(&#123;&#125;) 发放优惠劵]\", memberId); &#125;&#125; 注意一定要在 register() 方法上添加 @Transactional 注解，不然 register() 方法中有抛出异常之后，会有下面的错误提示： 12信息: Closing org.springframework.context.support.GenericApplicationContext@a67c67e: startup date [Thu Feb 04 18:06:53 CST 2021]; root of context hierarchy @Transactional一定要加上不然方法体内有一次抛出会有上的错误。 123456789101112131415@Transactional(rollbackFor = Exception.class)public Result register(String phone) &#123; // 1、 ... 执行注册逻辑 log.info(\"[Second register][执行用户(&#123;&#125;) 的注册逻辑]\", phone); // 2、 保存用户(假设这是) int affectRow = bizLocalMessageMapper.insert(entity); Long memberId = entity.getId(); log.info(\"插入的affectRow= [&#123;&#125;],主键[&#123;&#125;]\", affectRow, memberId); if (memberId != null) &#123; throw new RuntimeException(\"故意抛出异常\"); &#125; applicationEventPublisher.publishEvent(new ThreeUserRegisterEvent(this, memberId, phone)); return new Result();&#125; 其实，实现 register() 事务提交之后，再发布事件还有两种方式： 使用事务模板TransactionTemplate 使用之前需要在配置文件中配置事务模板 123456&lt;!-- 配置 transactionTemplate --&gt; &lt;bean id=\"transactionTemplate\" class=\"org.springframework.transaction.support.TransactionTemplate\"&gt; &lt;property name=\"transactionManager\"&gt; &lt;ref bean=\"transactionManager\"/&gt; &lt;/property&gt; &lt;/bean&gt; 在代码中使用事务模板提交事务 12345678910111213141516171819202122232425262728293031323334353637383940414243@Autowiredprivate TransactionTemplate transactionTemplate; public Result register(String phone) &#123; // 1、 ... 执行注册逻辑 log.info(\"[线程name=&#123;&#125;,register][执行用户(&#123;&#125;) 的注册逻辑]\", Thread.currentThread().getName(), phone); // 2、 保存用户(假设这是) Long memberId = transactionTemplate.execute(status -&gt; &#123; Long result = null; try &#123; BizLocalMessageEntity entity = new BizLocalMessageEntity(); ThreadLocalRandom random = ThreadLocalRandom.current(); entity.setBizModule((byte) 0); entity.setBizNo(\"\" + random.nextInt(200000)); entity.setBizType((byte) 0); entity.setMsg(\"消息内容\"); entity.setMsgDesc(\"消息备注\"); entity.setCreateTime(new Date()); entity.setUpdateTime(new Date()); int affectRow = bizLocalMessageMapper.insert(entity); result = entity.getId(); log.info(\"Version5 插入的affectRow= [&#123;&#125;],主键[&#123;&#125;]\", affectRow, result); if (affectRow &gt; 0) &#123; //throw new RuntimeException(\"测试主要业务失败，事件是否会发布执行\"); &#125; &#125; catch (Exception e) &#123; log.error(\"业务失败发生异常\", e); // 标记事务回滚 status.setRollbackOnly(); &#125; return result; &#125;); log.info(\"事务执行情况 用户的id= [&#123;&#125;]\", memberId); if (memberId != null) &#123; // 3、异步发布事件通知 log.info(\"发布注册成功事件\"); applicationEventPublisher.publishEvent(new UserRegisterEvent(this, memberId, phone)); &#125; return new Result(); &#125; 使用TransactionSynchronizationManager 这个可以直接使用无需配置，重写TransactionSynchronizationAdapter的afterCommit()方法 123456789101112131415161718192021222324252627282930313233@Transactional(rollbackFor = Exception.class)public Result register(String phone) &#123; // 1、 ... 执行注册逻辑 log.info(\"[Version4 register][执行用户(&#123;&#125;) 的注册逻辑]\", phone); // 2、 保存用户(假设这是) BizLocalMessageEntity entity = new BizLocalMessageEntity(); ThreadLocalRandom random = ThreadLocalRandom.current(); entity.setBizModule((byte) 0); entity.setBizNo(\"\" + random.nextInt(100000)); entity.setBizType((byte) 0); entity.setMsg(\"\"); entity.setMsgDesc(\"\"); entity.setCreateTime(new Date()); entity.setUpdateTime(new Date()); Long memberId = entity.getId(); int affectRow = bizLocalMessageMapper.insert(entity); log.info(\"Version4 插入的affectRow= [&#123;&#125;],主键[&#123;&#125;]\", affectRow, memberId); if (affectRow &gt; 0) &#123; //throw new RuntimeException(\"Version4 测试主要业务失败，事件是否会发布执行\"); &#125; TransactionSynchronizationManager.registerSynchronization(new TransactionSynchronizationAdapter() &#123; @Override public void afterCommit() &#123; super.afterCommit(); // 3、事务提交之后发布事件通知 applicationEventPublisher.publishEvent(new UserRegisterEvent(this, memberId, phone)); &#125; &#125;); return new Result();&#125; 我们把目前的代码执行一下看看情况怎么样 从截图上看，我们目前已经通过好几种方式解决了观察者执行异常导致事务回滚的问题，但是也可以从截图中看出用户注册逻辑和对应的观察者都是在main线程中执行的，那我们可以让观察者异步执行吗？这样也可以加快代码的执行速度。这个问题当然也是可以解决的，接下来我们来解决这个问题： 直接使用@Async注解 需要注意的是，使用这个注解一定要自定义线程池，不是它默认使用的是 org.springframework.core.task.SimpleAsyncTaskExecutor 。但这个 SimpleAsyncTaskExecutor 不是真的线程池，这个类不重用线程，每次调用都会创建一个新的线程。 12345678910111213public class SixCouponService &#123; @Async @Order(20) @EventListener public void addCoupon(SixUserRegisterEvent event) &#123; log.info(\"测试事件中发生异常对原有业务的影响\", \"\"); Long memberId = event.getMemberId(); log.info(\"线程name=&#123;&#125;,[addCoupon][给用户(&#123;&#125;) 发放优惠劵]\", Thread.currentThread().getName(), memberId); &#125;&#125; 启动异步线程来发布事件 意思就是事务提交之后，判断事务提交成功则新建一个线程然后发布注册成功事件。事务提交成功可以前面提到的事务模板transactionTemplate和事务管理器TransactionSynchronizationManager。 12345678910111213141516171819202122232425262728293031323334353637383940public Result register(String phone) &#123; // 1、 ... 执行注册逻辑 log.info(\"[线程name=&#123;&#125;,register][执行用户(&#123;&#125;) 的注册逻辑]\", Thread.currentThread().getName(), phone); // 2、 保存用户(假设这是) Long memberId = transactionTemplate.execute(status -&gt; &#123; Long result = null; try &#123; BizLocalMessageEntity entity = new BizLocalMessageEntity(); ThreadLocalRandom random = ThreadLocalRandom.current(); entity.setBizModule((byte) 0); entity.setBizNo(\"\" + random.nextInt(200000)); entity.setBizType((byte) 0); entity.setMsg(\"消息内容\"); entity.setMsgDesc(\"消息备注\"); entity.setCreateTime(new Date()); entity.setUpdateTime(new Date()); int affectRow = bizLocalMessageMapper.insert(entity); result = entity.getId(); log.info(\"Version5 插入的affectRow= [&#123;&#125;],主键[&#123;&#125;]\", affectRow, result); &#125; catch (Exception e) &#123; log.error(\"业务失败发生异常\", e); // 标记事务回滚 status.setRollbackOnly(); &#125; return result; &#125;); log.info(\"事务执行情况 用户的id= [&#123;&#125;]\", memberId); if (memberId != null) &#123; // 3、异步发布事件通知 log.info(\"发布注册成功事件\"); CompletableFuture.runAsync(()-&gt;&#123; applicationEventPublisher.publishEvent(new SixUserRegisterEvent(this, memberId, phone)); &#125;); &#125; return new Result(); &#125; 代码修改之后执行，运行截图如下： 我们可以发现代码分别main、taskExecutor-1和taskExecutor-2 三个线程执行的，完美的解决了上面提到的问题。 使用观察者模式之前的用户注册流程： 使用观察者模式之后的用户注册流程： 总结用户数据写入数据表成功，发布注册成功事件，通知相关的短信、优惠券、积分模块。用观察者模式实现之后，注册逻辑和其他的逻辑分离开，即使后续新加一个邮件通知或者去掉一个发送积分的操作,只是修改了对应的代码逻辑，不用回滚测试所有场景。这符合对修改关闭对扩展开放的规则，有效的降低了各个模块之间耦合度，使得代码的可维护性大大提高。有些同学可能会抱怨，自己的项目简单用不到这些看似高大上的设计模式，其实不然，你只是没有认真研究自己的项目，还有就是对自己的写的代码没有太高的要求。如果你看到了本文，而且看到此处，你回想一下你所做的项目，难道没有一个业务场景满足观察者模式条件么？你想想是不是！","categories":[],"tags":[{"name":"spring","slug":"spring","permalink":"https://ydstudios.gitee.io/tags/spring/"},{"name":"设计模式","slug":"设计模式","permalink":"https://ydstudios.gitee.io/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"},{"name":"观察者模式","slug":"观察者模式","permalink":"https://ydstudios.gitee.io/tags/%E8%A7%82%E5%AF%9F%E8%80%85%E6%A8%A1%E5%BC%8F/"}]},{"title":"船新版本的策略模式，你一定没有见过","slug":"船新版本的策略模式，你一定没有见过","date":"2021-01-30T15:06:49.000Z","updated":"2021-02-14T04:07:41.000Z","comments":true,"path":"post/cd1b2f51.html","link":"","permalink":"https://ydstudios.gitee.io/post/cd1b2f51.html","excerpt":"我在之前的文章设计模式学习之策略模式 一文中介绍了策略模式的优点、缺点、实现方法和具体的使用场景。策略类有策略类会增多,造成类膨胀的缺点，这个缺点我还是比较在意的。因为在实际开发中发现，有时策略的实现没有多少代码，这样的定义成一个策略类反而真的有点浪费，确实造成了类膨胀。自从发现了这个问题，我一直在思考如何解决这种问题，经过思考解决问题的方法已经想出来了，但是一直没有去尝试做出来看看是否真的能解决策略模式 类膨胀的问题,终于在这周我忙里偷闲用代码实现了我的想法，我自认为利用 Lambda表达式 + 反射 + 注解 组合解决了类膨胀的问题，这种船新版本实现方案，容我细细道来！","text":"我在之前的文章设计模式学习之策略模式 一文中介绍了策略模式的优点、缺点、实现方法和具体的使用场景。策略类有策略类会增多,造成类膨胀的缺点，这个缺点我还是比较在意的。因为在实际开发中发现，有时策略的实现没有多少代码，这样的定义成一个策略类反而真的有点浪费，确实造成了类膨胀。自从发现了这个问题，我一直在思考如何解决这种问题，经过思考解决问题的方法已经想出来了，但是一直没有去尝试做出来看看是否真的能解决策略模式 类膨胀的问题,终于在这周我忙里偷闲用代码实现了我的想法，我自认为利用 Lambda表达式 + 反射 + 注解 组合解决了类膨胀的问题，这种船新版本实现方案，容我细细道来！ 大扎好，我系渣渣辉，探挽懒月，介四里没有挽过的船新版本，挤需体验三番钟，里造会干我一样，爱象节款游戏。 —开心一哈 😄😄 代码结构图这种实现方法，UML图没法展现其中关系，所以就不贴UML图了，直接上代码的结构图。 主要思想 为何会出现类膨胀 策略模式类膨胀的缺点，就是策略类太多的缘故。如果策略类的实现代码不是很多，是否可以将策略不在类的级别上实现？于是想到了利用Lambda表达式的匿名类来实现策略接口，这样不就把实现从类降级到了方法级别了么，哈哈😄😄！！ 如何将这些策略方法统一起来，方便外部调用策略方法都在一个类中，这样首先要标注一下方法，哪个是策略方法，哪个是普通方法。这个问题我们用注解来实现，因为在Java 语言中的类、方法、变量、参数和包等都可以用注解来标注。 我想把策略方法存放在Map中，用策略的名字作为Map的key，策略方法作为Map的value。我们在这里用反射来将Lambda实现的匿名类存放到Map中。 代码实现 定义一个策略注解 Strategy，用来标注策略方法 1234567891011121314151617181920212223242526/** * * Strategy 注解 * * @author Sam * @date 2021/1/26 * @since 1.7.7 */@Inherited@Documented@Retention(RetentionPolicy.RUNTIME)@Target(&#123;ElementType.METHOD,ElementType.TYPE&#125;)public @interface Strategy &#123; /** * 标识不同的模块 product、order模块 * @return */ String module() default \"\"; /** * 具体的策略类型 * @return */ String value() default \"\";&#125; 策略接口,这里的策略接口如果不是很复杂的话可以用jdk自带的java.util.function.Function&lt;T, R&gt;替代 1234567891011121314151617181920/** * * @param &lt;T&gt; 策略的入参类型 * @param &lt;R&gt; 策略的返回值类型 * * @author Sam * @date 2021/1/26 * @since 1.7.7 */@FunctionalInterfacepublic interface IStrategyHandler&lt;T, R&gt; &#123; /** * 策略 * @param param 参数 * @return */ R apply(T param);&#125; AbstractStrategy实现使用Spring提供的扩展点ApplicationContextAware，在系统启动的时候将对应策略方法的存放在Map中，同时对外提供执行入口 execute 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283@Slf4jpublic abstract class AbstractStrategyContext&lt;T, R&gt; implements ApplicationContextAware &#123; private Map&lt;String, IStrategyHandler&lt;T, R&gt;&gt; implMap = new ConcurrentHashMap&lt;&gt;(); private final String DELIMITER = \"####\"; /** * 获得bean 的class * * @param &lt;K&gt; 类型 * @return */ abstract &lt;K&gt; Class&lt;K&gt; getClazz(); /** * 返回spring中的beanName * * @return */ abstract String getBeanName(); /** * 执行函数 * * @param strategy 策略类型 * @param module 模块 * @param param 参数 * @return */ public R execute(String strategy, String module, T param) &#123; String key = StringUtils.join(module, DELIMITER, strategy); IStrategyHandler&lt;T, R&gt; handler = implMap.get(key); log.debug(\"策略实现集合&#123;&#125;\", implMap); if (handler == null) &#123; throw new RuntimeException(\"没有相应的策略实现\"); &#125; R apply = handler.apply(param); return apply; &#125; @Override @SuppressWarnings(\"unchecked\") public void setApplicationContext(ApplicationContext applicationContext) throws BeansException &#123; log.error(\"AbstractStrategy 执行\"); Class&lt;Object&gt; clazz = getClazz(); Object bean = applicationContext.getBean(getBeanName(), clazz); if (bean == null) &#123; return; &#125; Strategy strategyAnnotation = bean.getClass().getAnnotation(Strategy.class); if (strategyAnnotation == null) &#123; log.error(\"类[&#123;&#125;]没有添加Strategy注解\", clazz); return; &#125; // 模块的名称 String module = strategyAnnotation.module(); Method[] declaredMethods = clazz.getDeclaredMethods(); if (ArrayUtils.isEmpty(declaredMethods)) &#123; throw new RuntimeException(clazz + \"没有相关策略方法\"); &#125; for (Method declaredMethod : declaredMethods) &#123; Strategy annotation = declaredMethod.getAnnotation(Strategy.class); if (annotation == null) &#123; continue; &#125; try &#123; // 用module和 四个 #### 加上 value 组成map的key String key = StringUtils.join(module, DELIMITER, annotation.value()); IStrategyHandler&lt;T, R&gt; handler = (IStrategyHandler&lt;T, R&gt;) declaredMethod.invoke(bean); implMap.put(key, handler); &#125; catch (IllegalAccessException | InvocationTargetException e) &#123; log.error(\"模块[&#123;&#125;]策略处理发生了错误\", module, e); &#125; &#125; &#125;&#125; 订单、产品模块有这种策略模式场景 订单模块的策略模式场景,IStrategyHandler的策略方法都在OrderStrategyImpl中实现。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667@Slf4j@Service@Strategy(module = \"Order\")public class OrderStrategyImpl &#123; @Autowired private BizLocalMessageService bizLocalMessageService; @Strategy(value = \"strategy1\") public IStrategyHandler&lt;BInDto, BOutDto&gt; strategy1() &#123; return bInDto -&gt; &#123; log.error(\"订单模块相关策略 strategy1 executeStart\"); String join = StringUtils.join(\"strategy1:\", bInDto.getParam1(), bInDto.getParam2()); BOutDto bOutDto = new BOutDto(); bOutDto.setOrderNo(join); bOutDto.setPrice(111); SaveBizLocalMessageDto dto = new SaveBizLocalMessageDto(); dto.setBizModule((byte)0); ThreadLocalRandom localRandom = ThreadLocalRandom.current(); dto.setBizNo(localRandom.nextInt(16000,115000)+\"\"); dto.setBizType((byte)0); dto.setMsg(\"\"); dto.setMsgDesc(\"\"); dto.setHandledCount((byte)0); dto.setMaxHandleCount((byte)0); bizLocalMessageService.saveBizLocalMessage(dto); log.error(\"订单模块相关策略 strategy1 executeEnd\"); return bOutDto; &#125;; &#125; @Strategy(value = \"strategy2\") public IStrategyHandler&lt;BInDto, BOutDto&gt; strategy2() &#123; return param -&gt; &#123; log.error(\"订单模块相关策略 strategy2 executeStart\"); String join = StringUtils.join(\"strategy2:\", param.getParam1(), param.getParam2()); BOutDto bOutDto = new BOutDto(); bOutDto.setOrderNo(join); bOutDto.setPrice(11121); SaveBizLocalMessageDto dto = new SaveBizLocalMessageDto(); dto.setBizModule((byte)0); ThreadLocalRandom localRandom = ThreadLocalRandom.current(); dto.setBizNo(localRandom.nextInt(6000,15000)+\"\"); dto.setBizType((byte)0); dto.setMsg(\"\"); dto.setMsgDesc(\"\"); dto.setHandledCount((byte)0); dto.setMaxHandleCount((byte)0); bizLocalMessageService.saveBizLocalMessage(dto); log.error(\"订单模块相关策略 strategy2 executeEnd\"); return bOutDto; &#125;; &#125;&#125; 产品模块的策略模式场景，IStrategyHandler的策略方法都在ProductStrategyImpl中实现 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768@Slf4j@Service@Strategy(module = \"Product\")public class ProductStrategyImpl &#123; @Autowired private BizLocalMessageService bizLocalMessageService; @Strategy(value = \"strategyA\") public IStrategyHandler&lt;BInDto, BOutDto&gt; strategy1() &#123; return bInDto -&gt; &#123; log.error(\"产品模块相关策略 strategy1 executeStart\"); String join = StringUtils.join(\"strategyA:\", bInDto.getParam1(), bInDto.getParam2()); BOutDto bOutDto = new BOutDto(); bOutDto.setOrderNo(join); bOutDto.setPrice(111); SaveBizLocalMessageDto dto = new SaveBizLocalMessageDto(); dto.setBizModule((byte)0); ThreadLocalRandom localRandom = ThreadLocalRandom.current(); dto.setBizNo(localRandom.nextInt(1000,5000)+\"\"); dto.setBizType((byte)0); dto.setMsg(\"\"); dto.setMsgDesc(\"\"); dto.setHandledCount((byte)0); dto.setMaxHandleCount((byte)0); bizLocalMessageService.saveBizLocalMessage(dto); log.error(\"产品模块相关策略 strategy1 executeEnd\"); return bOutDto; &#125;; &#125; @Strategy(value = \"strategyB\") public IStrategyHandler&lt;BInDto, BOutDto&gt; strategy2() &#123; return param -&gt; &#123; log.error(\"产品模块相关策略 strategy2 executeStart\"); String join = StringUtils.join(\"strategyB:\", param.getParam1(), param.getParam2()); BOutDto bOutDto = new BOutDto(); bOutDto.setOrderNo(join); bOutDto.setPrice(11121); SaveBizLocalMessageDto dto = new SaveBizLocalMessageDto(); dto.setBizModule((byte)0); ThreadLocalRandom localRandom = ThreadLocalRandom.current(); dto.setBizNo(localRandom.nextInt(5000,8000)+\"\"); dto.setBizType((byte)0); dto.setMsg(\"\"); dto.setMsgDesc(\"\"); dto.setHandledCount((byte)0); dto.setMaxHandleCount((byte)0); bizLocalMessageService.saveBizLocalMessage(dto); log.error(\"产品模块相关策略 strategy2 executeEnd\"); return bOutDto; &#125;; &#125;&#125; 将存放策略方法的类和策略上下文关联起来 订单模块相关 1234567891011121314@Slf4j@Componentpublic class OrderStrategyContext extends AbstractStrategyContext &#123; @Override Class&lt;OrderStrategyImpl&gt; getClazz() &#123; return OrderStrategyImpl.class; &#125; @Override String getBeanName() &#123; return \"orderStrategyImpl\"; &#125;&#125; 产品模块相关 1234567891011121314@Slf4j@Componentpublic class ProductStrategyContext extends AbstractStrategyContext &#123; @Override Class&lt;ProductStrategyImpl&gt; getClazz() &#123; return ProductStrategyImpl.class; &#125; @Override String getBeanName() &#123; return \"productStrategyImpl\"; &#125;&#125; 代码实现完毕，我们来测试一下 12345678910111213141516171819202122232425262728293031@ContextConfiguration(locations = &#123;\"classpath:spring/spring-dao.xml\",\"classpath:spring/spring-service.xml\"&#125;)@RunWith(value = SpringJUnit4ClassRunner.class)public class StrategyTest &#123; @Autowired private AbstractStrategy&lt;BInDto, BOutDto&gt; orderStrategyContext; @Autowired private AbstractStrategy&lt;BInDto, BOutDto&gt; productStrategyContext; @Test public void productTest()&#123; BInDto bInDto = new BInDto(); bInDto.setParam1(\"产品金额\"); bInDto.setParam2(222); productStrategyContext.execute(\"strategyA\", \"Product\", bInDto); productStrategyContext.execute(\"strategyB\", \"Product\", bInDto); &#125; @Test public void orderTest()&#123; BInDto bInDto = new BInDto(); bInDto.setParam1(\"优惠券金额\"); bInDto.setParam2(180); orderStrategyContext.execute(\"strategy1\", \"Order\", bInDto); orderStrategyContext.execute(\"strategy2\", \"Order\", bInDto); &#125;&#125; 产品模块的测试 订单模块的测试 总结策略接口不是很复杂的情况下 IStrategyHandler 接口可以复用，自己实现 StrategyContext 抽象类，然后再实现自己的策略方法就可以了，不知大家对于这种船新版本有什么看法！","categories":[],"tags":[{"name":"设计模式","slug":"设计模式","permalink":"https://ydstudios.gitee.io/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"},{"name":"策略模式","slug":"策略模式","permalink":"https://ydstudios.gitee.io/tags/%E7%AD%96%E7%95%A5%E6%A8%A1%E5%BC%8F/"},{"name":"反射","slug":"反射","permalink":"https://ydstudios.gitee.io/tags/%E5%8F%8D%E5%B0%84/"},{"name":"lambda表达式","slug":"lambda表达式","permalink":"https://ydstudios.gitee.io/tags/lambda%E8%A1%A8%E8%BE%BE%E5%BC%8F/"},{"name":"注解","slug":"注解","permalink":"https://ydstudios.gitee.io/tags/%E6%B3%A8%E8%A7%A3/"}]},{"title":"UML类图","slug":"UML类图","date":"2021-01-23T14:17:36.000Z","updated":"2021-01-23T07:26:07.000Z","comments":true,"path":"post/122fc4b2.html","link":"","permalink":"https://ydstudios.gitee.io/post/122fc4b2.html","excerpt":"UMLUML(Unified Modeling Language，统一建模语言)是一种为面向对象系统的产品进行说明、可视化和编制文档的一种标准语言，是非专利的第三代建模和规约语言。UML是面向对象设计的建模工具，独立于任何具体程序设计语言。","text":"UMLUML(Unified Modeling Language，统一建模语言)是一种为面向对象系统的产品进行说明、可视化和编制文档的一种标准语言，是非专利的第三代建模和规约语言。UML是面向对象设计的建模工具，独立于任何具体程序设计语言。 UML和设计模式设计模式（Design pattern）是一套被反复使用、多数人知晓的、经过分类编目的、代码设计经验的总结。个人觉得设计模式是前人大佬总结的一种思想，是一种脱离语言并且能够指导程序员更容易写出好的代码的编程习惯。对于一个没有太多经验的程序员，如何写出 简洁优雅，可复用性高，可扩展性强，高内聚低耦合 的代码至关重要。学习别人的设计模式就是为了在没有经验的情况下写出一手不错的代码，只看不写并不能深刻体验到设计模式的巧妙之处，而学会使用UML进行需求分析，是使用设计模式写好代码的前提。 UML图分类 用例图(Use Case Diagram)，描述系统功能； 类图(Class Diagram)，描述系统的静态结构； 对象图(Object Diagram)，描述系统在某个时刻的静态结构； 组件图(Component Diagram)，描述了实现系统的元素的组织； 配置图(Deployment Diagram)，描述了环境元素的配置，并把实现系统的元素映射到配置上； 状态图(State Diagram)，描述了系统元素的状态条件和响应； 时序图(Sequence Diagram)，按时间顺序描述系统元素间的交互； 协作图(Collaboration Diagram)，按照时间和空间顺序描述系统元素间的交互和它们之间的关系； 活动图(Activity Diagram)，描述了系统元素的活动； 其中类图是描述类与类之间的关系的，是UML图中最核心的。 UML作用 为软件系统建立可视化模型； 能够了解系统的结构和能力； 为软件系统建立构件； 为软件系统建立文档； 什么是模型？模型是对现实世界的形状或状态的抽象模拟和简化。为什么要建模？最简单的理由：为了能够更好地理解正在开发的系统。 UML 类图学习设计模式必定需要先读懂 UML 类图，下面就谈谈具体 UML 类图中的概念。类图是用于描述一组类、接口、协作以及它们之间的静态关系。在面向对象系统的建模中，类图是最为常用的图，它用来阐明系统的静态结构。事实上类是对一组具有相同属性、操作、关系和语义的对象的描述，其中对类的属性和操作进行描述时的一个最重要的细节就是它的可见性。 类图的基本符号与含义1、 类(Class)：一般是用三层矩形框表示，第一层表示类的名称，第二层表示的是字段和属性，第三层则是类的方法。第一层中，如果是抽象类，需用斜体显示。 类的属性表达方式 1权限 属性名：类型 [ = 默认值 ] 类的方法表达方式 1权限 方法名称(参数列表) [ : 返回类型] 注意：public 用 + 表示 ，protected 用 # 表示 ，private 用 - 表示 ，default 用 ~ 表示 2、接口(interface)：接口与类图类似，与类图的主要区别就是顶端有&lt;&lt;interface&gt;&gt;显示。 3、类之间的关系类与类一共有如下6种关系：泛化（Generalization）、实现（Realization）、组合（Composition）、聚合（Aggregation）、关联（Association）、依赖 （Dependence）。上述6种关系的强弱如下： 1泛化 = 实现 &gt; 组合 &gt; 聚合 &gt; 关联 &gt; 依赖 类关系图示 泛化（Generalization）：表示的是类之间的继承关系，注意是子类指向父类。UML中用带空心三角箭头的实线表示泛化关系，箭头指向的是父类。 123456789101112131415161718192021222324252627282930313233343536373839public class People &#123; private String name; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125;&#125;public class Student extends People &#123; private Long id; private Double grade; public void study(String course) &#123; System.out.println(\"学习\" + course); &#125; public Long getId() &#123; return id; &#125; public void setId(Long id) &#123; this.id = id; &#125; public Double getGrade() &#123; return grade; &#125; public void setGrade(Double grade) &#123; this.grade = grade; &#125;&#125; 实现(Realization)：实现关系用一条带空心箭头的虚线表示。UML中用带空心三角箭头的虚线表示实现关系，箭头指向的是接口。 12345678910111213141516171819202122public interface IWork &#123; void doWork();&#125;public class People implements IWork&#123; private String name; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; @Override public void doWork() &#123; System.out.println(\"工作\"); &#125;&#125; 组合(Composition)：是聚合的一种特殊形式，表示的是类之间更强的组合关系。他体现的是一种contains-a的关系，这种关系比聚合更强，也称为强聚合；他同样体现整体与部分间的关系，但此时整体与部分是不可分的，整体的生命周期结束也就意味着部分的生命周期结束。UML中用实心菱形头的实线来表示组合，菱形头指向整体。 例如公司由多个部门组成；公司不存在了，部门也将不存在了。 聚合（Aggregation）：用于表示实体对象之间的关系，表示整体由部分构成的语义。与组合关系不同的是，整体和部分不是强依赖的，即使整体不存在了，部分仍然存在；聚合关系是关联关系的一种，耦合度强于关联，他们的代码表现是相同的，仅仅是在语义上有所区别：关联关系的对象间是相互独立的，而聚合关系的对象之间存在着包容关系，他们之间是“整体-个体”的相互关系。UML中用空心菱形头的实线表示聚合关系，菱形头指向整体。 例如一个部门由多个员工组成，部门撤销了，人员不会消失，他们依然存在。 关联(Association)：表示的是类与类之间存在某种特定的对应关系。它是一种静态关系，通常与运行状态无关，一般由常识等因素决定的；关联关系默认不强调方向，表示对象间相互知道。UML中用双向、单向带箭头的虚线表示关联关系，箭头两端为相互关联的两个类。 注：在最终代码中，关联对象通常是以成员变量的形式实现的; 例如乘车人和车票之间就是一种关联关系。 依赖(Dependency)：表示的是类之间的调用关系。与关联关系不同的是，它是一种临时性的关系，通常在运行期间产生，并且随着运行时的变化；依赖关系也可能发生变化；显然，依赖也有方向，双向依赖是一种非常糟糕的结构，我们总是应该保持单向依赖，杜绝双向依赖的产生。UML中用带箭头的虚线表示依赖关系，而箭头所指的则是被依赖的类。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778public class Book &#123; private String name; private String author; private Date publishDate; private BigDecimal price; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; public String getAuthor() &#123; return author; &#125; public void setAuthor(String author) &#123; this.author = author; &#125; public Date getPublishDate() &#123; return publishDate; &#125; public void setPublishDate(Date publishDate) &#123; this.publishDate = publishDate; &#125; public BigDecimal getPrice() &#123; return price; &#125; public void setPrice(BigDecimal price) &#123; this.price = price; &#125;&#125;public class Student extends People &#123; private Long id; private Double grade; public void study(String course) &#123; System.out.println(\"学习\" + course); &#125; /** * Student的类依赖Book类 * */ public void read(Book book) &#123; System.out.println(\"阅读\" + book.getName()); &#125; public Long getId() &#123; return id; &#125; public void setId(Long id) &#123; this.id = id; &#125; public Double getGrade() &#123; return grade; &#125; public void setGrade(Double grade) &#123; this.grade = grade; &#125;&#125; 注：在最终代码中，依赖关系体现为类构造方法及类方法的传入参数，箭头的指向为调用关系；依赖关系除了临时知道对方外，还是“使用”对方的方法和属性； 关联、聚合、组合三种关系用代码表示的话，大多都是一个类是另一个类的成员变量。真正的关系只能配合语义，结合上下文才能够判断出来，而只给出一段代码让我们判断是关联、聚合还是组合关系，则是无法判断的。 [1]参考资料https://zhuanlan.zhihu.com/p/24576502[2]参考资料https://design-patterns.readthedocs.io/zh_CN/latest/read_uml.html","categories":[],"tags":[{"name":"UML","slug":"UML","permalink":"https://ydstudios.gitee.io/tags/UML/"},{"name":"类图","slug":"类图","permalink":"https://ydstudios.gitee.io/tags/%E7%B1%BB%E5%9B%BE/"},{"name":"设计模式","slug":"设计模式","permalink":"https://ydstudios.gitee.io/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}]},{"title":"RocketMQ的消费模式","slug":"RocketMQ的消费模式","date":"2021-01-18T23:32:39.000Z","updated":"2021-01-18T15:41:25.000Z","comments":true,"path":"post/122ee231.html","link":"","permalink":"https://ydstudios.gitee.io/post/122ee231.html","excerpt":"RocketMQ 是基于发布订阅模型的消息中间件。所谓的发布订阅就是说，consumer 订阅了 broker 上的某个 topic，当 producer 发布消息到 broker 上的该 topic 时，consumer 就能收到该条消息。RocketMQ默认支持两种消费模式，分别是集群消费模式和广播消费模式，默认集群消费模式。","text":"RocketMQ 是基于发布订阅模型的消息中间件。所谓的发布订阅就是说，consumer 订阅了 broker 上的某个 topic，当 producer 发布消息到 broker 上的该 topic 时，consumer 就能收到该条消息。RocketMQ默认支持两种消费模式，分别是集群消费模式和广播消费模式，默认集群消费模式。 集群MQ 约定使用相同 Consumer ID 的订阅者属于同一个集群，同一个集群下的订阅者消费逻辑必须完全一致（包括 Tag 的使用），这些订阅者在逻辑上可以认为是一个消费节点。 消费同一类消息的多个 consumer 实例组成一个消费者组，也可以称为一个 consumer 集群，这些 consumer 实例使用同一个 group name。需要注意一点，除了使用同一个 group name，订阅的 tag 也必须是一样的，只有符合这两个条件的 consumer 实例才能组成 consumer 集群。 集群消费模式当使用集群消费模式时，MQ 认为任意一条消息只需要被集群内的任意一个消费者处理即可。 12345678910DefaultMQPushConsumer consumer = new DefaultMQPushConsumer(\"consumer1\"); // 同样也要设置NameServer地址 consumer.setNamesrvAddr(\"127.0.0.1:9876;127.0.0.1:9876\"); // 默认就是集群消费模式consumer.setMessageModel(MessageModel.CLUSTERING); consumer.setConsumeFromWhere(ConsumeFromWhere.CONSUME_FROM_FIRST_OFFSET); // 设置consumer所订阅的Topic和Tag，*代表全部的Tag consumer.subscribe(\"TopicTest\", \"*\"); 适用场景&amp;注意事项 消费端集群化部署，每条消息只需要被处理一次。 由于消费进度在服务端维护，可靠性更高。 集群消费模式下，每一条消息都只会被分发到consumer集群内任意一个的consumer实例消费处理，如果需要被集群下所有的consumer实例消费处理，请使用广播模式。 集群消费模式下，不保证消息的每一次失败重投等逻辑都能路由到同一个consumer实例上，因此处理消息时不应该做任何确定性假设。 广播消费模式当使用广播消费模式时，MQ 会将每条消息推送给集群内所有消费者，保证消息至少被每个消费者消费一次。 12345678910DefaultMQPushConsumer consumer = new DefaultMQPushConsumer(\"consumer1\"); // 同样也要设置NameServer地址 consumer.setNamesrvAddr(\"127.0.0.1:9876;127.0.0.1:9876\"); // 设置为广播消费模式consumer.setMessageModel(MessageModel.BROADCASTING); consumer.setConsumeFromWhere(ConsumeFromWhere.CONSUME_FROM_FIRST_OFFSET); // 设置consumer所订阅的Topic和Tag，*代表全部的Tag consumer.subscribe(\"TopicTest\", \"*\"); 适用场景&amp;注意事项 每条消息都需要被consumer集群内所有的consumer 实例消费一次，也就是说每条消息至少被每一个consumer 实例消费一次。 消费进度在客户端维护，出现重复的概率稍大于集群模式。 广播模式下，MQ 保证每条消息至少被每台客户端消费一次，但是并不会对消费失败的消息进行失败重投，因此业务方需要关注消费失败的情况。 广播模式下，第一次启动时默认从最新消息消费，客户端的消费进度是被持久化在客户端本地的隐藏文件中，因此不建议删除该隐藏文件，否则会丢失部分消息。 广播模式下，每条消息都会被大量的客户端重复处理，因此推荐尽可能使用集群模式。 广播模式下服务端不维护消费进度，所以服务端不提供堆积查询和报警功能。 目前仅 Java 客户端支持广播模式。 虽然广播消费能保证集群内每个consumer实例都能消费消息，但是消费进度的维护、不具备消息重投的机制大大影响了实际的使用。因此，在实际使用中，更推荐使用集群消费，因为集群消费不仅拥有消费进度存储的可靠性，还具有消息重投的机制。而且，我们通过集群消费也可以达到广播消费的效果。 使用集群模式模拟广播模式有些场景希望一个消息需要多个消费者消费，并且也希望消费者消费失败，消息能够重新投递。 订单支付完成通知下游系统，积分服务扣减订单使用的积分、优惠券服务将优惠券状态更新为已使用、库存服务扣减商品库存、仓库准备发货等等 适用场景&amp;注意事项 每条消息都需要被多个消费者处理，每个消费者的逻辑可以相同也可以不一样。 消费进度在服务端维护，可靠性高于广播模式。 如果业务上确实需要使用广播消费，那么我们可以通过创建多个 consumer 实例，每个 consumer 实例属于不同的 consumer group，但是它们都订阅同一个 topic。 举个例子，我们创建 4 个 consumer 实例，consumer1（属于consumerGroup1）、consumer2（属于consumerGroup 2）、consumer 3（属于consumerGroup3）和 consumer4（属于consumerGroup4），它们都订阅了 topicA ，那么当 producer 发送一条消息到 topic A 上时，由于3个consumer 属于不同的 consumer group，所以 3 个consumer都能收到消息，也就达到了广播消费的效果了。 12345678910111213141516171819202122232425262728293031323334353637383940 /** * 设置不同的消费组名，实现集群模式模拟广播消费 * @param consumerGroup 消费组名 * @throws Exception */public static void consumerMessage(String consumerGroup) throws Exception &#123; if (StringUtils.isBlank(consumerGroup))&#123; consumerGroup = \"unique_group_name\"; &#125; // Instantiate with specified consumer group name. DefaultMQPushConsumer consumer = new DefaultMQPushConsumer(consumerGroup); // Specify name server addresses. consumer.setNamesrvAddr(\"127.0.0.1:9876;127.0.0.1:9870\"); // 重要：设置消费者消息最大重试次数 //consumer.setMaxReconsumeTimes(5); // Subscribe one more more topics to consume. consumer.subscribe(\"TopicTest\", \"*\"); consumer.setConsumeFromWhere(ConsumeFromWhere.CONSUME_FROM_FIRST_OFFSET); // Register callback to execute on arrival of messages fetched from brokers. consumer.registerMessageListener((MessageListenerConcurrently) (messages, context) -&gt; &#123; for (MessageExt message : messages) &#123; int reconsumeTimes = message.getReconsumeTimes(); String msgId = message.getMsgId(); SimpleDateFormat simpleDateFormat = new SimpleDateFormat(\"yyyy-MM-dd HH:mm:ss\"); String format = simpleDateFormat.format(new Date()); String messageContext = new String(message.getBody(), StandardCharsets.UTF_8); System.out.println(\"消费时间：\" + format + \" ,消息消费次数=\" + reconsumeTimes + \", msgId=\" + msgId + \", 消息内容:\" + messageContext); &#125; return ConsumeConcurrentlyStatus.CONSUME_SUCCESS; &#125;); //Launch the consumer instance. consumer.start(); &#125; 除此之外，每个 consumer 实例的消费逻辑可以一样也可以不一样，每个consumer group还可以根据需要增加 consumer 实例，比起广播消费来说更加灵活。","categories":[],"tags":[{"name":"rocketmq","slug":"rocketmq","permalink":"https://ydstudios.gitee.io/tags/rocketmq/"},{"name":"消费模式","slug":"消费模式","permalink":"https://ydstudios.gitee.io/tags/%E6%B6%88%E8%B4%B9%E6%A8%A1%E5%BC%8F/"}]},{"title":"系统的数据一致性到底是在说什么","slug":"系统的数据一致性到底是在说什么","date":"2021-01-17T11:58:02.000Z","updated":"2021-02-21T14:50:27.000Z","comments":true,"path":"post/e544327e.html","link":"","permalink":"https://ydstudios.gitee.io/post/e544327e.html","excerpt":"作为一名程序员，你是不是经常在很多场景，例如看博客、聊天吹水等等时候听到这样一个词”系统数据一致性”，是不是有时候感觉到了迷糊，不知道这个”系统数据一致性”到底是在说什么？其实，你可能只是不明白这个词，但是你肯定在实际工作中发现、解决过这样的问题。","text":"作为一名程序员，你是不是经常在很多场景，例如看博客、聊天吹水等等时候听到这样一个词”系统数据一致性”，是不是有时候感觉到了迷糊，不知道这个”系统数据一致性”到底是在说什么？其实，你可能只是不明白这个词，但是你肯定在实际工作中发现、解决过这样的问题。 单体架构下系统数据一致性问题在传统的系统应用中，一般都是使用单体架构来构建系统的。即所有的功能模块都放在一起实现，打成一个WAR包部署在Tomcat中，数据一般存放在关系型数据库中，如MySQL数据库。 前面我说过即使这种单体架构的系统也是数据一致性的问题的，举一个电商下单的例子，用户提交完订单，系统，系统在订单表order表中写入订单金额、用户等相关数据，在订单明细order_item表中写入商品价格、购买的数量等数据，最后更新商品的库存sku信息。用户下单成功之后，系统操作了order、order_item、sku这三个数据表，对于这三个表的操作无论成功与失败，都应该是原子的，操作成功则都要成功，失败则都要一起失败。不然就会出现脏数据，数据一致性被破坏。 1、 如果操作order和order_item表成功，操作sku表失败，则会导致本应该扣减的库存没有扣减，则商品有可能出现超卖。 2、如果操作order和order_item表失败，操作sku表成功，则会导致本不应该扣减的库存扣减了，则商品有可能出现少卖。 3、如果操作order和sku表成功，order_item操作失败，则这个订单数据丢失，订单后续的操作肯定也是操作不了了。 上面只是简单的举了三种可能出现的情况，也可能会有其他的情况发生。那我们怎么避免这些情况的发生呢？其实这种问题稍微有的开发经验的同学都会想到解决方案，那就是使用数据库的事务，事务的原子性保证上述的步骤成功则一起成功，失败则一起失败。 12345BEGIN;INSERT INTO order;INSERT INTO order_item;UPDATE sku;COMMIT; # ROLLBACK 在单体架构的系统下解决内部模块的数据一致性的问题，用数据库的ACID特性就能保证。 单体架构的优点就是相对分布式来说开发简单，功能可以集中管理，模块之间通信没有损耗。但随着业务越来越复杂、需求越来越庞大，人们对系统响应时间、吞吐量和出现故障的时候的系统可用性的要求也越来越高！传统的单体架构系统在这种情况下暴露的缺点也越来越多，人们开始寻求转变。既然部署在一个服务器上的单体架构系统搞不定，那就多部署几台，即用多台单机节点组成集群，再用负载均衡向外提供服务。 但是这样做还是解决不了单体架构存在的一些问题： 只能使用同种语言开发，不能针对不同业务场景利用不同语言的优势开发对应的模块。 系统模块耦合性太强，系统中某一个模块出现问题，例如高并发、大数据场景或者出现bug，整个系统都会受到牵连。 某个模块发布，整个系统都要停机发布，系统所有模块都不能对外提供服务，这样无法快速响应市场需求。 集群负担大，如果想要集群，只能对整个系统进行集群，即使只有一个模块有压力。 集群(Cluster)： 系统单机部署对外服务能力出现瓶颈，则将系统进行多机部署，这些系统对外提供相同的服务，每个单机系统我们称之为节点，多个节点统一起来则可以称之为集群。 分布式架构下系统数据一致性问题天下大事分久必合、合久必分！既然单体架构解决不了问题，那我们就尝试拆分系统，让专业的人做专业的事，那如何进行拆分呢？拆分一般分为水平拆分和垂直拆分。这里说的拆分并不单指数据库拆分，而是所有模块都进行拆分，每个模块都有自己的缓存、数据库等等。 水平拆分指的是单一的节点无法满足性能的需求，需要进行数量上的扩展。每一个节点都具有相同的功能，每一个节点都负责一部分请求，节点们组成一个集群，对外进行提供服务。 垂直拆分指的是按照功能进行拆分，秉着”专业的人干专业的事”，把复杂的系统拆分成各个模块。模块之间通过RPC进行通信，可以做到高内聚、低耦合，每个模块独立部署和维护，可以快速迭代响应市场需求。因此，分布式架构在这种背景下应运而生。 分布式(Distributed)系统是由集中式系统逐渐演变而来。所谓的集中式系统，就是把系统中所用的功能都集中到一起，从而向外提供服务的单体应用。 软件行业是没有银弹的，每一个被发明出来的新技术，都是一把双刃剑，都是在特定的领域解决了某些老问题，但是同时也会带来新的问题。那么微服务这种分布式架构解决了什么老问题？同时它又带来了哪些新问题呢？ 解决了老问题微服务这中分布式架构主要解决了单体架构存在的一些问题。 各个服务可以使用不同的语言开发，可以利用不同语言的优势开发不同模块。 服务之间可以做到高内聚、低耦合。每个服务可以独立维护、部署，可以快速响应市场需求。 可以单独对某个有高并发、大流量的服务单独进行优化，不浪费资源。 带来了新问题 系统的监控难度加大。 数据的一致性成为问题。 系统的复杂度提高，系统的维护、设计成本增加，调试、纠错难度加大。 新问题中的 数据一致性问题才是本文接下来的重点。 为啥会有这个数据一致性问题呢单体架构按照文中的说法，是一种不太时髦的架构方式，都能轻松解决数据一致性问题，新发明的分布式架构却又成了一个棘手的问题，这个到底是技术的进步还是技术在退步呢？哈哈😄😄(我的一点点吐槽)！！接下来我来解释一下为啥分布式系统会有这样的问题。分布式系统每个功能大都部署在不同的服务器上，部署在不同国家和地区的服务器中，部署在不同的网络中，部署在不同国家和地区的网络中。这样一个需要大量的服务器共同协作，向外提供服务的系统，面临着诸多的挑战： 良莠不齐的服务器和系统能力分布式系统中的服务器，可能配置不一样，其上部署的系统可能也是由不同的程序语言、架构实现，因此处理请求的能力也就不一样。 不可靠的网络如上文所说，系统中各个服务可能部署在不同国家和地区，各个服务通过网络进行通信，但是网络是不可靠的。网络经常会出现抖动、延时、分割、丢包等问题。网络通信中最让人头痛的是因为网络抖动、延时等问题导致系统之间的通信出现超时：A服务向B服务发出请求，A服务没有在约定的时间内接受到B服务的响应，你不能确定B服务到底有没有处理完A服务的请求，这样的不确定性就需要我们进行重试处理，那么B服务就要解决请求幂等性问题。 服务器的机房发生火灾、断电等事故。支付宝出现过服务器的电缆被挖断的问题。 普遍存在的单点故障分布式系统为了保证故障发生的时候，系统仍然保证可用，每个模块都采用集群部署。单个节点的故障概率较低，但是节点数量达到一定规模时，系统中的节点出现故障的概率可能就变高了。 分布式系统就是这样一些处在不同区域、有着不同能力和拥有单一功能的服务组成，他们通力合作才能向外提供服务，那如何保证他们的状态、信息一致并且协调有序就成了一个难题。 分布式系统就是要解决解决集中式的单体架构系统的各种缺陷，实现整个系统的 高性能、高可用、可扩展,但是要实现这三个目标并不容易，将系统进行拆分的过程中会出现上文中说到的问题，为了解决这些问题，诞生了很多关于分布式的基本理论，比如CAP、BASE等等。 分布式架构有很多相关的理论和算法，这里我只说了CAP、BASE理论，其他诸如Paxos算法、Raft算法、ZAB协议等等，这些大家自己找资料看看吧！ 我们先来说说CAP理论这个CAP理论相信很多人都听说过，下面请允许我写下教科书般的理论内容： CAP原则又称CAP定理，指的是在一个分布式系统中，Consistency（一致性）、 Availability（可用性）、Partition tolerance（分区容错性），三者不可得兼。 一致性（C）：在分布式系统中的所有数据副本，在同一时刻具有同样的值，也就是等同于所有节点访问同一份最新的数据副本。 假设系统中的某条记录是V0,客户端client向节点1发起一个写操作，将节点1中的值由V0修改成V1。 客户端client接着从节点1读取这条记录，返回的值为V1，这样的情况就叫做满足一致性。 可是客户端client的读取操作，也可能被分配到节点2上，由于节点2上的数据仍然是V0，所以节点2返回的数据为V0。客户端client相同的读取操作，系统却返回两种不同的数据，这样的情况就不满足一致性了。为了消除这种情况，就要在客户端client向节点1写入V1成功之时，节点1把数据同步给节点2，将节点2中的数据也更新成V1，只有这样客户端client相同的读取操作可以得到相同的响应。 可用性（A）：任何时候系统接收到读写请求，必须在正常时间内给出响应，可以返回不是最新版本的数据，但是不能超时timeout或者返回错误error。 分区容忍性（P）：系统中出现消息丢失或者网络分区的问题时，整个系统仍然能对外提供的服务。 什么情况？这个CAP理论上来就给出三个概念或者说是指标，还说分布式系统只能满足上面两个指标。大家是不是经常听到CAP理论，但是却又不是很理解为什么CAP三个指标只能满足其中的两个，那么接下来我给大家解释一下： 就如前面的“分布式架构图”展示的一样，系统一个对外的服务涉及到多个节点通讯和交互，节点所处的网络发生分区故障的问题又无法避免，所以分布式系统中分区容错性必须要考虑，那么系统自然也不可能同时满足上面说的三个指标。 分布式系统中CAP如何抉择在分布式系统内，各种因素导致分区是必然的会发生的，不考虑分区容忍性（P），一旦发生分区错误，整个分布式系统就完全无法使用了，这其实和最开始的单体应用一样有单点问题，这样的系统是和分布式架构理论是相违背的，同时也是不符合实际需要的。所以，对于分布式系统，我们只能能考虑当发生分区错误时，如何选择一致性（C）和可用性（A）。 根据一致性和可用性的选择不同，开源的分布式系统往往又被分为 CP 系统和 AP 系统。当系统在发生分区故障后，客户端的任何请求都被阻塞或者超时，但是，系统的每个节点总是会返回一致的数据，则这样的系统就是 CP 系统，经典的比如 Zookeeper。当系统发生分区故障后，客户端依然可以访问系统，但是获取的数据是不一致的，有的是新的数据，有的还是老数据，那么这样系统就是 AP 系统，经典的比如 Eureka。 前面说分布式系统不考虑分区容忍性（P）为啥分区错误发生，系统就不能用了，这里我再解释一下：不考虑分区容忍性（P），那就是选择CA。假设节点1发生了分区错误，由于可用性（A）的要求，即任何时候系统接收到读写请求，必须在正常时间内给出响应，可以返回不是最新版本的数据，但是不能超时timeout或者返回错误error。因为服务肯定的包含对数据的读取、写入、更新、删除，可是由于一致性（C）的要求，系统中所有的节点数据都要保持一致，所以写入节点1的数据无法同步到节点2，节点1得不到节点2同步数据成功的响应，则节点1可能就不能在合理的时间内,给客户端client返回正确的响应，有可能就是出现错误或者超时。那这样CA就相互矛盾，系统无法保证可用性（A）和一致性（C），系统自然是不能使用了，也就是说没有选择CA的分布式系统，而且这分区容忍性（P）必须要考虑！而且，不是一个系统选择了可用性（A）或者一致性（C），可以是其中的模块选择了可用性（A）或者一致性（C）。 Zookeeper常常有人用它作为dubbo的注册中心，Eureka作为Spring Cloud体系中的注册中心，其实对于注册中心角色来说，我觉得Eureka比Zookeeper更适合！还有一点这里我说一下，其实大部分情况下分布式系统是没有问题的，C和A两个指标都是同时满足的，只是在分区问题发生的情况下，才需要我们考虑到底是选择C还是A。 前文说到解决单点故障的问题，我们引入了集群。在分布式系统中我们为了提高系统的可用性，也是不可避免的使用副本的机制，引入了副本则就需要同步数据到不同的副本，从而引发了副本一致性的问题。就如前面展示的“分布式架构图”中，会员、订单和产品服务都是独立部署且分别使用不同的数据库，每个服务内部又是使用数据库集群，数据在服务与服务之间、在某个服务的数据库集群中间等等的流转、同步，这些过程都是有网络、时间消耗的，一个数据从最开始的产生到它应该到的地方不会瞬时完成，而CAP理论是基于瞬时，在同一时刻任意节点都保持着最新的数据副本，它是忽略网络延迟、节点处理数据的速度的，这个在目前的技术下是不可能做到的，从这个角度来看，CAP理论实在是乐观主义了。 CAP理论的缺点是什么CAP理论其实是有缺点的，前文也提到一些，具体的缺点如下： 理论忽略网络延迟、节点处理数据的速度CAP的理论的作者布鲁尔在定义一致性时，并没有将上述的问题考虑进去。即当事务提交之后，数据能够瞬间复制到所有节点。但实际情况下，数据从产生到复制到各个服务、各个节点，总是需要花费一定时间的。如果在相同机房可能是几毫秒，如果跨地域、跨机房，可能是几十毫秒甚至是一百多毫秒。这也就是说，CAP理论中的C在实践中是不可能完美实现的，在数据副本的同步的过程中，节点之间的数据在一个短时间内并不一致。 理论中的一致性是强一致性CAP理论中的一致性的概念是，在分布式系统中的所有数据备份，在同一时刻是否同样的值，也就是等同于所有节点访问同一份最新的数据副本。在某些场景下这种强一致性要求并不是那么高。在一个日志搜集系统，在高并发、大数据的情况下，一条日志写入需要稍后一会才能在ELK中展示出来，这样是没有问题的。通过牺牲强一致性获得可用性，在一定时间之后最终数据达成一致性即可。 理论中的指标的选择和放弃并不是三选二的关系CAP理论告诉我们三者只能取两个，需要放弃另外一个，这里的放弃是有一定误导作用的，因为“放弃”让很多人理解成什么也不做。实际上，CAP理论的“放弃”只是说在系统分区错误过程中，我们无法同时保证C和A，但并不意味着什么都不做。分区期间放弃C或者A，并不意味着永远放弃C和A，我们可以在分区期间进行一些操作，从而让分区故障解决后，系统能够重新达到CA的状态。最典型的就是主从数据库中主数据挂了，后面进行修复，使得重新达到CA状态。 CAP理论的改进版BASE理论由于CAP理论在定义时过于的乐观，导致他有些缺陷，于是又有大神改进了CAP理论，从而引申出理论改进版本：BASE理论。eBay的架构师Dan Pritchett根据他自身在大规模分布式系统的实践经验，提出了BASE理论。BASE理论是对CAP理论的延伸和补充，它满足CAP理论，通过牺牲强一致性获得可用性，在一定的时间窗口内，达到数据的最终一致性。 BASE理论模型包含如下三个元素： BA：Basically Available，基本可用。 S：Soft State,软状态，状态可以在一定时间内不同步。 E：Eventually Consistent，最终一致性，在一定的时间窗口内，最终数据达成一致即可。 Basically Available 基本可用BASE理论中的Basically Available 基本可用，就是系统在出现问题的时候，牺牲一部分的功能，来保障核心功能正常。这其实就是一种妥协，相当于壁虎断臂求生。就像前几年的双十一淘宝，订单支付、退款直接崩掉了，后面就进行改进限流需要你多试几次才能付款、退款，再后来双十一那几天是不能申请退款的，直接就把你这个功能给关闭了，相当于服务熔断了。这就是牺牲非核心的功能，将所有的资源都用来保障核心的支付功能。 Soft State,软状态允许系统在一定时间内的状态不同步，允许系统处于软状态，这个软状态其实就是中间状态。比如采用分布式架构的电商系统，用户下单完成并付款，是否支付成功，是支付系统完成的，订单系统不会等支付系统返回是否支付成功再把结果返回给客户的，而是先把订单状态设置为付款中，返回给客户，然后支付系统收到异步通知确定支付成功成功，再把状态设置为付款完成，再把付款完成信息推送给订单系统。这样，就可以提高系统的响应速度。即使这支付系统出现故障宕机了，系统重启之后可以通过定时任务补偿处理未完成的数据，然后根据数据所处的状态进行补偿处理，最终完成数据处理。付款中这个状态，就是软状态即中间状态。 Eventually Consistent，最终一致性数据不会一直处在中间状态，就如上面的例子所说，处于中间状态的数据会有采用类似定时任务一样的补偿处理，将数据修复成正确的状态，最终数据达成一致。 重说“带来了新问题”前文说到分布式架构解决了单体架构的一些问题，但是同时也带来了一些新的问题，这里我们着重说一下，本来不是大问题的“数据一致性”问题。前面举了一个电商系统中的经典案例：下订单与扣库存。单体架构的应用我们直接用数据库事务的ACID特性就可解决，但是采用分布式架构的系统就没有那么好解决了，我们先说一下在分布式架构下的系统是如何完成“下订单与扣库存”的，这里就不画图了直接用伪代码来展示： 1234567public void buildOrder(OrderDto orderDto) &#123; // 1.保存订单 orderService.saveOrder(orderDto); // 2.扣除产品库存 inventoryService.deductInventory(orderDto);&#125; 这步骤一保存订单是在订单系统中执行，步骤二扣除产品库存是在库存系统执行，这个下订单与扣库存两个步骤分别涉及到了两个系统，使用RPC的方式和两个系统进行交互。由于这两个步骤不是原子的，不能保持一致的话会导致很多的问题： 比如先保存订单成功，然后扣除产品库存失败，那订单就要回滚处理； 如果先扣除产品库存成功，然后保存订单失败，那库存就要回滚； 或者说先保存订单然后扣除产品库存时请求超时，其实库存已经扣除成功等等问题。 这些问题你不解决，就有可能导致产品多卖或者是产品出现少卖，不管出现哪个都会造成资损或者客诉，任何一种情况都不是我们想发生的。 由于库存系统和订单系统分别使用各自的数据库，那原先使用数据库事务的ACID特性保证数据的一致性就不能奏效了，分布式架构的系统就产生了数据一致性的问题，这种跨多个数据库的事务问题，其实就是分布式事务问题。要解决分布式架构的系统的数据一致性问题，其实就是解决分布式事务的问题。 目前业界也出现了很多分布式事务的解决方案，例如两阶段提交2PC、三阶段提交3PC、TCC还有基于可靠消息等方案，他们用不同的方案实现分布式事务，解决数据一致性的问题，这里就不再详述。 基于可靠消息解决分布式事务，解决数据一致性的问题，其中一种方案叫做本地消息表，这种方案名称大家可能不知道，但是你很有可能这样做过，这个后面的文章再细说。 [参考资料：《分布式服务架构 原理、设计与实践》 李艳鹏 杨彪 著 ]","categories":[],"tags":[{"name":"分布式","slug":"分布式","permalink":"https://ydstudios.gitee.io/tags/%E5%88%86%E5%B8%83%E5%BC%8F/"},{"name":"一致性","slug":"一致性","permalink":"https://ydstudios.gitee.io/tags/%E4%B8%80%E8%87%B4%E6%80%A7/"},{"name":"CAP理论","slug":"CAP理论","permalink":"https://ydstudios.gitee.io/tags/CAP%E7%90%86%E8%AE%BA/"},{"name":"BASE理论","slug":"BASE理论","permalink":"https://ydstudios.gitee.io/tags/BASE%E7%90%86%E8%AE%BA/"}]},{"title":"2020年的年终小结","slug":"2020年的年终小结","date":"2021-01-11T20:06:57.000Z","updated":"2021-01-17T03:48:14.000Z","comments":true,"path":"post/b4e52eb3.html","link":"","permalink":"https://ydstudios.gitee.io/post/b4e52eb3.html","excerpt":"2020年真的是一个神奇年度，这一年作为吃瓜群众的我，见到了太多牛鬼蛇神。","text":"2020年真的是一个神奇年度，这一年作为吃瓜群众的我，见到了太多牛鬼蛇神。 周边环境的不平凡新冠疫情席卷全球，中国政府的封闭管理效果显著，生产活动得到了有序恢复。6月16日在加勒万河谷地区中印发生摩擦事件，印军被威武的解放军暴揍后死亡20人。老美的经济波动，股市破纪录的四次熔断，川建国的美国总统之旅在11月结束。北京时间12月17日1时59分，嫦娥五号返回器在内蒙古四子王旗成功着陆，标志着我国首次地外天体采样返回任务圆满完成。 国内疫情概览 国外疫情概览 个人生活趁着国庆放假又去了一趟合肥，这一次吃上了丈母娘亲手做的红烧鱿鱼，味道真是不错！国庆的第四天和女朋友去了黄山市旅游，爬了莲花峰、去了大峡谷还吃了臭鳜鱼。黄山的风景真是不错，山峰陡峭、景色秀丽。因为想看看黄山的日出，所以我们在山上住了一晚。第二天早上我们急急忙忙的起床，穿上酒店里面厚厚大大棉衣，外面下着小雨、刮着冷风还有大雾，自然也就没有看到日出。听宾馆的服务人员说山上365天有360天都在下雨，你要是能看到日出，证明你的运气真是爆棚！莲花峰拍照留念 工作在2020年的最后几天有幸和几位大佬聊了聊，发现大佬就是大佬，不管是在技术上还是在思想上都远远超过我。大佬有句话关于系统设计的话，我甚是赞同，话的内容大致如下： 业务的诉求转化成产品的需求，产品的需求转化成技术的要求，最终转换成系统的能力。 一句话就说出了系统设计的过程和目的，言简意赅呀！谈话中大佬还指出了我的一些问题，让我以后多多关注一下系统设计，要想法子去提高自己的核心竞争力。我事后也想了想，我在知识总结、系统设计和语言组织能力上都有不足，新的一年要注意这几个方面的锻炼。知识总结这方面，我决定后面多多总结，把知识串联起来，多输出一些博客。语言组织能力我想我平时多注意一下，然后适当的锻炼锻炼后面应该会渐渐变好。系统设计能力这方面的话，只能看看工作上有没有类似的机会了，这个光靠想应该不行，还是得实践见真知！","categories":[],"tags":[{"name":"年终","slug":"年终","permalink":"https://ydstudios.gitee.io/tags/%E5%B9%B4%E7%BB%88/"},{"name":"感悟","slug":"感悟","permalink":"https://ydstudios.gitee.io/tags/%E6%84%9F%E6%82%9F/"}]},{"title":"巧用二进制表示不同的状态","slug":"巧用二进制表示不同的状态","date":"2021-01-06T21:28:00.000Z","updated":"2021-01-07T15:21:55.000Z","comments":true,"path":"post/b7145215.html","link":"","permalink":"https://ydstudios.gitee.io/post/b7145215.html","excerpt":"我们在开发的过程中会遇到这种场景，在系统的权限、选项设置，只有两种状态开启或者关闭。","text":"我们在开发的过程中会遇到这种场景，在系统的权限、选项设置，只有两种状态开启或者关闭。 具体我们以哔哩哔哩的隐私设置为例： 这么几种开关你会怎么设计数据表？按照正常的操作针对一个开关添加字段进行表示。可是后续就会有下面的这样的情况： 哈哈，产品需求迭代增加了后续几种开关，你准备怎么做？还是继续增加字段进行保存么？增加字段进行保存也是可以的，但是感觉上会有那么一点点low！新的开关出现增加字段进行保存，既要修改表结构，又要改实体属性，还是比较麻烦的！所以，对于这种类似的需求，我们要有一个良好的设计来应对，那我们如何解决呢？ 用过linux系统的同学会知道这样一个知识点： 12chomd 777 something# 可读r=4，可写w=2，可执行x=1，即 二进制的001代表可执行，010代表可写，100代表可读 上面的linux命令代表把something的权限修改成所有者、用户组、其他用户都可以进行可读、可写、可执行。 数字 权限 rwx 二进制 7 读 + 写 + 执行 rwx 111 6 读 + 写 rw- 110 5 读 + 执行 r-x 101 4 只读 r– 100 3 写 + 执行 -wx 011 2 只写 -w- 010 1 只执行 –x 001 0 无 — 000 用的是二进制的001代表可执行，010代表可写，100代表可读，那么只需要3位二进制数即可表示这三种状态的混合搭配。借用这种思想，我们来解决上述的问题，我们用一个int类型字段就可以表示上述所有的状态开关。使用2的次幂值代表一种状态，比如我们用 2的零次方 $2^{0}$ = 1 表示打开我的收藏 2的一次方 $2^{1}$ = 2 表示打开追番追剧 2的二次方 $2^{2}$ = 4 表示打开订阅标签 2的三次方 $2^{3}$ = 8 表示打开最近投币的视频 2的四次方 $2^{4}$ = 16 表示打开个人资料 2的五次方 $2^{5}$ = 32 表示打开最近玩过的游戏 博客使用hexo搭建，采用默认的landscape主题不支持LaTeX 公式，所以上面的次方公式显示的有问题。 二进制 数字（十进制） 含义 000 000 0 全部关闭 000 001 1 开启我的收藏 000 011 3 开启我的收藏、追番追剧 000 111 7 开启我的收藏、追番追剧、订阅标签 111 111 63 全部开启 其实就是用bit位表示开启或者关闭，如果是1表示开启，0表示关闭。 123或运算：bit位上有1为1。例如：2 | 1 == 0000 0010 | 0000 0001 == 3与运算：bit位都为1才为1。 例如 5 &amp; 2 == 0000 0101 &amp; 0000 0010 == 0异或运算：0^0=0； 0^1=1； 1^0=1； 1^1=0；即：参加运算的两个对象，如果两个相应位为“异”（值不同），则该位结果为1，否则为0。 那我们就来编码实现上述功能：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657package com.ydstudio.flashsale.module.business.bit;import lombok.AllArgsConstructor;import lombok.Getter;/** * @author 刘洋 Sam * @date 2021/1/7 * @since 1.0.0 */@Getter@AllArgsConstructorpublic enum MemberOption &#123; FAVORITE(1, \"我的收藏\", 1), FOLLOW(2, \"追番追剧\", 2), SUBSCRIBE(3, \"订阅标签\", 4), VIDEO(4, \"最近投币视频\", 8), PROFILE(5, \"个人资料\", 16), GAME(6, \"最近玩的游戏\", 32),; /** * 枚举 */ private Integer code; /** * 描述 */ private String desc; /** * 状态位 */ private Integer tag; /** * 是否有某个tag * * @param tags 拥有的tag集合 * @return */ public boolean hasTag(int tags) &#123; return (tags &amp; this.tag) == this.tag; &#125; public static void main(String[] args) &#123; int tags = 37; // true System.out.println(MemberOption.FAVORITE.hasTag(tags)); // true System.out.println(MemberOption.SUBSCRIBE.hasTag(tags)); // false System.out.println(MemberOption.VIDEO.hasTag(tags)); // true System.out.println(MemberOption.GAME.hasTag(tags)); &#125;&#125; 开启我的收藏和最近玩的游戏，其他选项都关闭，则计算过程如下：1234567891011121314# 我的收藏 tag = 1 最近玩的游戏 tag = 32tags = 1 + 32 = 33 或者采用位或运算 tags = 1 | 32 = 33 /** * 计算状态位 * tags: 已有状态位 * values: 需要添加的状态值 */public static int addTag(int tags, int... values) &#123; for (int value : values) &#123; tags |= value; &#125; return tags;&#125; 关闭最近玩的游戏，则计算过程如下：1234567891011121314上面计算的tags = 33 ，最近玩的游戏 tag = 32关闭最近玩的游戏，tags = 33 ^ 32 = 0010 0001 ^ 0010 0000 = 0000 0001 = 1 /** * 移除状态位 * tags: 已有状态位 * tag: 需要移除的状态值 */public static int delTag(int tags, int tag) &#123; // tags和value与运算不等于tag，说明没有开启tag对应的选项，则直接返回tags if ((tags &amp; tag) != tag) return tags; return tags ^ tag;&#125; 判断是否开启最近玩的游戏，则计算过程如下：12345678910111213上面计算的tags = 33 ，最近玩的游戏 tag = 32 33 &amp; 32 = 0010 0001 &amp; 0010 0000 = 0010 0000 = 32 (0010 0000) /** * 是否有某个tag * * @param tags 拥有的tag集合 * @param tag 待判断的tag * @return */ public boolean hasTag(int tags, int tag) &#123; return (tags &amp; tag) == tag; &#125; 如何用SQL查询开启最近玩的游戏选项的用户12# MySQL也是支持位运算的，其他的数据库应该也是支持的select member_id from table_name where member_option &amp; #&#123;option&#125; = #&#123;option&#125; 就这样我们就可以用一个int型字段保存4*8 = 32个状态位（无符号的情况下）,看起来还是很不错，运算的效率高，只用一个字段可以保存较多的状态。缺点嘛也是有的，就是可读性差，不能很直观的看出到底有哪些状态。其实这种方法可以用一个字段表示多种状态，例如 订单一般有支付状态：待支付 1 、支付中 2 、支付成功4 、支付失败8和发货状态：待发货16、已发货32、在途64、已收货128，这样的话用一个orderStatus来表示这两种状态：支付成功4 + 待发货16 = 4 | 16 = 20，也是可以的但是如上面所说，优缺点都有，看你的实际情况进行选择了！","categories":[],"tags":[{"name":"二进制","slug":"二进制","permalink":"https://ydstudios.gitee.io/tags/%E4%BA%8C%E8%BF%9B%E5%88%B6/"},{"name":"位运算","slug":"位运算","permalink":"https://ydstudios.gitee.io/tags/%E4%BD%8D%E8%BF%90%E7%AE%97/"}]},{"title":"Lua脚本在秒杀场景下的使用","slug":"Lua脚本在秒杀场景下的使用","date":"2020-12-24T22:40:10.000Z","updated":"2021-01-06T13:07:07.000Z","comments":true,"path":"post/d958d490.html","link":"","permalink":"https://ydstudios.gitee.io/post/d958d490.html","excerpt":"在秒杀这种高并发、大流量的场景下，利用数据库进行库存扣减、数据查询这种想法就不要考虑了。你要是有这种想法，劝你早点放弃，P0事故一定在等着你。之前用Lua脚本实现秒杀场景下的库存扣减感觉很不错，配合Redis秒杀效果杠杠的！Redis 2.6 版本通过内嵌支持 Lua 环境，会单线程原子性执行 Lua 脚本，保证 Lua 脚本在处理的过程中不会被任意其它请求打断。","text":"在秒杀这种高并发、大流量的场景下，利用数据库进行库存扣减、数据查询这种想法就不要考虑了。你要是有这种想法，劝你早点放弃，P0事故一定在等着你。之前用Lua脚本实现秒杀场景下的库存扣减感觉很不错，配合Redis秒杀效果杠杠的！Redis 2.6 版本通过内嵌支持 Lua 环境，会单线程原子性执行 Lua 脚本，保证 Lua 脚本在处理的过程中不会被任意其它请求打断。 先说一下大概的情况，在上秒杀的商品的时候，就直接将产品的库存、用户每人的限购的数量等信息直接用Hash数据类型保存在Redis的键goods_商品ID中，不设置缓存有效时间。用户的购买记录用zset保存purchase_goods_商品id中，用商品的ID作为key，用户唯一标识为member,购买的数量为score。相关的命令如下： 12345678910# 初始化一些秒杀产品hmset seckill:goods:1000 goodsId 1000 name 戴森吸尘器V10Motorhead入门款,性价比之选 stock 10 purchaseLimit 1 price 999.99 startSaleTime 1614137304000 endSaleTime 1614140904000 coverImage https://dss2.bdstatic.com/8_V1bjqh_Q23odCf/pacific/1955446237.pnghmset seckill:goods:2000 goodsId 2000 name 戴森无叶风扇 stock 15 purchaseLimit 1 price 1999.99 startSaleTime 1640406504000 endSaleTime 1614140904000 coverImage https://dss2.bdstatic.com/8_V1bjqh_Q23odCf/pacific/1957729018.pnghmset seckill:goods:3000 goodsId 3000 name 戴森智能照明灯 stock 32 purchaseLimit 1 price 299.99 startSaleTime 1640406504000 endSaleTime 1614140904000 coverImage https://dss2.bdstatic.com/8_V1bjqh_Q23odCf/pacific/1947896752.jpghmset seckill:goods:4000 goodsId 4000 name 戴森AM10除菌加湿器 stock 15 purchaseLimit 1 price 1299.99 startSaleTime 1640752104000 endSaleTime 1610166504000 coverImage https://dss2.bdstatic.com/8_V1bjqh_Q23odCf/pacific/1947896752.jpghmset seckill:goods:5000 goodsId 5000 name 苹果AirPodsMax stock 13 purchaseLimit 1 price 2099.99 startSaleTime 1640752104000 endSaleTime 1610166504000 coverImage https://dss2.bdstatic.com/8_V1bjqh_Q23odCf/pacific/1983533493.jpg 具体的Lua脚本编写过程比较简单，稍微熟悉一下就能看懂了，具体内容如下（临时重写的没有优化，凑合着看吧）： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960--KEYS[1] 对应商品的购买记录:purchase_goods_1000--KEYS[2] 商品的key:goods_1000--ARGV[1] 购买人唯一标识memberId:2000--ARGV[2] 购买数量number: 2local goodsExists=redis.call('EXISTS', KEYS[2])if goodsExists == 0then -- 产品不存在 return -1endlocal goodsStockExists=redis.call('hexists', KEYS[2], 'stock')if goodsStockExists == 0 then -- 代表产品库存不存在 return -2end --用户限购数量local purchaseLimitExists=redis.call('hexists', KEYS[2], 'purchaseLimit')if purchaseLimitExists == 0then -- 商品限购数量不存在 return -3end--用户限购数量local purchaseLimit=tonumber(redis.call('hget', KEYS[2], 'purchaseLimit'))--用户已经购买的数量local purchasedNumber=redis.call('ZSCORE', KEYS[1], ARGV[1])--购买数量不存在则默认为0if not purchasedNumberthen purchasedNumber = 0end-- 用户现在要购买的数量local nowPurchasedNumber = tonumber(ARGV[2])-- 用户将要购买的总数量local totalPurchaseNumber = nowPurchasedNumber+purchasedNumberif purchaseLimit &lt; totalPurchaseNumberthen -- 用户购买数量超过了限购数量 return -4endlocal stock=tonumber(redis.call('hget', KEYS[2], 'stock'))if stock &lt;= 0then -- 库存为0 return 1endif stock &lt; nowPurchasedNumberthen -- 库存不足 return 2end--扣减商品库存redis.call('hincrby', KEYS[2], 'stock', -nowPurchasedNumber)-- 更新用户的总购买数量redis.call('zadd',KEYS[1], totalPurchaseNumber, ARGV[1])-- 用户购买成功return 0 尽管Lua会提供字符串和数字的自动转换，但是更好还是转换一下。tonumber就是把字符串数字转换成数字的函数。 redis.call是用来执行redis函数的方法。 上述的脚本可以通过EVAL、EVALSHA或者redis-cli 参数的形式执行。但是上面的脚本已经格式化了，最好还是以后面的方式运行。 Lua的注释是以 – 开头作为单行注释 具体执行演示如下： 123Sam-Mac:Downloads Sam$ rediscli --eval /Users/sam/Downloads/seckill.lua purchase_goods_3000 seckill:goods:1000 , 202 1(integer) 0Sam-Mac:Downloads Sam$ –eval参数是告诉rediscli(这个是自定义redis-cli命令的地址)读取并运行后面的Lua脚本。 /Users/sam/Downloads/seckill.lua 是脚本的位置，后面跟着是传给Lua脚本的参数。 逗号”,”前的purchase_goods_3000 seckill:goods:1000 是要操作的键，可以在脚本中用KEYS[1]、KEYS[2]获取，逗号”,”后面的202和1是参数，在脚本中能够使用ARGV[1]和ARGV[2]获得。 注意”,”逗号两边的空格不能省略，否则会出错 在Redis中调用Lua脚本，也是可以调试的。Redis 从 v3.2.0 开始支持 Lua debugger，可以加断点、print 变量信息、展示正在执行的代码。可以在命令中添加–ldb进行调试，这个参数是开启 Lua dubegger 的意思，这个模式下Redis会 fork 一个进程进入隔离环境，不会影响Redis正常提供服务，但调试期间，原始 redis 执行命令、脚本的结果也不会体现到 fork 之后的隔离环境之中。因此呢，还有另外一种调试模式 –ldb-sync-mode，也就是前面提到的同步模式，这个模式下，会阻塞 redis 上所有的命令、脚本，直到脚本退出，完全模拟了正式环境使用时候的情况，使用的时候务必注意这点。 添加和删除断点很简单，只需使用b 1 2 3 4在第1,2,3,4行添加b 0断点。该命令将删除所有断点。可以使用as参数删除选定的断点，即我们要删除的断点所在的行，但前缀为减号。例如b -3，从第3行中删除断点。 1Sam-Mac:Downloads Sam$ rediscli --ldb --eval /Users/sam/Downloads/seckill.lua purchase_goods_3000 seckill:goods:1000 , 202 1 上面就是今日文章的所以内容，这样看起来是不是觉得Lua脚本也是挺容易的吧！","categories":[],"tags":[{"name":"redis","slug":"redis","permalink":"https://ydstudios.gitee.io/tags/redis/"},{"name":"lua","slug":"lua","permalink":"https://ydstudios.gitee.io/tags/lua/"},{"name":"秒杀","slug":"秒杀","permalink":"https://ydstudios.gitee.io/tags/%E7%A7%92%E6%9D%80/"}]},{"title":"利用MySQL的IGNORE关键字避免重复插入的骚操作你见过么","slug":"MySQL的IGNORE关键字在避免重复插入记录时存在的问题及最佳实践","date":"2020-12-16T21:57:39.000Z","updated":"2021-02-27T08:59:54.000Z","comments":true,"path":"post/cef7abb0.html","link":"","permalink":"https://ydstudios.gitee.io/post/cef7abb0.html","excerpt":"我们在实际开发中总有这样的需求，表里的数据要保持唯一，所以我们开发中经常都是先查询这样的数据有没有，有的话跳过，没有的话再插入，然后在能表示数据记录唯一的列上加上唯一索引进行兜底。虽然有的同学基本上也是这么做的，但是又稍微的做了一点其他的骚操作😄😄😄。","text":"我们在实际开发中总有这样的需求，表里的数据要保持唯一，所以我们开发中经常都是先查询这样的数据有没有，有的话跳过，没有的话再插入，然后在能表示数据记录唯一的列上加上唯一索引进行兜底。虽然有的同学基本上也是这么做的，但是又稍微的做了一点其他的骚操作😄😄😄。 12# 😄😄😄 骚操作, 当有重复记录就会忽略错误返回影响行数0INSERT IGNORE INTO tableName VALUES () 有的同学是这么想的，在高并发的情况下，先查询再插入的话是有并发的问题的，我直接在重复插入的时候屏蔽错误，不用捕获或者抛出异常，直接用 insert 的返回影响行数来判断插入是否成功。这样的做法看上去感觉没啥问题，其实还是有很大的问题的。 语义上的不正确你在表中定义了唯一索引，插入重复的记录直接屏蔽了错误，这就造成异常被吞掉了，导致功能有数据重复的bug就无法通过异常日志来定位。 破坏MySQL主从的数据一致性在说这个IGNORE是如何造成 MySQL 主从数据不一致的原因之前，我们先说说 MySQL 中的一个作为开发很少听说的一个表级锁：AUTO-INC锁（即自增锁）。 在使用 MySQL 的过程中，我们可以为表的某一列添加 AUTO_INCREMENT属性，在插入记录时该列可以不指定，MySQL 会自动为该列赋予一个递增的值。MySQL 自动给AUTO_INCREMENT修饰的列进行递增赋值的实现方式主要有以下两个： 采用 AUTO_INC锁，在执行插入语句时就加一个表级别的AUTO_INC锁，然后为每条待插入的记录分配递增值。在语句执行结束后，再把AUTO_INC锁释放。所以在一个事务持有AUTO_INC锁的过程中，其他插入语句都要阻塞，从而保证一个语句分配的递增值是连续的。 采用一个轻量级的锁，在为INSERT语句生成递增值时获取这个轻量级的锁，然后在生成本次插入语句需要用到的递增值后就把轻量级锁释放掉，不需要等到整个插入语句执行完才释放锁。 如果我们的插入语句在执行前就可以确定具体要插入多少条记录，比如 1INSERT INTO test(a) VALUES ('1'),('b'); 此时我们可以在执行前就确定要插入2条记录，那么一般采用轻量级锁的方式对 AUTO_INCREMENT修饰的列进行赋值，用这种方式可以避免锁定表，可以提高插入性能。 Mysql 提供了一个名为 innodb_atuoinc_lock_mode 的系统变量，来控制 MySQL使用上述的两种方式中的哪一中来为 AUTO_INCREMENT修饰的列进行赋值。 当 innodb_atuoinc_lock_mode 的值为0时，一律采用AUTO_INC锁； 当 innodb_atuoinc_lock_mode 的值为1时，两种方式混合使用，插入记录确定时采用轻量级锁，不确定时使用AUTO_INC锁。 当 innodb_atuoinc_lock_mode 的值为2时，一律采用轻量级锁； 当innodb_autoinc_lock_mode=0时，只有插入成功之后，AUTO_INCREMENT 值才会递增，插入失败不会递增。当innodb_autoinc_lock_mode=1时，无论插入是否成功，AUTO_INCREMENT的值都会递增。当innodb_atuoinc_lock_mode为2时，可能会造成不同事物中的插入语句为AUTO_INCREMENT修饰的列生的值是交叉的，这在主从复制的场景是不安全的。[1] 我查看过我目前负责的系统，MySQL 采用的是主从模式，隔离级别设置的是读已提交(Read Committed)，binlog的格式设置为 ROW， innodb_atuoinc_lock_mode设置的是1，也就是说无论插入成功与否 AUTO_INCREMENT 的值都会递增。在本地搭建的主从模式的 mysql（安装本人博客中之前的文章搭建的 MySQL主从） 中的master库 新建一个如下的表： 查看一下 slave3307数据库中对应的表结构和自增值：确定和 master 一致 在 master库上执行 show master status 查看 master 的binglog 信息： 在 master 库上连续执行下图的三个 SQL，观察 master 与 salve 的情况。 master 的表的AUTO_INCREMENT值已经发生了变化，如下图： master 的 binglog 如下图： 我们从上面两个图可以发现，使用ignore关键字，尽管待插入的记录因为唯一键冲突而没有插入成功，但AUTO_INCREMENT值却递增了，而且 binlog 中也没有 INSERT IGNORE 语句日志，这很明显导致数据库 slave 不会执行该语句，导致数据库 slave数据库中的表的主键和 master 数据库中的表主键不一致！ 我们由此可以猜测一下 IGNORE 的实现机制： 1、尝试把新行插入到表中 ； 2、如果插入成功则返回正常的影响行数；如果唯一键冲突（错误）则忽略该错误，返回影响行数为0 ； InnoDB表insert语句主要分为三种类型： Simple insert(简单插入)可以通过语句预先判断插入的行数。包括不包含子查询的单行、多行 INSERT 和 REPLACE 语句，还有语句INSERT ... ON DUPLICATE KEY UPDATE。 Bulk inserts(大量插入)无法通过语句预先判断插入的行数。包括INSERT ... SELECT、REPLACE ... SELECT &gt;和LOAD DATA语句。InnoDB每处理一行才会为 AUTO_INCREMENT 列分配一个值。 Mixed-mode inserts(混合模式插入)在简单插入语句当中，有的行有为自增列指定值，而有的行没有为自增列指定值。 例如:混合模式插入 12# id列作为自增列 INSERT INTO t1 (id,c1,c2) VALUES (1,1,'a'), (NULL,NULL,'b'), (10,5,'c'), (NULL,NULL,'d') ; 在实际生产环境中业务上是需要当出现唯一键冲突时发生重复要抛出异常而不是吞掉异常，这个前面也有说到。此外，由上面的执行过程可知，我们期望插入的记录因为唯一键冲突而没有插入成功，但AUTO_INCREMENT字段值却递增了。因为插入语句并未执行成功，在binlog中并不会有执行记录，这意味着从库的AUTO_INCREMENT字段值不会递增，即主库和从库的AUTO_INCREMENT值出现了不一致。这带来的问题是，后续如果因为主库发生故障而发生主从切换，从库的AUTO_INCREMENT值落后于主库，就会导致一段时间内在原从库现主库插入的数据在原主库现从库上因为AUTO_INCREMENT主键冲突而导致插入失败，这个肯定是用INSERT IGNORE之前没有想到的，所以建议在实际开发中不要使用此种骚操作! 😄😄😄 [1] 参考《MySQL是怎样运行的》","categories":[],"tags":[{"name":"mysql","slug":"mysql","permalink":"https://ydstudios.gitee.io/tags/mysql/"},{"name":"ignore","slug":"ignore","permalink":"https://ydstudios.gitee.io/tags/ignore/"}]},{"title":"设计模式学习之模板和策略模式实战","slug":"设计模式学习之模板和策略模式实战","date":"2020-12-04T21:41:34.000Z","updated":"2020-12-07T14:02:17.000Z","comments":true,"path":"post/c14af19a.html","link":"","permalink":"https://ydstudios.gitee.io/post/c14af19a.html","excerpt":"前面文章由抽奖活动赠送不同的奖励从而引出了策略模式，定义一个策略接口，不同的奖励发放实现不同的策略。即使后续新增加奖励种类，只要重新实现一个策略即可，符合了设计模式中的开闭原则-对扩展开放对修改关闭。我在一次订单活动功能开发中使用到了策略模式，在开发的过程中发现，订单符合活动条件的判断、发放奖励给用户，可以提升到父类实现，具体的条件判断逻辑可以延迟到子类去实现，从而引出本篇文章的主题：模板模式。","text":"前面文章由抽奖活动赠送不同的奖励从而引出了策略模式，定义一个策略接口，不同的奖励发放实现不同的策略。即使后续新增加奖励种类，只要重新实现一个策略即可，符合了设计模式中的开闭原则-对扩展开放对修改关闭。我在一次订单活动功能开发中使用到了策略模式，在开发的过程中发现，订单符合活动条件的判断、发放奖励给用户，可以提升到父类实现，具体的条件判断逻辑可以延迟到子类去实现，从而引出本篇文章的主题：模板模式。 模板模式在模板模式（Template Pattern）中，一个抽象类公开定义了执行它的方法的方式/模板。它的子类可以按需要重写方法实现，但调用将以抽象类中定义的方式进行，这种类型的设计模式属于行为型模式。模板模式中涉及到在父类实现算法骨架，具体步骤在子类实现，所以必须要有抽象类（Java8中的接口的 default 方法貌似也可以实现）。 介绍 意图：定义一个操作中的算法的骨架，而将一些步骤延迟到子类中。模板方法使得子类可以不改变一个算法的结构即可重定义该算法的某些特定步骤。 主要解决：一些方法通用，却在每一个子类都重新写了这一方法。 何时使用：有一些通用的方法。 如何解决：将这些通用算法抽象出来。 关键代码：在抽象类实现，其他步骤延迟到子类实现。 应用实例： JDK中 ReentrantLock中公平锁和非公平锁的实现 Spring 中对 Hibernate 的支持，将一些已经定好的方法封装起来，比如开启事务、获取 Session、关闭 Session 等，程序员不重复写那些已经规范好的代码，直接丢一个实体就可以保存。 优点： 封装不变部分，扩展可变部分。 提取公共代码，便于维护。 行为由父类控制，子类实现。 缺点： 每一个不同的实现都需要一个子类来实现，导致类的个数增加，使得系统更加庞大。 使用场景： 有多个子类共有的方法，且逻辑相同。 重要的、复杂的方法，可以考虑作为模板方法。 注意事项： 为防止恶意操作，一般模板方法都加上 final 关键词。 本文示例UML图 代码结构图 示例代码1、策略接口，定义了一个活动策略需要实现的方法 123456789101112131415161718192021222324252627282930313233343536373839package com.ydstudio.flashsale.module.pattern.service;import com.ydstudio.flashsale.common.Result;import com.ydstudio.flashsale.module.pattern.dto.ActiveOrderDto;/** * * 活动接口 * * @author Sam * @date 2020/11/26 * @since 1.7.3 */public interface IActiveHandle &#123; /** * 返回活动的类型 * ActiveCategoryEnum 枚举 * * @return */ String getCategory(); /** * 返回活动的详细类型 * ActiveCategoryDetailEnum 枚举 * * @return */ String getCategoryDetail(); /** * 订单检查 * @param temporaryOrderDto 临时订单 * @return Result */ Result checkOrder(ActiveOrderDto temporaryOrderDto);&#125; 2、活动抽象类包含订单规则是否符合的判断以及符合规则之后发送奖励的算法骨架。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758package com.ydstudio.flashsale.module.pattern.service;import com.ydstudio.flashsale.common.Result;import com.ydstudio.flashsale.module.pattern.dto.ActiveOrderDto;import org.springframework.stereotype.Component;/** * 活动抽象类,抽取公共方法, * 把订单是否符合奖励的判断之后发送奖励的公共逻辑在此处实现, * 订单具体条件的判断延迟由子类去实现. * * 策略和模板模式组合使用 * * @author Sam * @date 2020/11/26 * @since 1.7.3 */@Componentpublic abstract class AbstractActiveHandle implements IActiveHandle &#123; /** * 其他抽象方法 */ public abstract void otherMethod(); public void otherMethod1() &#123; System.out.println(\"其他公用方法\"); &#125; /** * 外部真正要调用的方法 * * @param temporaryOrderDto 订单 */ public final String handle(ActiveOrderDto temporaryOrderDto) &#123; // 调用接口中需要子类实现的方法 Result result = checkOrder(temporaryOrderDto); if (!result.isSuccess()) &#123; System.out.println(\"不符合奖励发放条件\"); return \"不符合奖励发放条件\"; &#125; return sendReward(temporaryOrderDto, temporaryOrderDto.getMemberId(), \"积分\"); &#125; /** * 统一的发送奖励的方法 * @param temporaryOrderDto 订单 * @param memberId 用户ID * @param reward 奖励 */ private String sendReward(ActiveOrderDto temporaryOrderDto, long memberId, String reward) &#123; String s = \"给用户\" + memberId + \"的订单\" + temporaryOrderDto + \"发送奖励\" + reward; System.out.println(s); return s; &#125;&#125; 3、 活动策略的实现类 会籍订单策略实现类 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647package com.ydstudio.flashsale.module.pattern.service.impl;import com.ydstudio.flashsale.common.Result;import com.ydstudio.flashsale.module.pattern.dto.ActiveOrderDto;import com.ydstudio.flashsale.module.pattern.service.AbstractActiveHandle;import lombok.extern.slf4j.Slf4j;import org.springframework.stereotype.Service;import java.util.Random;/** * * 会籍订单的处理 * * @author Sam * @date 2020/11/26 * @since 1.7.3 */@Slf4j@Servicepublic class LevelOrderActiveHandle extends AbstractActiveHandle &#123; @Override public void otherMethod() &#123; System.out.println(\"会籍订单的实现\"); &#125; @Override public String getCategory() &#123; return \"2\"; &#125; @Override public String getCategoryDetail() &#123; return \"2\"; &#125; @Override public Result checkOrder(ActiveOrderDto temporaryOrderDto) &#123; Random random = new Random(); int i = random.nextInt(4); if (i &gt;=2)&#123; return new Result(); &#125; return new Result(\"不符合会籍订单活动条件\",\"不符合会籍订单活动条件\"); &#125;&#125; 酒店订单活动策略实现类 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748package com.ydstudio.flashsale.module.pattern.service.impl;import com.ydstudio.flashsale.common.Result;import com.ydstudio.flashsale.module.pattern.dto.ActiveOrderDto;import com.ydstudio.flashsale.module.pattern.service.AbstractActiveHandle;import lombok.extern.slf4j.Slf4j;import org.springframework.stereotype.Service;import java.util.Random;/** * * 酒店订单处理 * * @author Sam * @date 2020/11/26 * @since 1.7.3 */@Slf4j@Servicepublic class OrderActiveHandle extends AbstractActiveHandle &#123; @Override public void otherMethod() &#123; System.out.println(\"酒店订单的实现\"); &#125; @Override public String getCategory() &#123; return \"1\"; &#125; @Override public String getCategoryDetail() &#123; return \"1\"; &#125; @Override public Result checkOrder(ActiveOrderDto temporaryOrderDto) &#123; Random random = new Random(); int i = random.nextInt(4); if (i&lt;=2)&#123; return new Result(); &#125; return new Result(\"不符合酒店订单活动条件\",\"不符合酒店订单活动条件\"); &#125;&#125; 4 、统一入口提供一个Context 给外部使用 调用方直接使用@Autowired 注入ActiveHandleContext即可使用。 12345678910111213141516171819202122232425262728293031323334353637383940package com.ydstudio.flashsale.module.pattern.service;import lombok.extern.slf4j.Slf4j;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.stereotype.Component;import java.util.List;/** * 负责所有活动处理的入口,根据 getImpl(String categoryDetail)类型来判断调用具体的活动策略 * * @author Sam * @date 2020/7/13 * @since 1.6.8 */@Slf4j@Componentpublic class ActiveHandleContext &#123; @Autowired private List&lt;AbstractActiveHandle&gt; activeHandleList; /** * 对外的统一入口 * @param categoryDetail 类型 * @return */ public AbstractActiveHandle getImpl(String categoryDetail) &#123; log.info(\"参数 categoryDetail =[&#123;&#125;] 获取实现 \", categoryDetail); for (AbstractActiveHandle activeHandle : activeHandleList) &#123; if (activeHandle.getCategoryDetail().equals(categoryDetail)) &#123; return activeHandle; &#125; &#125; throw new UnsupportedOperationException(\"没有该实现\"); &#125;&#125; 父类AbstractActiveHandle实现活动规则判断、发放奖励的算法骨架，子类实现具体的规则判断，这样就实现了代码的优化和公用。","categories":[],"tags":[{"name":"java","slug":"java","permalink":"https://ydstudios.gitee.io/tags/java/"},{"name":"设计模式","slug":"设计模式","permalink":"https://ydstudios.gitee.io/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"},{"name":"策略模式","slug":"策略模式","permalink":"https://ydstudios.gitee.io/tags/%E7%AD%96%E7%95%A5%E6%A8%A1%E5%BC%8F/"},{"name":"模板模式","slug":"模板模式","permalink":"https://ydstudios.gitee.io/tags/%E6%A8%A1%E6%9D%BF%E6%A8%A1%E5%BC%8F/"}]},{"title":"IDEA 隐藏.idea与.iml文件","slug":"IDEA-隐藏-idea与-iml文件","date":"2020-12-04T20:56:58.000Z","updated":"2020-12-04T13:15:09.000Z","comments":true,"path":"post/c41bdbee.html","link":"","permalink":"https://ydstudios.gitee.io/post/c41bdbee.html","excerpt":"在 IDEA 中会自动帮我们创建.idea 和.iml 文件，其实这两个文件对我们没啥用的，但是也不能删掉(项目需要而且删了只好还会自动创建)，那我们只好想法子给他隐藏掉。","text":"在 IDEA 中会自动帮我们创建.idea 和.iml 文件，其实这两个文件对我们没啥用的，但是也不能删掉(项目需要而且删了只好还会自动创建)，那我们只好想法子给他隐藏掉。 Windows 系统则依次点击 File —- Setting —- Editor —- File Types Mac系统依次点击Preferences—- Editor —- File Types 在图中红框处添加 .idea 和 *iml，然后保存即可！忽略其他结尾的文件也可以这么操作的。","categories":[],"tags":[{"name":"idea","slug":"idea","permalink":"https://ydstudios.gitee.io/tags/idea/"},{"name":"开发工具","slug":"开发工具","permalink":"https://ydstudios.gitee.io/tags/%E5%BC%80%E5%8F%91%E5%B7%A5%E5%85%B7/"}]},{"title":"设计模式学习之策略模式","slug":"设计模式学习之策略模式","date":"2020-10-24T11:58:02.000Z","updated":"2020-12-06T14:04:33.000Z","comments":true,"path":"post/170b3d6a.html","link":"","permalink":"https://ydstudios.gitee.io/post/170b3d6a.html","excerpt":"策略模式（Strategy Pattern）我觉得在23中常见的设计模式里面是一种比较简单的，使用场景也很多。在策略模式中，一个类的行为或其算法可以在运行时更改。这种类型的设计模式属于行为型模式。在策略模式中，我们创建表示各种策略的对象和一个行为随着策略对象改变而改变的 context 对象,策略对象改变 context 对象的执行算法。","text":"策略模式（Strategy Pattern）我觉得在23中常见的设计模式里面是一种比较简单的，使用场景也很多。在策略模式中，一个类的行为或其算法可以在运行时更改。这种类型的设计模式属于行为型模式。在策略模式中，我们创建表示各种策略的对象和一个行为随着策略对象改变而改变的 context 对象,策略对象改变 context 对象的执行算法。 介绍意图:定义一系列的算法,把它们一个个封装起来, 并且使它们可相互替换。 主要解决：在有多种算法相似的情况下，使用 if…else 所带来的复杂和难以维护。 何时使用：一个系统有许多许多类，而区分它们的只是他们直接的行为。 如何解决：将这些算法封装成一个一个的类，任意地替换。 关键代码：需要实现同一个接口。 应用实例： 诸葛亮的锦囊妙计，每一个锦囊就是一个策略。 旅行的出游方式，选择骑自行车、坐汽车，每一种旅行方式都是一个策略。 JAVA AWT 中的 LayoutManager。 优点： 算法可以自由切换。 避免使用多重条件判断。 扩展性良好。 缺点： 策略类会增多。 所有策略类都需要对外暴露(这个也不一定)。 使用场景： 如果在一个系统里面有许多类，它们之间的区别仅在于它们的行为，那么使用策略模式可以动态地让一个对象在许多行为中选择一种行为。 一个系统需要动态地在几种算法中选择一种。 如果一个对象有很多的行为，如果不用恰当的模式，这些行为就只好使用多重的条件选择语句来实现。 注意： 如果一个系统的策略多于四个，就需要考虑使用混合模式，解决策略类膨胀的问题。 业务场景以抽奖活动发奖为例，奖品多种多样，可能是现金奖，话费奖品，实物奖等等，每种奖品的发放方式都不一样，比如现金是直接转账，话费奖品是调用运营商提供接口发放，实物奖需要人工快递寄送。在未采用策略模式之前，少不了使用 if…else…来判断发放，当增加一种奖品类型时，就需要增加 if 判断。而采取策略模式之后，只需实现一个策略类即可，对原来的逻辑无需做任何改动，也不会影响其他策略的正常逻辑。 UML图 代码实现客户端需要判断要使用哪一个具体的策略类，若还是按照传统的方法 if…else…来判断策略模式就没有意义了，因此策略模式一般都是结合其他模式共同使用。本文中策略类使用 Byte 来标识，也可以在策略类中增加抽象方法，返回值为枚举类型。 定义策略接口 12345678910111213141516171819202122package com.ydstudio.flashsale.service;/** * 策略接口 * * @author 刘洋 Sam * @date 2020/10/24 * @since 1.0.0 */public interface RewardSendStrategy &#123; /** * 发放奖励的类型,通过这个方法来标示不同的策略 * @return */ Byte type(); /** * 方法奖励的方法 */ void sendReward();&#125; 发送现金奖励策略实现类 12345678910111213141516171819202122232425262728package com.ydstudio.flashsale.service.impl;import com.ydstudio.flashsale.common.enums.RewardTypeEnum;import com.ydstudio.flashsale.service.RewardSendStrategy;import lombok.extern.slf4j.Slf4j;import org.springframework.stereotype.Service;/** * 发送现金奖品策略 * * @author 刘洋 Sam * @date 2020/10/24 * @since 1.0.0 */@Slf4j@Servicepublic class CashRewardSendStrategy implements RewardSendStrategy &#123; @Override public Byte type() &#123; return RewardTypeEnum.CASH.getCode(); &#125; @Override public void sendReward() &#123; System.out.println(\"发放现金奖品\"); &#125;&#125; 发放积分奖品策略实现类 12345678910111213141516171819202122232425262728package com.ydstudio.flashsale.service.impl;import com.ydstudio.flashsale.common.enums.RewardTypeEnum;import com.ydstudio.flashsale.service.RewardSendStrategy;import lombok.extern.slf4j.Slf4j;import org.springframework.stereotype.Service;/** * 发放积分奖品策略 * * @author 刘洋 Sam * @date 2020/10/24 * @since 1.0.0 */@Slf4j@Servicepublic class PointRewardSendStrategy implements RewardSendStrategy &#123; @Override public Byte type() &#123; return RewardTypeEnum.POINT.getCode(); &#125; @Override public void sendReward() &#123; System.out.println(\"发放积分奖品\"); &#125;&#125; 发放优惠券奖品策略实现类 12345678910111213141516171819202122232425262728 package com.ydstudio.flashsale.service.impl;import com.ydstudio.flashsale.common.enums.RewardTypeEnum;import com.ydstudio.flashsale.service.RewardSendStrategy;import lombok.extern.slf4j.Slf4j;import org.springframework.stereotype.Service; /** * 发送优惠券策略 * * @author 刘洋 Sam * @date 2020/10/24 * @since 1.0.0 */@Slf4j@Servicepublic class CouponRewardSendStrategy implements RewardSendStrategy &#123; @Override public Byte type() &#123; return RewardTypeEnum.COUPON.getCode(); &#125; @Override public void sendReward() &#123; System.out.println(\"发放优惠券奖品\"); &#125;&#125; 发放谢谢参与策略实现类 12345678910111213141516171819202122232425262728package com.ydstudio.flashsale.service.impl;import com.ydstudio.flashsale.common.enums.RewardTypeEnum;import com.ydstudio.flashsale.service.RewardSendStrategy;import lombok.extern.slf4j.Slf4j;import org.springframework.stereotype.Service;/** * 谢谢参与奖品发放策略 * * @author 刘洋 Sam * @date 2020/10/24 * @since 1.0.0 */@Slf4j@Servicepublic class ThankYouRewardSendStrategy implements RewardSendStrategy &#123; @Override public Byte type() &#123; return RewardTypeEnum.THANK_YOU.getCode(); &#125; @Override public void sendReward() &#123; System.out.println(\"谢谢参与\"); &#125;&#125; 实现对抽象策略封装的上下文对象 通常 JavaWeb 开发使用 Spring 框架的比较多，在这里我们用Spring 提供的@Autowired注解的一个小技巧，来更方便的实现封装。不用再使用 java RewardSendStrategyContext 去实现 java org.springframework.context.ApplicationContextAware 或者每个策略类再实现 java org.springframework.beans.factory接口然后将策略类添加到 RewardSendStrategyContext 中的 属性中。 123456789101112131415161718192021222324252627282930313233343536373839404142434445package com.ydstudio.flashsale.service;import org.springframework.beans.BeansException;import org.springframework.context.ApplicationContext;import org.springframework.context.ApplicationContextAware;import org.springframework.stereotype.Component;import java.util.Map;import java.util.concurrent.ConcurrentHashMap;/** * * @author 刘洋 Sam * @date 2020/10/25 * @since 1.0.0 */@Componentpublic class RewardSendStrategyContext implements ApplicationContextAware &#123; private final static Map&lt;Byte, RewardSendStrategy&gt; strategyMap = new ConcurrentHashMap&lt;&gt;(); @Override public void setApplicationContext(ApplicationContext applicationContext) throws BeansException &#123; Map&lt;String, RewardSendStrategy&gt; beans = applicationContext.getBeansOfType(RewardSendStrategy.class); beans.values().forEach(bean -&gt; strategyMap.put(bean.type(), bean)); &#125; /** * 通过 type 获取奖励实现 * * @param type * @return */ public static RewardSendStrategy getRewardSendStrategy(Byte type) &#123; RewardSendStrategy sendStrategy = strategyMap.get(type); if (sendStrategy == null) &#123; throw new UnsupportedOperationException(\"没有此种奖励的发放实现\"); &#125; return sendStrategy; &#125;&#125; @Autowried 小技巧，看下这个注解的上的注释，我下面来通过举例子说明： 12345678910111213141516171819/* * &lt;p&gt;In case of a &#123;@link java.util.Collection&#125; or &#123;@link java.util.Map&#125; * dependency type, the container will autowire all beans matching the * declared value type. In case of a Map, the keys must be declared as * type String and will be resolved to the corresponding bean names. * */@Target(&#123;ElementType.CONSTRUCTOR, ElementType.FIELD, ElementType.METHOD, ElementType.ANNOTATION_TYPE&#125;)@Retention(RetentionPolicy.RUNTIME)@Documentedpublic @interface Autowired &#123; /** * Declares whether the annotated dependency is required. * &lt;p&gt;Defaults to &#123;@code true&#125;. */ boolean required() default true;&#125; 举例说明： 1234567# 这样定义属性的时候，Spring 会自动的将这个接口的实现类bean全都自动添加到这个 rewardSendStrategyList 中@Autowiredprivate List&lt;RewardSendStrategy&gt; rewardSendStrategyList;# 这样定义属性的时候，Spring 会自动的以实现类 beanName 作为 key，bean 作为 value 添加到这个 Map 中@Autowiredprivate Map&lt;String, RewardSendStrategy&gt; strategyMap; 所以策略类上下文对象应该这样去编码实现： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647package com.ydstudio.flashsale.service;import lombok.extern.slf4j.Slf4j;import org.apache.commons.collections.CollectionUtils;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.stereotype.Service;import java.util.List;import java.util.concurrent.ConcurrentMap;import java.util.stream.Collectors;/** * 发送奖励的 Context 类 * * @author 刘洋 Sam * @date 2020/10/25 * @since 1.0.0 */@Slf4j@Servicepublic class RewardSendStrategyContext &#123; @Autowired private List&lt;RewardSendStrategy&gt; rewardSendStrategyList; /** * 通过 type 获取奖励实现 * * @param type * @return */ public RewardSendStrategy getRewardSendStrategy(Byte type) &#123; if (CollectionUtils.isEmpty(rewardSendStrategyList)) &#123; throw new RuntimeException(\"RewardSendStrategy 注入失败\"); &#125; ConcurrentMap&lt;Byte, RewardSendStrategy&gt; strategyMap = rewardSendStrategyList.stream() .collect(Collectors.toConcurrentMap(RewardSendStrategy::type, RewardSendStrategy -&gt; RewardSendStrategy)); RewardSendStrategy sendStrategy = strategyMap.get(type); if (sendStrategy == null) &#123; throw new UnsupportedOperationException(\"没有此种奖励的发放实现\"); &#125; return sendStrategy; &#125;&#125; 这样写是不是很优雅！！！那调用方如何使用呢？很简单的直接注入就能使用了，哈哈 12345678910@Autowiredprivate RewardSendStrategyContext strategyContext;public void test()&#123; Byte aByte = new Byte(\"1\"); RewardSendStrategy rewardSendStrategy = strategyContext.getRewardSendStrategy(aByte); log.info(\"奖励发放实现 &#123;&#125;\",sendStrategy);&#125; 总结到此策略模式的概念、结构和代码实现都说完了，是不是很简单！大家在撸码的过程中，如果遇到类似的场景可以使用策略模式的时候，一定要尝试去使用策略模式去优化自己代码，这样既锻炼了自己，也写出了牛逼的代码何乐而不为呢？难道你一直想当 CURD boy 么？？？？","categories":[],"tags":[{"name":"java","slug":"java","permalink":"https://ydstudios.gitee.io/tags/java/"},{"name":"设计模式","slug":"设计模式","permalink":"https://ydstudios.gitee.io/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"},{"name":"策略模式","slug":"策略模式","permalink":"https://ydstudios.gitee.io/tags/%E7%AD%96%E7%95%A5%E6%A8%A1%E5%BC%8F/"}]},{"title":"Windows下搭建RocketMQ双主双从集群","slug":"Windows下搭建RocketMQ双主双从集群","date":"2020-10-02T11:06:35.000Z","updated":"2020-10-10T14:35:40.000Z","comments":true,"path":"post/459042a5.html","link":"","permalink":"https://ydstudios.gitee.io/post/459042a5.html","excerpt":"RocketMQ是阿里巴巴2016年MQ中间件，使用Java语言开发，在阿里内部，RocketMQ承接了例如“双11”等高并发场景的消息流转，能够处理万亿级别的消息。我们今天就在 windows下搭建RocketMQ多Master多Slave模式（同步）的集群。","text":"RocketMQ是阿里巴巴2016年MQ中间件，使用Java语言开发，在阿里内部，RocketMQ承接了例如“双11”等高并发场景的消息流转，能够处理万亿级别的消息。我们今天就在 windows下搭建RocketMQ多Master多Slave模式（同步）的集群。 1 准备工作1.1 下载RocketMQRocketMQ版本：4.7.1 下载地址 1.2 环境要求 Linux64位系统 JDK1.8(64位) 源码安装需要安装Maven 3.2.x 2 安装RocketMQ2.1 安装步骤本教程以二进制包方式安装 解压安装包 进入安装目录 2.2 目录介绍 bin：启动脚本，包括shell脚本和CMD脚本 conf：实例配置文件 ，包括broker配置文件、logback配置文件等 lib：依赖jar包，包括Netty、commons-lang、FastJSON等 3 配置mqnamesrv双主在RocketMQ 的配置文件夹的目录下，分别创建文件namesrv-a.properties、namesrv-b.properties 内容分别是 123listenPort=9876listenPort=9870 即表示namesrv-a监听端口是9876，namesrv-b监听端口是9870. 3.1 启动双主namesrv1234start mqnamesrv.cmd -c D:/ProgramFiles/rocketmq-all-4.7.1/conf/namesrv/namesrv-a.propertiesstart mqnamesrv.cmd -c D:/ProgramFiles/rocketmq-all-4.7.1/conf/namesrv/namesrv-b.properties 启动成功会在终端打印 1The Name Server boot success serializeType=JSON 4 配置broker双主双从此次我们采用多Master多Slave模式（同步）的方式，每个Master配置一个Slave，有多对Master-Slave，HA采用同步双写方式，即只有主备都写成功，才向应用返回成功，这种模式的优缺点如下： 优点：数据与服务都无单点故障，Master宕机情况下，消息无延迟，服务可用性与数据可用性都非常高； 缺点：性能比异步复制模式略低（大约低10%左右），发送单个消息的RT会略高，且目前版本在主节点宕机后，备机不能自动切换为主机。 namesrvAddr 我们就不直接写到配置文件里，我们采用在命令行的形式来指定 namesrvAddr 4.1 broker-a 的配置123456789101112131415161718192021222324252627282930313233343536# broker-a 的配置brokerClusterName=DefaultCluster# broker名字，注意此处不同的配置文件填写的不一样brokerName=broker-abrokerId=0#nameServer地址，分号分割#namesrvAddr=127.0.0.1:9876;127.0.0.1:9870#在发送消息时，自动创建服务器不存在的topic，默认创建的队列数defaultTopicQueueNums=4#是否允许 Broker 自动创建Topic，建议线下开启，线上关闭autoCreateTopicEnable=true#是否允许 Broker 自动创建订阅组，建议线下开启，线上关闭autoCreateSubscriptionGroup=true#Broker 对外服务的监听端口,listenPort=10911brokerIP1=127.0.0.1brokerIP2=127.0.0.1deleteWhen=04fileReservedTime=48brokerRole=SYNC_MASTERflushDiskType=ASYNC_FLUSHwaitTimeMillsInSendQueue=500#存储路径storePathRootDir=D:/ProgramFiles/rocketmq-all-4.7.1/store-broker-a/broker-a#commitLog 存储路径storePathCommitLog=D:/ProgramFiles/rocketmq-all-4.7.1/store-broker-a/broker-a/commitlog#消费队列存储路径存储路径storePathConsumeQueue=D:/ProgramFiles/rocketmq-all-4.7.1/store-broker-a/broker-a/consumequeue#消息索引存储路径storePathIndex=D:/ProgramFiles/rocketmq-all-4.7.1/store-broker-a/broker-a/index#checkpoint 文件存储路径storeCheckpoint=D:/ProgramFiles/rocketmq-all-4.7.1/store-broker-a/broker-a/checkpoint#abort 文件存储路径abortFile=D:/ProgramFiles/rocketmq-all-4.7.1/store-broker-a/broker-a/abort 4.2 broker-a-s 的配置1234567891011121314151617181920212223242526# broker-a-s 的配置brokerClusterName=DefaultClusterbrokerName=broker-abrokerId=1listenPort=10931#nameServer地址，分号分割#namesrvAddr=127.0.0.1:9876;127.0.0.1:9870brokerIP1=127.0.0.1brokerIP2=127.0.0.1deleteWhen=04fileReservedTime=48brokerRole=SLAVEflushDiskType=ASYNC_FLUSH#存储路径storePathRootDir=D:/ProgramFiles/rocketmq-all-4.7.1/store-broker-a-s/broker-a-s#commitLog 存储路径storePathCommitLog=D:/ProgramFiles/rocketmq-all-4.7.1/store-broker-a-s/broker-a-s/commitlog#消费队列存储路径存储路径storePathConsumeQueue=D:/ProgramFiles/rocketmq-all-4.7.1/store-broker-a-s/broker-a-s/consumequeue#消息索引存储路径storePathIndex=D:/ProgramFiles/rocketmq-all-4.7.1/store-broker-a-s/broker-a-s/index#checkpoint 文件存储路径storeCheckpoint=D:/ProgramFiles/rocketmq-all-4.7.1/store-broker-a-s/broker-a-s/checkpoint#abort 文件存储路径abortFile=D:/ProgramFiles/rocketmq-all-4.7.1/store-broker-a-s/broker-a-s/abort 4.3 broker-b 的配置1234567891011121314151617181920212223242526272829303132333435# broker-b 的配置brokerClusterName=DefaultClusterbrokerName=broker-bbrokerId=0#nameServer地址，分号分割#namesrvAddr=127.0.0.1:9876;127.0.0.1:9870#在发送消息时，自动创建服务器不存在的topic，默认创建的队列数defaultTopicQueueNums=4#是否允许 Broker 自动创建Topic，建议线下开启，线上关闭autoCreateTopicEnable=true#是否允许 Broker 自动创建订阅组，建议线下开启，线上关闭autoCreateSubscriptionGroup=true#Broker 对外服务的监听端口,listenPort=10951brokerIP1=127.0.0.1brokerIP2=127.0.0.1deleteWhen=04fileReservedTime=48brokerRole=SYNC_MASTERflushDiskType=ASYNC_FLUSHwaitTimeMillsInSendQueue=500#存储路径storePathRootDir=D:/ProgramFiles/rocketmq-all-4.7.1/store-broker-b/broker-b#commitLog 存储路径storePathCommitLog=D:/ProgramFiles/rocketmq-all-4.7.1/store-broker-b/broker-b/commitlog#消费队列存储路径存储路径storePathConsumeQueue=D:/ProgramFiles/rocketmq-all-4.7.1/store-broker-b/broker-b/consumequeue#消息索引存储路径storePathIndex=D:/ProgramFiles/rocketmq-all-4.7.1/store-broker-b/broker-b/index#checkpoint 文件存储路径storeCheckpoint=D:/ProgramFiles/rocketmq-all-4.7.1/store-broker-b/broker-b/checkpoint#abort 文件存储路径abortFile=D:/ProgramFiles/rocketmq-all-4.7.1/store-broker-b/broker-b/abort 4.4 broker-b-s 的配置1234567891011121314151617181920212223242526272829# broker-b-s 的配置brokerClusterName=DefaultClusterbrokerName=broker-bbrokerId=1listenPort=10971#nameServer地址，分号分割#namesrvAddr=127.0.0.1:9876;127.0.0.1:9870# 可以不配置brokerIP1=127.0.0.1brokerIP2=127.0.0.1deleteWhen=04fileReservedTime=48brokerRole=SLAVEflushDiskType=ASYNC_FLUSH#存储路径storePathRootDir=D:/ProgramFiles/rocketmq-all-4.7.1/store-broker-b-s/broker-b-s#commitLog 存储路径storePathCommitLog=D:/ProgramFiles/rocketmq-all-4.7.1/store-broker-b-s/broker-b-s/commitlog#消费队列存储路径存储路径storePathConsumeQueue=D:/ProgramFiles/rocketmq-all-4.7.1/store-broker-b-s/broker-b-s/consumequeue#消息索引存储路径storePathIndex=D:/ProgramFiles/rocketmq-all-4.7.1/store-broker-b-s/broker-b-s/index#checkpoint 文件存储路径storeCheckpoint=D:/ProgramFiles/rocketmq-all-4.7.1/store-broker-b-s/broker-b-s/checkpoint#abort 文件存储路径abortFile=D:/ProgramFiles/rocketmq-all-4.7.1/store-broker-b-s/broker-b-s/abort 4.5 启动双主双从boker1234567start mqbroker.cmd -c D:/ProgramFiles/rocketmq-all-4.7.1/conf/2m-2s-sync/broker-a.properties -n \"127.0.0.1:9876;127.0.0.1:9870\"start mqbroker.cmd -c D:/ProgramFiles/rocketmq-all-4.7.1/conf/2m-2s-sync/broker-a-s.properties -n \"127.0.0.1:9876;127.0.0.1:9870\"start mqbroker.cmd -c D:/ProgramFiles/rocketmq-all-4.7.1/conf/2m-2s-sync/broker-b.properties -n \"127.0.0.1:9876;127.0.0.1:9870\"start mqbroker.cmd -c D:/ProgramFiles/rocketmq-all-4.7.1/conf/2m-2s-sync/broker-b-s.properties -n \"127.0.0.1:9876;127.0.0.1:9870\" 5 配置rocketmq-consoleRocketMQ有一个对其扩展的开源项目incubator-rocketmq-externals，这个项目中有一个子模块叫rocketmq-console，这个便是管理控制台项目了，先将incubator-rocketmq-externals拉到本地，因为我们需要自己对rocketmq-console进行编译打包运行。 5.1 下载并编译打包123git clone https://github.com/apache/rocketmq-externalscd rocketmq-consolemvn clean package -Dmaven.test.skip=true 注意：打包前在rocketmq-console中配置namesrv集群地址： 12server.port=80rocketmq.config.namesrvAddr=127.0.0.1:9876;192.168.25.138:9870 启动rocketmq-console： 1java -jar rocketmq-console-ng-2.0.0.jar 注意 你执行上述命令之后可能得到的 jar 包的名字和我的不一样，毕竟人家也是一直在更新版本，注意看自己对应目录下的文件，适当替换就行。 启动成功后，我们就可以通过浏览器访问http://localhost:80进入集群界面了如下图： 6 在搭建过程中出现的问题6.1 Lock failed,MQ already started12345java.lang.RuntimeException: Lock failed,MQ already started at org.apache.rocketmq.store.DefaultMessageStore.start(DefaultMessageStore.java:227) at org.apache.rocketmq.broker.BrokerController.start(BrokerController.java:853) at org.apache.rocketmq.broker.BrokerStartup.start(BrokerStartup.java:64) at org.apache.rocketmq.broker.BrokerStartup.main(BrokerStartup.java:58) 出现这个问题 是因为我们在集群中master和slave共用一个storePath造成的，这个时候我们要启动的每一个broker要指定不一样的storePath 路径就行，也就是在我们的配置文件中修改即可，完美解决。 6.2 集群状态 SLAVE_NOT_AVAILABLE主从的brokerName 一定要设置成一样的，主brokerName=broker-a 那么从的brokerName=broker-a，不然发送消息会出现 SLAVE_NOT_AVAILABLE 1234567891011# broker名字，注意此处不同的配置文件填写的不一样# broker-a 的配置brokerClusterName=DefaultClusterbrokerName=broker-a# broker-a-s 的配置brokerClusterName=DefaultClusterbrokerName=broker-a 6.3 消息发送出现 [TIMEOUT_CLEAN_QUEUE]broker busy 1org.apache.rocketmq.client.exception.MQBrokerException: CODE: 2 DESC: [TIMEOUT_CLEAN_QUEUE]broker busy, start flow control for a while, period in queue: 205ms, size of queue: 2 解决方式就是合理设置waitTimeMillsInSendQueue的值 12# 清除发送线程池任务队列的等待时间。如果系统时间减去任务放入队列中的时间小于waitTimeMillsInSendQueue，本次请求任务暂时不移除该任务 默认200mswaitTimeMillsInSendQueue=200 到此RocketMQ双主双从集群就搭建完毕。 参考资料参考1参考2参考3","categories":[],"tags":[{"name":"Windows","slug":"Windows","permalink":"https://ydstudios.gitee.io/tags/Windows/"},{"name":"RocketMQ","slug":"RocketMQ","permalink":"https://ydstudios.gitee.io/tags/RocketMQ/"},{"name":"双主双从集群","slug":"双主双从集群","permalink":"https://ydstudios.gitee.io/tags/%E5%8F%8C%E4%B8%BB%E5%8F%8C%E4%BB%8E%E9%9B%86%E7%BE%A4/"}]},{"title":"JavaScript接收后端Long类型数据精度丢失问题","slug":"JavaScript接收后端Long类型数据精度丢失问题","date":"2020-08-26T22:30:22.000Z","updated":"2020-08-27T14:30:42.000Z","comments":true,"path":"post/175ad151.html","link":"","permalink":"https://ydstudios.gitee.io/post/175ad151.html","excerpt":"在开发过程中，通常我们的数据库主键会使用 bigint，它对应 Java 中的 Long，但是当Long的数值超过一定值时传到前端后会丢失精度。原因是JavaScript的number类型有个安全值，即2的53次方，为9007199254740991，如果超过这个值，那么JavaScript会出现不精确的问题。","text":"在开发过程中，通常我们的数据库主键会使用 bigint，它对应 Java 中的 Long，但是当Long的数值超过一定值时传到前端后会丢失精度。原因是JavaScript的number类型有个安全值，即2的53次方，为9007199254740991，如果超过这个值，那么JavaScript会出现不精确的问题。 我们可以直接在浏览器中打印出JavaScript的number类型有个安全值，如下图所示： 如何解决这个问题呢？最简单的办法就是直接后台在返回给前端的时候，把 Long 类型的数据转换成字符串即可。如果自己手动转我想这个会是一个工作量不小的活，那我们怎么安全快速的解决这个问题呢？ 方案一 如果你们是使用jackson框架的话，可以在相应的字段上加上以下的注解（若是用的 FastJson 也有相应的注解） 123456import com.fasterxml.jackson.databind.annotation.JsonSerialize;import com.fasterxml.jackson.databind.ser.std.ToStringSerializer;//序列化成String的传 @JsonSerialize(using = ToStringSerializer.class) private Long id; 方案二 可以自定义Jackson的ObjectMapper 123456789101112public class CustomJacksonObjectMapper extends ObjectMapper &#123; public CustomJacksonObjectMapper() &#123; super(); // 设置日期转换yyyy-MM-dd HH:mm:ss setDateFormat(new SimpleDateFormat(\"yyyy-MM-dd HH:mm:ss\")); // 序列换成json时,将所有的long变成string,因为js中得数字类型不能包含所有的java long值 SimpleModule simpleModule = new SimpleModule(\"LongModule\", new Version(1, 0, 0, \"\")); simpleModule.addSerializer(Long.class, ToStringSerializer.instance); simpleModule.addSerializer(Long.TYPE, ToStringSerializer.instance); registerModule(simpleModule); &#125;&#125; 方案三 前端自己解决，使用第三方库如json-bigint 等","categories":[],"tags":[{"name":"java","slug":"java","permalink":"https://ydstudios.gitee.io/tags/java/"},{"name":"JavaScript","slug":"JavaScript","permalink":"https://ydstudios.gitee.io/tags/JavaScript/"},{"name":"数据精度","slug":"数据精度","permalink":"https://ydstudios.gitee.io/tags/%E6%95%B0%E6%8D%AE%E7%B2%BE%E5%BA%A6/"}]},{"title":"编程过程中抽象类和接口的抉择","slug":"编程过程中抽象类和接口的抉择","date":"2020-08-24T21:51:35.000Z","updated":"2020-10-10T14:35:31.000Z","comments":true,"path":"post/665bcc51.html","link":"","permalink":"https://ydstudios.gitee.io/post/665bcc51.html","excerpt":"作为一位程序员在编码的过程中，终会遇到这种情况：这个功能我到底是用接口呢？还是用抽象类呢？","text":"作为一位程序员在编码的过程中，终会遇到这种情况：这个功能我到底是用接口呢？还是用抽象类呢？ 最近在项目中负责支付功能，由于项目是在原有的基础上，由小程序扩充终端到了App，小程序之前又对接了微信的 jsapi ，所以现在的对接支付平台的情况变为，银联支付宝App 支付、易付宝 App支付、微信jspai支付、微信App支付。（为啥没有对接小程序支付，却对接了 jsapi 支付，那我就不晓得了）。对接一个平台的支付就需要实现下单、异步回调处理、订单查询、订单退款、订单退款查询，于是我就在抽象类和接口之间徘徊，不过短暂的思考过后我还是选择了接口。 那什么时候使用接口？什么时候使用抽象类？ 使用抽象类是为了代码的复用，而使用接口的动机是为了实现多态性。抽象类适合用来定义某个领域的固有属性，也就是本质，接口适合用来定义某个领域的扩展功能。 什么时候使用抽象类? 当2个或多个类中有重复部分的时候，我们可以抽象出来一个基类，如果希望这个基类不能被实例化，就可以把这个基类设计成抽象类。 当需要为一些类提供公共的实现代码时，应优先考虑抽象类。 因为抽象类中的非抽象方法可以被子类继承下来，因为抽象类中的非抽象方法可以被子类继承下来，使实现功能的代码更简单。抽象类只能是单继承的，不能多继承。 什么时候使用接口？ 当注重代码的扩展性跟可维护性时，应当优先采用接口。 接口与实现它的类之间可以不存在任何层次关系，接口可以实现毫不相关类的相同行为，比抽象类的使用更加方便灵活; 接口只关心对象之间的交互的方法，而不关心对象所对应的具体类。接口是程序之间的一个协议，比抽象类的使用更安全、清晰。一般使用接口的情况更多。","categories":[],"tags":[{"name":"编程","slug":"编程","permalink":"https://ydstudios.gitee.io/tags/%E7%BC%96%E7%A8%8B/"},{"name":"抽象类","slug":"抽象类","permalink":"https://ydstudios.gitee.io/tags/%E6%8A%BD%E8%B1%A1%E7%B1%BB/"},{"name":"接口","slug":"接口","permalink":"https://ydstudios.gitee.io/tags/%E6%8E%A5%E5%8F%A3/"}]},{"title":"设计模式学习之责任链模式","slug":"设计模式学习之责任链模式","date":"2020-05-12T22:36:21.000Z","updated":"2020-12-06T04:57:23.000Z","comments":true,"path":"post/8ec2718b.html","link":"","permalink":"https://ydstudios.gitee.io/post/8ec2718b.html","excerpt":"责任链模式顾名思义，责任链模式（Chain of Responsibility Pattern）为请求创建了一个接收者对象的链。这种模式给予请求的类型，对请求的发送者和接收者进行解耦。这种类型的设计模式属于行为型模式。在这种模式中，通常每个接收者都包含对另一个接收者的引用。如果一个对象不能处理该请求，那么它会把相同的请求传给下一个接收者，依此类推。","text":"责任链模式顾名思义，责任链模式（Chain of Responsibility Pattern）为请求创建了一个接收者对象的链。这种模式给予请求的类型，对请求的发送者和接收者进行解耦。这种类型的设计模式属于行为型模式。在这种模式中，通常每个接收者都包含对另一个接收者的引用。如果一个对象不能处理该请求，那么它会把相同的请求传给下一个接收者，依此类推。 介绍意图：避免请求发送者与接收者耦合在一起，让多个对象都有可能接收请求，将这些对象连接成一条链，并且沿着这条链传递请求，直到有对象处理它为止。 主要解决：职责链上的处理者负责处理请求，客户只需要将请求发送到职责链上即可，无须关心请求的处理细节和请求的传递，所以职责链将请求的发送者和请求的处理者解耦了。 何时使用 ：在处理消息的时候以过滤很多道。 如何解决：拦截的类都实现统一接口。 关键代码：Handler 里面聚合它自己，在 HandlerRequest 里判断是否合适，如果没达到条件则向下传递，向谁传递之前 set 进去。 应用实例： 1、红楼梦中的”击鼓传花”。 2、JS 中的事件冒泡。 3、JAVA WEB 中 Apache Tomcat 对 Encoding 的处理，Struts2 的拦截器，jsp servlet 的 Filter。 优点： 降低耦合度。它将请求的发送者和接收者解耦。 简化了对象。使得对象不需要知道链的结构。 增强给对象指派职责的灵活性。通过改变链内的成员或者调动它们的次序，允许动态地新增或者删除责任。 增加新的请求处理类很方便。 缺点： 不能保证请求一定被接收。 系统性能将受到一定影响，而且在进行代码调试时不太方便，可能会造成循环调用。 可能不容易观察运行时的特征，有碍于除错。 使用场景： 有多个对象可以处理同一个请求，具体哪个对象处理该请求由运行时刻自动确定。 在不明确指定接收者的情况下，向多个对象中的一个提交一个请求。 可动态指定一组对象处理请求 实际场景本文以各种电商系统订单实付金额计算为例，订单最终用户所需要付的金额可能是这样的： 应付金额=订单金额-优惠券优惠金额-促销活动优惠金额-会员权益优惠金额 当然也有可能还会增加其他的计算步骤，使用责任链模式来实现订单金额计算，若增加了其他计算步骤，直接将步骤加入到链中即可，而无需改动以前的代码，最大程度减小出错的可能性。责任链分为纯责任链与不纯责任链，在日常开发中，很少有纯的责任链，所谓纯的责任链，就是单个链上处理器要么独立处理，要么处理不了交给下一个处理器进行处理。 本文示例UML图 为了简化示例，代码中关于优惠金额的计算都写固定值。 12345678910111213141516171819202122232425262728293031323334 /** * 封装处理器链处理元素上下文，即待处理的订单 * 省略 get/set 方法 * * @author Sam * @date 2020/5/4 * @since 1.6.5 */public class OrderDto &#123; /** * 当前处理器位于处理器 chain 上的位置 */ private int pos; /** * 订单号 */ private String orderNo; /** * 订单金额 */ private Double amount; /** * VIP 等级 */ private Integer vipLevel; /** * 优惠券 */ private String couponNo;&#125; 123456789101112131415161718192021222324/** * 抽象处理者 * * @author Sam * @date 2020/5/4 * @since 1.6.5 */public abstract class OrderAbstractHandler &#123; /** * 减免金额处理 * * @param orderDto 待处理订单 * @param chain 处理器链 */ protected abstract void doHandle(OrderDto orderDto, OrderHandlerChain chain); /** * 订单处理器的权重 * @return */ protected abstract int weight();&#125; 订单处理器链1234567891011121314151617181920212223242526272829303132333435/** * 订单处理器链 * @author Sam * @date 2020/5/4 * @since 1.6.5 */public class OrderHandlerChain &#123; private List&lt;OrderAbstractHandler&gt; chainList = new ArrayList&lt;&gt;(10); /** * 初始化订单处理器链 * 真实编码的时候尽量不要这样写，这样不够优雅 * 可以实现 ApplicationContextAware 进行 bean 的注入或者其他方式实现自动注入就好 */ public OrderHandlerChain() &#123; chainList.add(new CouponOrderHandler()); chainList.add(new VipOrderHandler()); chainList.add(new SalesOrderHandler()); // 根据处理器的权重，对处理器链中元素进行排序 chainList.sort(Comparator.comparingInt(OrderAbstractHandler::weight)); System.out.println(this.chainList); &#125; public void handle(OrderDto context) &#123; if (context.getPos() &lt; chainList.size()) &#123; OrderAbstractHandler handler = chainList.get(context.getPos()); // 移动位于处理器链中的位置 context.setPos(context.getPos() + 1); handler.doHandle(context, this); &#125; &#125;&#125;` 具体处理实现 优惠券减免金额实现 12345678910111213141516171819202122/** * 优惠券减免金额 * * @author 刘洋 Sam * @date 2020/5/4 * @since 1.6.5 */public class CouponOrderHandler extends OrderAbstractHandler &#123; @Override protected void doHandle(OrderDto context, OrderHandlerChain chain) &#123; if (context.getCouponNo() != null) &#123; context.setAmount(context.getAmount() - 10); &#125; // 调用下一个处理器进行处理 chain.handle(context); &#125; @Override protected int weight() &#123; return 1; &#125;&#125; 会员等级减免金额实现12345678910111213141516171819202122/** * 会员等级减免金额 * * @author Sam * @date 2020/5/4 * @since 1.6.5 */public class VipOrderHandler extends OrderAbstractHandler &#123; @Override protected void doHandle(OrderDto context, OrderHandlerChain chain) &#123; if (context.getVipLevel() &gt; 2) &#123; context.setAmount(context.getAmount() - 5); &#125; // 调用下一个处理器进行处理 chain.handle(context); &#125; @Override protected int weight() &#123; return 2; &#125;&#125; 促销活动优惠金额实现1234567891011121314151617181920212223/** * 促销活动优惠金额 * * @author Sam * @date 2020/5/4 * @since 1.6.5 */public class SalesOrderHandler extends OrderAbstractHandler &#123; @Override protected void doHandle(OrderDto context, OrderHandlerChain chain) &#123; Double amount = context.getAmount(); if (amount != null &amp;&amp; amount &gt; 80d) &#123; context.setAmount(amount * 0.9); &#125; // 调用下一个处理器进行处理 chain.handle(context); &#125; @Override protected int weight() &#123; return 3; &#125;&#125; 测试结果 12345678910111213public class Test &#123; public static void main(String[] args) &#123; OrderHandlerChain orderHandlerChain = new OrderHandlerChain(); OrderDto order = new OrderDto(); order.setOrderNo(\"123\"); order.setAmount(100d); order.setVipLevel(3); order.setCouponNo(\"111\"); orderHandlerChain.handle(order); System.out.println(\"订单最终金额为： \" + order.getAmount()); &#125;&#125; 最终输出结果： 订单最终金额为： 76.5如果删除优惠券，订单的实付金额就又变动了。 总结： 处理器链调用handle方法，逐个调用处理器链中的处理器的doHanle方法，对订单进行处理，当前处理器处理完毕后，可以选择是否继续交由下一个处理器进行处理，即设置chain.handle(context);，如果不需要继续往下处理，不调用此代码即可。 网上流传的代码都是直接在抽象处理器中包含下一个处理器的引用，这样导致在客户端使用的时候，就需要手动去逐个set下级处理器，手误很容易造成处理器死循环的情况，也可能出现缺失某个处理器的情况，因而本文参照Tomcat源码中Filter的作法，引入了Chain类，统一对处理器封装为链，减少客户端使用时出错的可能。 链式处理的好处在于增加减少新的处理器不会影响其他处理器的逻辑，各个处理器之间相互独立，可以减小耦合带来的影响。 参考设计模式之责任链模式实战","categories":[],"tags":[{"name":"java","slug":"java","permalink":"https://ydstudios.gitee.io/tags/java/"},{"name":"设计模式","slug":"设计模式","permalink":"https://ydstudios.gitee.io/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"},{"name":"责任链模式","slug":"责任链模式","permalink":"https://ydstudios.gitee.io/tags/%E8%B4%A3%E4%BB%BB%E9%93%BE%E6%A8%A1%E5%BC%8F/"}]},{"title":"MySQL的bin-log","slug":"MySQL的bin-log","date":"2020-03-15T15:22:12.000Z","updated":"2020-12-18T14:02:34.000Z","comments":true,"path":"post/d1d9a9.html","link":"","permalink":"https://ydstudios.gitee.io/post/d1d9a9.html","excerpt":"什么是binlog？mysql-binlog是MySQL数据库的二进制日志，用于记录用户对数据库操作的SQL语句（除了数据查询语句，即记录insert、update、delete的sql）信息。可以使用mysqlbin命令查看二进制日志的内容。","text":"什么是binlog？mysql-binlog是MySQL数据库的二进制日志，用于记录用户对数据库操作的SQL语句（除了数据查询语句，即记录insert、update、delete的sql）信息。可以使用mysqlbin命令查看二进制日志的内容。 MySQL bin-log 的格式binlog的格式也有三种：STATEMENT、ROW、MIXED 1、 STATMENT模式：基于SQL语句的复制(statement-based replication, SBR)，每一条会修改数据的sql语句会记录到binlog中。优点：不需要记录每一条SQL语句与每行的数据变化，这样子binlog的日志也会比较少，减少了磁盘IO，提高性能。缺点：在某些情况下会导致master-slave中的数据不一致(如sleep()函数， last_insert_id()，以及user-defined functions(udf)等会出现问题) 2、ROW模式：基于行的复制(row-based replication, RBR)：不记录每一条SQL语句的上下文信息，仅需记录哪条数据被修改了，修改成了什么样子了。优点：不会出现某些特定情况下的存储过程、或function、或trigger的调用和触发无法被正确复制的问题。缺点：会产生大量的日志，尤其是alter table的时候会让日志暴涨。 3、MIXED模式：混合模式复制(mixed-based replication, MBR)：以上两种模式的混合使用，一般的复制使用STATEMENT模式保存binlog，对于STATEMENT模式无法复制的操作使用ROW模式保存binlog，MySQL会根据执行的SQL语句选择日志保存方式。 服务器上用哪种模式好呢？我们来举例子说明： insert into xxtable values (x,y,z)为例 影响：1行,且为新增1行, 对于其他行没有影响. 这个情况,用row格式,直接复制磁盘上1行的新增变化。 update xxtable set age=21 where name=’sss’为例影响：一般也只是影响1行. 用row也比较合适。 过年发红包,全公司的人,都涨薪100元.update xxtable set salary=salary+100;影响: 这个语句带来的影响是针对每一行的, 因此磁盘上很多row都发生了变化.此处适合用statment格式的日志. MySQL后来又提供了一个新的混合模式（MIXED），很好理解为上述两种的模式的自由切换。Mysql会根据执行SQL来选择使用STATEMENT还是ROW模式。所以我们用这种模式是最好的，能兼顾两者的优点。 MySQL如何配置bin-log在MySQL配置文件my.cnf文件中的mysqld节中添加下面的配置文件： 123456789101112131415161718[mysqld]#设置日志格式binlog_format = mixed#设置日志路径，注意路经需要mysql用户有权限写log-bin = /data/mysql/logs/mysql-bin.log#设置binlog清理时间expire_logs_days = 7#binlog每个日志文件大小max_binlog_size = 100m#binlog缓存大小binlog_cache_size = 4m#最大binlog缓存大小max_binlog_cache_size = 512m 重启MySQL生效，如果不方便重启服务，也可以直接修改对应的变量即可。 MySQL的bin-log如何查看MySQL的bin-log是二进制格式的无法直接查看，MySQL提供了两种方式进行查看。 1、 简单查看 show binlog events语法 ：1234SHOW BINLOG EVENTS[IN 'log_name'][FROM pos][LIMIT [offset,] row_count] 在MySQL客户端中执行123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119mysql&gt; show binlog events in 'mysql-bin.000004' \\G*************************** 1. row *************************** Log_name: mysql-bin.000004 Pos: 4 Event_type: Format_desc Server_id: 1End_log_pos: 107 Info: Server ver: 5.5.48-log, Binlog ver: 4*************************** 2. row *************************** Log_name: mysql-bin.000004 Pos: 107 Event_type: Query Server_id: 1End_log_pos: 331 Info: use `db01`; CREATE TABLE `student` ( `id` int(11) NOT NULL AUTO_INCREMENT, `student_name` varchar(32) NOT NULL, PRIMARY KEY (`id`)) ENGINE=InnoDB DEFAULT CHARSET=utf8*************************** 3. row *************************** Log_name: mysql-bin.000004 Pos: 331 Event_type: Query Server_id: 1End_log_pos: 399 Info: BEGIN*************************** 4. row *************************** Log_name: mysql-bin.000004 Pos: 399 Event_type: Intvar Server_id: 1End_log_pos: 427 Info: INSERT_ID=1*************************** 5. row *************************** Log_name: mysql-bin.000004 Pos: 427 Event_type: Query Server_id: 1End_log_pos: 563 Info: use `db01`; insert into `db01`.`student` ( `id`, `student_name`) values ( '0', 'jim')*************************** 6. row *************************** Log_name: mysql-bin.000004 Pos: 563 Event_type: Xid Server_id: 1End_log_pos: 590 Info: COMMIT /* xid=22 */*************************** 7. row *************************** Log_name: mysql-bin.000004 Pos: 590 Event_type: Query Server_id: 1End_log_pos: 842 Info: use `db02`; CREATE TABLE `student` ( `id` int(11) NOT NULL AUTO_INCREMENT, `student_name` varchar(32) NOT NULL, `birthday` date NOT NULL, PRIMARY KEY (`id`)) ENGINE=InnoDB DEFAULT CHARSET=utf8*************************** 8. row *************************** Log_name: mysql-bin.000004 Pos: 842 Event_type: Query Server_id: 1End_log_pos: 910 Info: BEGIN*************************** 9. row *************************** Log_name: mysql-bin.000004 Pos: 910 Event_type: Intvar Server_id: 1End_log_pos: 938 Info: INSERT_ID=2*************************** 10. row *************************** Log_name: mysql-bin.000004 Pos: 938 Event_type: Query Server_id: 1End_log_pos: 1077 Info: use `db01`; insert into `db01`.`student` ( `id`, `student_name`) values ( '0', '刘洋')*************************** 11. row *************************** Log_name: mysql-bin.000004 Pos: 1077 Event_type: Xid Server_id: 1End_log_pos: 1104 Info: COMMIT /* xid=128 */*************************** 12. row *************************** Log_name: mysql-bin.000004 Pos: 1104 Event_type: Query Server_id: 1End_log_pos: 1172 Info: BEGIN*************************** 13. row *************************** Log_name: mysql-bin.000004 Pos: 1172 Event_type: Intvar Server_id: 1End_log_pos: 1200 Info: INSERT_ID=3*************************** 14. row *************************** Log_name: mysql-bin.000004 Pos: 1200 Event_type: Query Server_id: 1End_log_pos: 1342 Info: use `db01`; insert into `db01`.`student` ( `id`, `student_name`) values ( '0', '张从容')*************************** 15. row *************************** Log_name: mysql-bin.000004 Pos: 1342 Event_type: Xid Server_id: 1End_log_pos: 1369 Info: COMMIT /* xid=146 */15 rows in set (0.00 sec)mysql&gt; 其实我在14和15行之间，进行了一次Select查询，但是这个操作在bin-log中并没有发现，印证了开头说的bin-log不记录Select查询。 上述出现的参数解释： Log_name:The name of the file that is being listed. Pos：The position at which the event occurs. Event_type：An identifier that describes the event type. Server_id：The server ID of the server on which the event originated. End_log_pos：The position at which the next event begins, which is equal to Pos plus the size of the event. Info：More detailed information about the event type. The format of this information depends on the event type. 1、 详细查看 mysqlbinlog 1语法： mysqlbinlog [bin-log-name] 问题1：无法识别的编码 12Sam-Mac:data Sam$ /usr/local/mysql/bin/mysqlbinlog ./mysql-bin.000004/usr/local/mysql/bin/mysqlbinlog: unknown variable 'default-character-set=utf8' 原因： mysqlbinlog这个工具无法识别binlog中的配置中的default-character-set=utf8mb4这个指令，使用参数–no-defaults 问题2：bin-log日志文件不存在 1/usr/local/mysql/bin/mysqlbinlog: File './mysql-bin.000004' not found (Errcode: 13) 原因：权限问题使用sudo 最终命令： sudo /usr/local/mysql/bin/mysqlbinlog –no-defaults /usr/local/mysql/data/mysql-bin.000004 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697Sam-Mac:data Sam$ sudo /usr/local/mysql/bin/mysqlbinlog --no-defaults /usr/local/mysql/data/mysql-bin.000004/*!50530 SET @@SESSION.PSEUDO_SLAVE_MODE=1*/;/*!40019 SET @@session.max_insert_delayed_threads=0*/;/*!50003 SET @OLD_COMPLETION_TYPE=@@COMPLETION_TYPE,COMPLETION_TYPE=0*/;DELIMITER /*!*/;# at 4#200315 13:18:28 server id 1 end_log_pos 107 Start: binlog v 4, server v 5.5.48-log created 200315 13:18:28 at startup# Warning: this binlog is either in use or was not closed properly.ROLLBACK/*!*/;BINLOG 'pLptXg8BAAAAZwAAAGsAAAABAAQANS41LjQ4LWxvZwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACkum1eEzgNAAgAEgAEBAQEEgAAVAAEGggAAAAICAgCAA=='/*!*/;# at 107#200315 13:21:09 server id 1 end_log_pos 331 Query thread_id=5 exec_time=0 error_code=0use `db01`/*!*/;SET TIMESTAMP=1584249669/*!*/;SET @@session.pseudo_thread_id=5/*!*/;SET @@session.foreign_key_checks=1, @@session.sql_auto_is_null=0, @@session.unique_checks=1, @@session.autocommit=1/*!*/;SET @@session.sql_mode=0/*!*/;SET @@session.auto_increment_increment=1, @@session.auto_increment_offset=1/*!*/;/*!\\C utf8 *//*!*/;SET @@session.character_set_client=33,@@session.collation_connection=33,@@session.collation_server=33/*!*/;SET @@session.lc_time_names=0/*!*/;SET @@session.collation_database=DEFAULT/*!*/;CREATE TABLE `student` ( `id` int(11) NOT NULL AUTO_INCREMENT, `student_name` varchar(32) NOT NULL, PRIMARY KEY (`id`)) ENGINE=InnoDB DEFAULT CHARSET=utf8/*!*/;# at 331#200315 13:22:39 server id 1 end_log_pos 399 Query thread_id=59 exec_time=0 error_code=0SET TIMESTAMP=1584249759/*!*/;BEGIN/*!*/;# at 399#200315 13:22:39 server id 1 end_log_pos 427 IntvarSET INSERT_ID=1/*!*/;# at 427#200315 13:22:39 server id 1 end_log_pos 563 Query thread_id=59 exec_time=0 error_code=0SET TIMESTAMP=1584249759/*!*/;insert into `db01`.`student` ( `id`, `student_name`) values ( '0', 'jim')/*!*/;# at 563#200315 13:22:39 server id 1 end_log_pos 590 Xid = 22COMMIT/*!*/;# at 590#200315 13:25:56 server id 1 end_log_pos 842 Query thread_id=5 exec_time=0 error_code=0use `db02`/*!*/;SET TIMESTAMP=1584249956/*!*/;CREATE TABLE `student` ( `id` int(11) NOT NULL AUTO_INCREMENT, `student_name` varchar(32) NOT NULL, `birthday` date NOT NULL, PRIMARY KEY (`id`)) ENGINE=InnoDB DEFAULT CHARSET=utf8/*!*/;# at 842#200315 16:36:37 server id 1 end_log_pos 910 Query thread_id=1201 exec_time=0 error_code=0SET TIMESTAMP=1584261397/*!*/;BEGIN/*!*/;# at 910#200315 16:36:37 server id 1 end_log_pos 938 IntvarSET INSERT_ID=2/*!*/;# at 938#200315 16:36:37 server id 1 end_log_pos 1077 Query thread_id=1201 exec_time=0 error_code=0use `db01`/*!*/;SET TIMESTAMP=1584261397/*!*/;insert into `db01`.`student` ( `id`, `student_name`) values ( '0', '刘洋')/*!*/;# at 1077#200315 16:36:37 server id 1 end_log_pos 1104 Xid = 128COMMIT/*!*/;# at 1104#200315 16:37:09 server id 1 end_log_pos 1172 Query thread_id=1201 exec_time=0 error_code=0SET TIMESTAMP=1584261429/*!*/;BEGIN/*!*/;# at 1172#200315 16:37:09 server id 1 end_log_pos 1200 IntvarSET INSERT_ID=3/*!*/;# at 1200#200315 16:37:09 server id 1 end_log_pos 1342 Query thread_id=1201 exec_time=0 error_code=0SET TIMESTAMP=1584261429/*!*/;insert into `db01`.`student` ( `id`, `student_name`) values ( '0', '张从容')/*!*/;# at 1342#200315 16:37:09 server id 1 end_log_pos 1369 Xid = 146COMMIT/*!*/;DELIMITER ;# End of log fileROLLBACK /* added by mysqlbinlog */;/*!50003 SET COMPLETION_TYPE=@OLD_COMPLETION_TYPE*/;/*!50530 SET @@SESSION.PSEUDO_SLAVE_MODE=0*/;Sam-Mac:data Sam$ 总结无论是增量备份还是主从复制，都是需要开启mysql-binlog日志，最好跟数据目录设置到不同的磁盘分区，可以降低io等待，提升性能；并且在磁盘故障的时候可以利用mysql-binlog恢复数据。","categories":[],"tags":[{"name":"mysql","slug":"mysql","permalink":"https://ydstudios.gitee.io/tags/mysql/"},{"name":"binlog","slug":"binlog","permalink":"https://ydstudios.gitee.io/tags/binlog/"}]},{"title":"MySQL 删除 relay log 的方法","slug":"MySQL-删除-relay-log-的方法","date":"2020-03-15T14:35:19.000Z","updated":"2020-12-18T13:59:10.000Z","comments":true,"path":"post/da13f861.html","link":"","permalink":"https://ydstudios.gitee.io/post/da13f861.html","excerpt":"方法1 ：自动 设置参数 relay_log_purge = 11234mysql&gt; SET GLOBAL relay_log_purge = 1;Query OK, 0 rows affected (0.00 sec)或者在/etc/my.cnf [mysqld]模块下面增加以下内容relay-log-purge = 1","text":"方法1 ：自动 设置参数 relay_log_purge = 11234mysql&gt; SET GLOBAL relay_log_purge = 1;Query OK, 0 rows affected (0.00 sec)或者在/etc/my.cnf [mysqld]模块下面增加以下内容relay-log-purge = 1 这个参数需要重启数据库才可生效。默认情况下relay-log-purge 是开启的。如果你对从数据库要求不高，可以将relay_log_purge = 1。如果数据库要手动删除relay log请执行 SET GLOBAL relay_log_purge=0。或者在/etc/my.cnf 12# [mysqld]模块下面增加以下内容relay-log-purge = 0 模拟场景：1）从库查看relay log情况 12345678[root@aliyun1212 data]# lltotal 5255240-rw-r----- 1 mysql mysql 56 Aug 18 15:40 auto.cnf-rw-r----- 1 mysql mysql 207 Aug 23 16:06 mysqld-relay-bin.000032-rw-r----- 1 mysql mysql 344 Aug 23 18:14 mysqld-relay-bin.000033-rw-r----- 1 mysql mysql 207 Aug 23 18:14 mysqld-relay-bin.000034-rw-r----- 1 mysql mysql 321 Aug 23 18:14 mysqld-relay-bin.000035-rw-r----- 1 mysql mysql 100 Aug 23 18:14 mysqld-relay-bin.index 2）从库执行以下操作： 12345678910mysql&gt; SET GLOBAL relay_log_purge = 1;Query OK, 0 rows affected (0.00 sec)mysql&gt; show variables like 'relay_log_purge';+-----------------+-------+| Variable_name | Value |+-----------------+-------+| relay_log_purge | ON |+-----------------+-------+1 row in set (0.00 sec) 3）主库执行flush logs; 12mysql&gt; flush logs;Query OK, 0 rows affected (0.12 sec) 生成新的binlog4）从库查看relay log 情况 123456[root@aliyun1212 data]# lltotal 5255232-rw-r----- 1 mysql mysql 56 Aug 18 15:40 auto.cnf-rw-r----- 1 mysql mysql 255 Aug 26 03:16 mysqld-relay-bin.000036-rw-r----- 1 mysql mysql 369 Aug 26 03:16 mysqld-relay-bin.000037-rw-r----- 1 mysql mysql 50 Aug 26 03:16 mysqld-relay-bin.index 可以看到从库的relay log 自动删除只保留最近的两个relay log。说明下如果从库同步出现问题，没有应用的relay log 也不会自动删除relay log。直到解决掉错误，否则relay log 会越挤越多。 方法2：手动 rm -rf 删除reloy log1）查看从库 123456789101112[root@aliyun1212 data]# lltotal 5255256-rw-r----- 1 mysql mysql 56 Aug 18 15:40 auto.cnf-rw-r----- 1 mysql mysql 255 Aug 26 03:16 mysqld-relay-bin.000036-rw-r----- 1 mysql mysql 422 Aug 26 03:27 mysqld-relay-bin.000037-rw-r----- 1 mysql mysql 255 Aug 26 03:27 mysqld-relay-bin.000038-rw-r----- 1 mysql mysql 683 Aug 26 03:27 mysqld-relay-bin.000039-rw-r----- 1 mysql mysql 255 Aug 26 03:27 mysqld-relay-bin.000040-rw-r----- 1 mysql mysql 683 Aug 26 03:27 mysqld-relay-bin.000041-rw-r----- 1 mysql mysql 255 Aug 26 03:27 mysqld-relay-bin.000042-rw-r----- 1 mysql mysql 369 Aug 26 03:27 mysqld-relay-bin.000043-rw-r----- 1 mysql mysql 200 Aug 26 03:27 mysqld-relay-bin.index 2）从库执行以下操作： 12345678910[root@aliyun1212 data]# rm -rf mysqld-relay-bin.000036 mysqld-relay-bin.37 mysqld-relay-bin.38[root@aliyun1212 data]# lltotal 5255244-rw-r----- 1 mysql mysql 56 Aug 18 15:40 auto.cnf-rw-r----- 1 mysql mysql 683 Aug 26 03:27 mysqld-relay-bin.000039-rw-r----- 1 mysql mysql 255 Aug 26 03:27 mysqld-relay-bin.000040-rw-r----- 1 mysql mysql 683 Aug 26 03:27 mysqld-relay-bin.000041-rw-r----- 1 mysql mysql 255 Aug 26 03:27 mysqld-relay-bin.000042-rw-r----- 1 mysql mysql 369 Aug 26 03:27 mysqld-relay-bin.000043-rw-r----- 1 mysql mysql 200 Aug 26 03:27 mysqld-relay-bin.index 从库操作系统层面rm relay log 主库更新数据从库可以继续同步：主库操作： 12mysql&gt; insert into test values(13,'ff');Query OK, 1 row affected (0.44 sec) 从库可以看到数据被同步 12345678910111213141516mysql&gt; select * from test;+----+------+| id | name |+----+------+| 1 | abc || 4 | cc || 5 | dd || 6 | ee || 7 | ee || 8 | ee || 9 | ff || 10 | ff || 11 | ff || 12 | ff || 13 | ff |+----+------+ 如果此时 stop slave然后在执行start slave ,会提示以下错误： 12mysql&gt; start slave;ERROR 1872 (HY000): Slave failed to initialize relay log info structure from the repository 此时要执行步骤33）编辑文件mysqld-relay-bin.index 删除刚才删除的relay log mysqld-relay-bin.000039之前的。 123456[root@aliyun1212 data]# vi mysqld-relay-bin.index ./mysqld-relay-bin.000039./mysqld-relay-bin.000040./mysqld-relay-bin.000041./mysqld-relay-bin.000042./mysqld-relay-bin.000043 再次启动start slave 12mysql&gt; start slave;ERROR 1872 (HY000): Slave failed to initialize relay log info structure from the repository 还是会报错。此时必须重启从库mysql 数据库 123456[root@aliyun1212 ~]# service mysql restartShutting down MySQL.. [ OK ]Starting MySQL.... [ OK ]# 查看从库状态正常mysql&gt; show slave status/G","categories":[],"tags":[{"name":"mysql","slug":"mysql","permalink":"https://ydstudios.gitee.io/tags/mysql/"},{"name":"relay-log","slug":"relay-log","permalink":"https://ydstudios.gitee.io/tags/relay-log/"}]},{"title":"利用Mac电脑自带应用将iPhone投屏到Mac电脑的教程","slug":"利用Mac电脑自带应用将iPhone投屏到Mac电脑的教程","date":"2020-03-10T12:40:32.000Z","updated":"2020-03-10T07:01:03.000Z","comments":true,"path":"post/29afd8a9.html","link":"","permalink":"https://ydstudios.gitee.io/post/29afd8a9.html","excerpt":"你只需要准备一根USB数据线，也就是手机的充电线即可，最好是原装的USB线。","text":"你只需要准备一根USB数据线，也就是手机的充电线即可，最好是原装的USB线。 手机通过USB线连接电脑如果手机是第一次插上这台电脑可能会弹出信任等弹框，手机端点击【信任】，电脑端点击【继续】。总的来说，就是允许电脑访问手机。 打开Mac 系统自带的播放器QuickTime Player直接command + 空格键 调出SpotLight 搜索 “QuickTime”，即可调出QuickTime Player播放器。 菜单栏【文件】–&gt;【新建影片录制】打开【QuickTime Player】后，在左上角的菜单栏中【文件】菜单，找到第一项【新建影片录制】。 更改摄像头选项，选中手机即可。点击【新建影片录制】后，此时默认是打开的电脑自带的摄像头，此时可以更新摄像头的来源和麦克风来源。推荐相机选择iPhone的，麦克风选择内置麦克风。","categories":[],"tags":[{"name":"MAC","slug":"MAC","permalink":"https://ydstudios.gitee.io/tags/MAC/"},{"name":"iPhone","slug":"iPhone","permalink":"https://ydstudios.gitee.io/tags/iPhone/"},{"name":"投屏","slug":"投屏","permalink":"https://ydstudios.gitee.io/tags/%E6%8A%95%E5%B1%8F/"}]},{"title":"JVM配置参数的类型","slug":"JVM配置参数的类型","date":"2020-02-06T15:55:13.000Z","updated":"2020-02-06T07:57:04.000Z","comments":true,"path":"post/ea8be9c5.html","link":"","permalink":"https://ydstudios.gitee.io/post/ea8be9c5.html","excerpt":"参数名称的说明JVM配置参数分别用于跟踪监控JVM状态，分配堆内存以及分配栈内存。按照类型分为三类标准参数、非标准参数和非稳定参数","text":"参数名称的说明JVM配置参数分别用于跟踪监控JVM状态，分配堆内存以及分配栈内存。按照类型分为三类标准参数、非标准参数和非稳定参数 标准参数(-)所有 JVM 都必须支持这些参数的功能, 而且向后兼容, 如: -client : 设置 JVM 使用 client 模式, 特点是启动速度比较快, 但运行时性能和内存管理效率不高. 通常用于客户端应用程序或开发调试; 在32位环境下直接运行 Java 程序默认启用该模式. -server : 设置 JVM 使 server 模式, 特点是启动速度比较慢, 但运行时性能和内存管理效率很高, 适用于生产环境; 在具有64位能力的JDK环境下默认启用该模式. 123456Sam-Mac:hexo-blog Sam$ java -versionjava version \"1.8.0_74\"Java(TM) SE Runtime Environment (build 1.8.0_74-b02)Java HotSpot(TM) 64-Bit Server VM (build 25.74-b02, mixed mode)Sam-Mac:hexo-blog Sam$ 非标准参数(-X)各 JVM 厂商应该都实现这些参数的功能, 但是并不保证所有的 JVM 实现都满足, 且不保证向后兼容。这些用的不多，看看就好。 12345678// 解释执行-Xint// 第一次使用就编译成本地代码-Xcomp// 混合模式-Xmixed 非稳定参数(-XX)此类参数各个JVM实现会有所不同, 将来可能会不被支持, 需要慎重使用。这种类型的参数自己又可分为两种，分别如下： 布尔类型。 -XX:+ 或者 - 某个属性值，+ 表示开启，-表示关闭。例如： 12345// 打印GC详细信息-XX:+PrintGCDetails// 不打印GC详细信息-XX:-PrintGCDetails KV类型。-XX:属性key=属性值value，例如： 12// 设置Metaspace的大小-XX:MetaspaceSize=1024m 注意: 在”-XX:”后的参数若不需要赋值, 即只是用来配置开启或关闭相应选项, 则需要有 “+” (开启) 或 “-“ (禁止) , 否则应用程序将在日志文件 (如 Tomcat 的日志文件 catalina.out ) 中抛出如下错误: 123Missing +/- setting for VM option 'UseConcMarkSweepGC'. Error: Could not create the Java Virtual Machine.Error: A fatal exception has occurred. Program will exit. 可以看出, 这里缺少了 “+/-“ 符号, 导致虚拟机启动异常, 修改后即可正常启动. 我们常设置的JVM参数Xms和Xmx,属于哪种类型的JVM参数？-Xms等价于-XX:InitialHeapSize,-Xmx等价于-XX:MaxHeapSize，所以他们都是XX类型参数。","categories":[],"tags":[{"name":"Jvm","slug":"Jvm","permalink":"https://ydstudios.gitee.io/tags/Jvm/"}]},{"title":"Spring配置文件中的classpath:和classpath*:的区别","slug":"Spring配置文件中的classpath-和classpath-的区别","date":"2020-01-28T22:46:48.000Z","updated":"2020-01-28T14:53:41.000Z","comments":true,"path":"post/38b6491c.html","link":"","permalink":"https://ydstudios.gitee.io/post/38b6491c.html","excerpt":"classpath是spring的写法，classpath是指WEB-INF文件夹下的classes目录，对应源代码src目录，用于存放从src中拷贝过去的各种配置文件，以及编译过后的class文件。Spring可以通过指定classpath*:与classpath:前缀加上路径的方式从classpath加载配置文件。","text":"classpath是spring的写法，classpath是指WEB-INF文件夹下的classes目录，对应源代码src目录，用于存放从src中拷贝过去的各种配置文件，以及编译过后的class文件。Spring可以通过指定classpath*:与classpath:前缀加上路径的方式从classpath加载配置文件。 classpath: 的具体含义classpath:表示从类路径中加载资源，classpath:和classpath:/是等价的，都是相对于类的根路径。资源文件库标准的在文件系统中，也可以在JAR或ZIP的类包中。 classpath*: 的具体含义classpath*:：假设多个JAR包或文件系统类路径都有一个相同的配置文件，classpath:只会在第一个加载的类路径下查找，而classpath:会扫描所有这些JAR包及类路径下出现的同名文件。 具体例子如下 classpath:applicationContext.xml，匹配classpath下的指定文件 classpath:/spring/applicationContext.xml，匹配classpath子目录下的指定文件 classpath:spring-*.xml，匹配classpath下的指定前缀的文件 classpath:*/spring-.xml，匹配classpath下所有目录下的指定前缀的文件 classpath:**/spring-.xml，匹配当前classpath和jar中classpath下所有目录下的指定前缀的文件","categories":[],"tags":[{"name":"spring","slug":"spring","permalink":"https://ydstudios.gitee.io/tags/spring/"}]},{"title":"利用Docker Compose管理服务","slug":"利用Docker-Compose管理服务","date":"2020-01-14T22:17:18.000Z","updated":"2020-01-14T14:57:32.000Z","comments":true,"path":"post/a5d4c4d4.html","link":"","permalink":"https://ydstudios.gitee.io/post/a5d4c4d4.html","excerpt":"docker-compose 简介Compose是用于定义和运行多容器Docker应用程序的工具。通过Compose，您可以使用YML文件来配置应用程序需要的所有服务。然后，使用一个命令，就可以从 YML 文件配置中创建并启动所有服务。","text":"docker-compose 简介Compose是用于定义和运行多容器Docker应用程序的工具。通过Compose，您可以使用YML文件来配置应用程序需要的所有服务。然后，使用一个命令，就可以从 YML 文件配置中创建并启动所有服务。 docker-compose 使用的三个步骤： 使用 Dockerfile 定义应用程序的环境。 使用 docker-compose.yml 定义构成应用程序的服务，这样它们可以在隔离环境中一起运行。 执行 docker-compose up 命令来启动并运行整个应用程序。 docker-compose 安装123#以centos为例yum install -y docker-compose 使用docker-compose编排一个jdk8、tomcat7、mysql5.7和redis3.2的实例准备的材料和目录结构如下： ├── docker-compose.yml└── tomcat7 ├── apache-tomcat-7.0.99.zip ├── Dockerfile ├── jdk-8u131-linux-x64.tar.gz └── my.cnf #挂在到容器的目录├── mysql│ ├── conf│ │ └── my.cnf│ ├── data│ ├── init│ │ ├── init.sh│ │ └── mysql_init.sql│ └── log└── tomcat7 ├── conf │ ├── Catalina │ │ └── localhost │ ├── catalina.policy │ ├── catalina.properties │ ├── context.xml │ ├── logging.properties │ ├── server.xml │ ├── tomcat-users.xml │ ├── tomcat-users.xsd │ └── web.xml ├── data ├── logs └── webapps docker-compose.yml 的内容如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748version: \"3\"services: samjavawebenv: container_name: javawebenv build: context: ./tomcat7/ dockerfile: Dockerfile image: compose-javawebenv:v1 depends_on: - mysql links: - mysql - redis ports: - \"80:80\" - \"443:443\" - \"8080:8080\" privileged: true environment: - Dspring.profiles.active=prod restart: always volumes: - \"../../docker-compose/tomcat7/conf:/usr/local/tomcat7/conf\" - \"../../docker-compose/tomcat7/data:/usr/local/tomcat7/data\" - \"../../docker-compose/tomcat7/logs:/usr/local/tomcat7/logs\" - \"../../docker-compose/tomcat7/webapps:/usr/local/tomcat7/webapps\" mysql: container_name: mysql5.7 environment: MYSQL_ROOT_HOST: \"%\" MYSQL_HOST: \"localhost\" MYSQL_ROOT_PASSWORD: \"xxxx\" image: \"mysql:5.7\" ports: - \"3306:3306\" restart: always volumes: - \"../../docker-compose/mysql/data:/var/lib/mysql\" - \"../../docker-compose/mysql/conf:/etc/mysql/conf.d\" - \"../../docker-compose/mysql/init:/docker-entrypoint-initdb.d\" command: --default-authentication-plugin=mysql_native_password redis: container_name: redis3.2 image: \"redis:3.2\" ports: - \"6380:6379\" restart: always command: \"redis-server --appendonly yes\" init.sh中的内容 123#!/bin/bashmysql -uroot -pxxx &lt;&lt; EOFsource /docker-entrypoint-initdb.d/mysql_init.sql; mysql_init.sql中的内容 123456789101112use mysql;update user set authentication_string=password('xxxx') where user='root' and host='localhost'; INSERT INTO `mysql`.`user` (`Host`, `User`, `Select_priv`, `Insert_priv`, `Update_priv`, `Delete_priv`, `Create_priv`, `Drop_priv`, `Reload_priv`, `Shutdown_priv`, `Process_priv`, `File_priv`, `Grant_priv`, `References_priv`, `Index_priv`, `Alter_priv`, `Show_db_priv`, `Super_priv`, `Create_tmp_table_priv`, `Lock_tables_priv`, `Execute_priv`, `Repl_slave_priv`, `Repl_client_priv`, `Create_view_priv`, `Show_view_priv`, `Create_routine_priv`, `Alter_routine_priv`, `Create_user_priv`, `Event_priv`, `Trigger_priv`, `Create_tablespace_priv`, `ssl_type`, `ssl_cipher`, `x509_issuer`, `x509_subject`, `max_questions`, `max_updates`, `max_connections`, `max_user_connections`, `plugin`, `authentication_string`, `password_expired`, `password_last_changed`, `password_lifetime`, `account_locked`) VALUES ('%', 'xx', 'Y', 'Y', 'Y', 'Y', 'Y', 'Y', 'Y', 'Y', 'Y', 'Y', 'Y', 'Y', 'Y', 'Y', 'Y', 'Y', 'Y', 'Y', 'Y', 'Y', 'Y', 'Y', 'Y', 'Y', 'Y', 'Y', 'Y', 'Y', 'Y', '', '', '', '', '0', '0', '0', '0', 'mysql_native_password', password('xxx'), 'N', '2020-01-09 11:41:17', NULL, 'N');flush privileges; 容器间访问mysql和redis的链接地址要修改 1234# 把主机地址换成service的名字jbdc.url=jdbc:mysql://mysql:3306/flash_sale?useUnicode=true&amp;characterEncoding=utf-8#mysql的用户主机为 javawebenv.compose_default 即 container_name + compose_default docker-compose 命令1234567891011121314#构建docker-compose build# 启动docker-compose up#如果你想在后台执行该服务可以加上 -d 参数docker-compose up -d#关闭docker-compose stop#删除docker-compose rm 以上的docker-compose命令都是在 docker-compose.yml 文件目录下执行","categories":[],"tags":[{"name":"docker","slug":"docker","permalink":"https://ydstudios.gitee.io/tags/docker/"},{"name":"docker-compose","slug":"docker-compose","permalink":"https://ydstudios.gitee.io/tags/docker-compose/"}]},{"title":"我常用的docker基本命令","slug":"我常用的docker基本命令","date":"2020-01-13T22:00:51.000Z","updated":"2020-01-14T14:45:55.000Z","comments":true,"path":"post/c9b0a620.html","link":"","permalink":"https://ydstudios.gitee.io/post/c9b0a620.html","excerpt":"docker 客户端非常简单 ,我们可以直接输入 docker 命令来查看到 Docker 客户端的所有命令选项。可以通过命令 docker command –help 更深入的了解指定的 Docker 命令使用方法。例如我们要查看 docker stats 指令的具体使用方法： 1docker stats --help","text":"docker 客户端非常简单 ,我们可以直接输入 docker 命令来查看到 Docker 客户端的所有命令选项。可以通过命令 docker command –help 更深入的了解指定的 Docker 命令使用方法。例如我们要查看 docker stats 指令的具体使用方法： 1docker stats --help docker查找镜像1234567# 查询镜像docker search 镜像名称#docker查找官方原版镜像docker search --filter \"is-official=true\" centos#或者docker search -f is-official=true centos docker 安装镜像12345678#命令语法docker pull 镜像名称:标签#安装 tomcat8 docker pull tomcat:8#安装 mysql5.7 docker pull mysql:5.7 删除images123456789#若删除不掉，说明有容器正在使用此 imagesdocker rmi imgId#先执行 docker rm containerId#再执行 docker rmi imgId 启动容器以下命令使用 ubuntu 镜像启动一个容器，参数为以命令行模式进入该容器： 1234567891011#命令语法docker run &lt;相关参数&gt; &lt;镜像 ID&gt; &lt;初始命令&gt; docker run -itd --name 自定义容器名称 镜像名称:标签 shell#运行tomcat8 docker run -p 8080:8080 --name mytomcat tomcat:8#运行ubuntu镜像docker run -itd ubuntu /bin/bash 参数说明： -i: 交互式操作。 -t: 终端。 -d：指定容器的运行模式为后台运行。 -P:是容器内部端口随机映射到主机的高端口。 -p: 是容器内部端口绑定到指定的主机端口。 –name: 标识来命名容器。 ubuntu: ubuntu 镜像。 /bin/bash：放在镜像名后的是命令，这里我们希望有个交互式 Shell，因此用的是 /bin/bash。 -v：表示需要将本地哪个目录挂载到容器中，格式：-v &lt;宿主机目录&gt;:&lt;容器目录&gt; 需要说明的是，不一定要使用“镜像 ID”，也可以使用“仓库名:标签名”，例如：centos:latest。初始命令表示一旦容器启动，需要运行的命令，此时使用“/bin/bash”，表示什么也不做，只需进入命令行即可。 123456docker run -d --name mysql5.7 -p 3306:3306 -v /usr/local/mysql/data:/var/lib/mysql -v /usr/local/mysql/conf.d:/etc/mysql/conf.d -e MYSQL_ROOT_PASSWORD=123456 mysql:5.7.25#将windows下F盘的目录javaResource挂载到docker中centos中的/usr/local/src下，docker中可以先设置分享盘符docker run -it --privileged=true -v /F/javaResource/:/usr/local/src/ 5e35e350aded /bin/bash 要退出终端，直接输入 exit:通过 docker inspect 命令，我们可以获取镜像的详细信息，其中，包括创建者，各层的数字摘要等。123456docker inspect repository:tagdocker inspect mysql:5.7docker inspect 返回的是 JSON 格式的信息，如果您想获取其中指定的一项内容，可以通过 -f 来指定，如获取镜像大小：docker inspect -f &#123;&#123;\".Size\"&#125;&#125; mysql:5.7 容器其他命令1234567891011121314#查看正在运行的容器docker ps#查看所有的容器docker ps -a#停止容器docker stop containerId#启动容器docker start containerId#重启容器docker restart containerId 进入容器在使用 -d 参数时，容器启动后会进入后台。此时想要进入容器，可以通过以下指令进入： docker attach docker exec：推荐大家使用 docker exec 命令，因为此退出容器终端，不会导致容器的停止。 1docker exec -it mytomcat bash #在这里用到了上面提到的容器别名mytomcat 什么是 DockerfileDockerfile 是一个用来构建镜像的文本文件，文本内容包含了一条条构建镜像所需的指令和说明。 FROM 和 RUN 指令的作用 FROM：定制的镜像都是基于 FROM 的镜像，这里的 nginx 就是定制需要的基础镜像。后续的操作都是基于 nginx。 RUN：用于执行后面跟着的命令行命令。有以下俩种格式： 12345678#shell格式RUN &lt;命令行命令&gt;# &lt;命令行命令&gt; 等同于，在终端操作的 shell 命令。#exec 格式RUN [\"可执行文件\", \"参数1\", \"参数2\"]# 例如：# RUN [\"./test.php\", \"dev\", \"offline\"] 等价于 RUN ./test.php dev offline 注意：Dockerfile 的指令每执行一次都会在 docker 上新建一层。所以过多无意义的层，会造成镜像膨胀过大。例如： 1234FROM centosRUN yum install wgetRUN wget -O redis.tar.gz \"http://download.redis.io/releases/redis-5.0.3.tar.gz\"RUN tar -xvf redis.tar.gz 以上执行会创建 3 层镜像。可简化为以下格式： 1234FROM centosRUN yum install wget \\ &amp;&amp; wget -O redis.tar.gz \"http://download.redis.io/releases/redis-5.0.3.tar.gz\" \\ &amp;&amp; tar -xvf redis.tar.gz 如上，以 &amp;&amp; 符号连接命令，这样执行后，只会创建 1 层镜像。 开始构建镜像在 Dockerfile 文件的存放目录下，执行构建动作。 以下示例，通过目录下的 Dockerfile 构建一个 tomcat和jdk整合的镜像:v1（镜像名称:镜像标签）注：最后的 . 代表本次执行的上下文路径，下一节会介绍。 构建镜像命令123456#命令语法 docker build -t name:tag .-t 镜像的名字及标签，通常 name:tag 或者 name 格式；可以在一次构建中为一个镜像设置多个标签。# 例如docker build -t javawebenv:v1 . Dockerfile内容1234567891011121314151617181920212223FROM centos:7 #使用centos7最新的官方镜像MAINTAINER samdockerdeveloperRUN mkdir -p /usr/local/tomcat7 \\&amp;&amp;mkdir -p /usr/local/jdk8WORKDIR /usr/local/src/ #使用该目录作为工作目录COPY apache-tomcat-7.0.99.zip /usr/local/src/COPY jdk-8u131-linux-x64.tar.gz /usr/local/src/COPY my.cnf /usr/local/src/RUN yum update -y \\&amp;&amp;yum install -y unzip \\&amp;&amp;unzip apache-tomcat-7.0.99.zip \\&amp;&amp;rpm --import /etc/pki/rpm-gpg/RPM* \\&amp;&amp;cp -R apache-tomcat-7.0.99/* /usr/local/tomcat7 \\&amp;&amp;chmod +x /usr/local/tomcat7/bin/*.sh \\&amp;&amp;tar -xf jdk-8u131-linux-x64.tar.gz -C /usr/local/jdk8 --strip-components 1 # 放弃上级目录解压到/usr/local/jdk8目录下ENV JAVA_HOME /usr/local/jdk8ENV CLASSPATH $JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jarENV CATALINA_HOME /usr/local/tomcat7ENV CATALINA_BASE /usr/local/tomcat7ENV PATH $PATH:$JAVA_HOME/bin:$CATALINA_HOME/lib:$CATALINA_HOME/binEXPOSE 8080 #暴露8080端口ENTRYPOINT [\"/usr/local/tomcat7/bin/catalina.sh\",\"run\"] 上下文路径，是指 docker 在构建镜像，有时候想要使用到本机的文件（比如复制），docker build 命令得知这个路径后，会将路径下的所有内容打包。 解析：由于 docker 的运行模式是 C/S。我们本机是 C，docker引擎是S。实际的构建过程是在docker引擎下完成的，所以这个时候无法用到我们本机的文件。这就需要把我们本机的指定目录下的文件一起打包提供给 docker 引擎使用。 如果未说明最后一个参数，那么默认上下文路径就是 Dockerfile 所在的位置。注意：上下文路径下不要放无用的文件，因为会一起打包发送给 docker 引擎，如果文件过多会造成过程缓慢。 使用的过程中遇到的问题123456789101112Docker报错“Dockerfile parse error line 1: FROM requires either one or three arguments” 看官方文档Format：以'#' 开头一行被视为评论，出现在其他位置视为参数,也就不难理解报错原因：将写在同一行的注释视为参数了。Failed to get D-Bus connection: Operation not permitted /etc/init.d/mysqld:启动命令 docker run -d -p 8080:8080 -v /root/webapps:/usr/local/tomcat7/webapps --name mycentos mycentos:7 /bin/bash以特权模式运行容器启动命令改成 docker run -d -p 8080:8080 -v /root/webapps:/usr/local/tomcat7/webapps --name mycentos mycentos:7 /usr/sbin/init/etc/sysconfig/network 文件夹不存在编辑/etc/init.d/mysqld 中的 . /etc/sysconfig/network修改成/etc/sysconfig/network-scripts/ifcfg-lo或者用软连接 ln -s /etc/sysconfig/network-scripts/ifcfg-lo /etc/sysconfig/network","categories":[],"tags":[{"name":"docker","slug":"docker","permalink":"https://ydstudios.gitee.io/tags/docker/"}]},{"title":"以centos7镜像为基础构建一个jdk8和tomcat7的镜像","slug":"以centos7镜像为基础构建一个jdk8和tomcat7的镜像","date":"2020-01-04T23:22:55.000Z","updated":"2020-01-05T03:14:41.000Z","comments":true,"path":"post/97b746d3.html","link":"","permalink":"https://ydstudios.gitee.io/post/97b746d3.html","excerpt":"首先我简单说一下容器的一些知识点 一个容器只跑一个进程，并且该进程只能前台运行。 容器有层的概念，如果需要定制容器，应该尽量通过Dockerfile来构建容器。","text":"首先我简单说一下容器的一些知识点 一个容器只跑一个进程，并且该进程只能前台运行。 容器有层的概念，如果需要定制容器，应该尽量通过Dockerfile来构建容器。 什么是 Dockerfile Dockerfile 是一个用来构建镜像的文本文件，文本内容包含了一条条构建镜像所需的指令和说明。 1234567891011121314151617181920FROM centos:7 MAINTAINER samdockerdeveloperRUN mkdir -p /usr/local/tomcat7 \\&amp;&amp;mkdir -p /usr/local/jdk8WORKDIR /usr/local/src/COPY apache-tomcat-7.0.99.zip /usr/local/src/COPY jdk-8u131-linux-x64.tar.gz /usr/local/src/RUN yum update -y \\&amp;&amp;yum install -y unzip \\&amp;&amp;unzip apache-tomcat-7.0.99.zip \\&amp;&amp;cp -R apache-tomcat-7.0.99/* /usr/local/tomcat7 \\&amp;&amp;chmod +x /usr/local/tomcat7/bin/*.sh \\&amp;&amp;tar -xf jdk-8u131-linux-x64.tar.gz -C /usr/local/jdk8 --strip-components 1ENV JAVA_HOME /usr/local/jdk8 \\ENV CLASSPATH $JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar \\ENV CATALINA_HOME /usr/local/tomcat7 \\ENV CATALINA_BASE /usr/local/tomcat7 \\ENV PATH $PATH:$JAVA_HOME/bin:$CATALINA_HOME/lib:$CATALINA_HOME/binEXPOSE 8080ENTRYPOINT [\"/usr/local/tomcat7/bin/catalina.sh\",\"run\"] 在Dockerfile同级目录里存放已经下载好的apache-tomcat-7.0.99.zip、jdk-8u131-linux-x64.tar.gz。 注意：Dockerfile 的指令每执行一次都会在 docker 上新建一层。所以过多无意义的层，会造成镜像膨胀过大。例如： 123456789FROM centosRUN yum install wgetRUN wget -O redis.tar.gz \"http://download.redis.io/releases/redis-5.0.3.tar.gz\"RUN tar -xvf redis.tar.gz以上执行会创建 3 层镜像。可简化为以下格式：FROM centosRUN yum install wget \\ &amp;&amp; wget -O redis.tar.gz \"http://download.redis.io/releases/redis-5.0.3.tar.gz\" \\ &amp;&amp; tar -xvf redis.tar.gz 如上，以 &amp;&amp; 符号连接命令，这样执行后，只会创建 1 层镜像。 开始构建镜像在 Dockerfile 文件的存放目录下，执行构建动作。 以下示例，通过目录下的 Dockerfile 构建一个 samjavawebenv:v2（镜像名称:镜像标签）。 注：最后的 . 代表本次执行的上下文路径 12# docker build -t name:tag .docker build -t samjavawebenv:v2 .","categories":[],"tags":[{"name":"linux","slug":"linux","permalink":"https://ydstudios.gitee.io/tags/linux/"},{"name":"docker","slug":"docker","permalink":"https://ydstudios.gitee.io/tags/docker/"}]},{"title":"SpringMvc使用@Async注解实现有返回值和无返回值的异步处理","slug":"SpringMvc使用-Async注解实现有返回值和无返回值的异步处理","date":"2020-01-03T23:12:21.000Z","updated":"2020-10-02T10:34:35.000Z","comments":true,"path":"post/e7e78673.html","link":"","permalink":"https://ydstudios.gitee.io/post/e7e78673.html","excerpt":"1. SpringMvc使用@Async注解实现有返回值和无返回值的异步处理 异步调用对应的是同步调用，同步调用指程序按照定义顺序依次执行，每一行程序都必须等待上一行程序执行完成之后才能执行；异步调用指程序在顺序执行时，不等待异步调用的语句返回结果就执行后面的程序。","text":"1. SpringMvc使用@Async注解实现有返回值和无返回值的异步处理 异步调用对应的是同步调用，同步调用指程序按照定义顺序依次执行，每一行程序都必须等待上一行程序执行完成之后才能执行；异步调用指程序在顺序执行时，不等待异步调用的语句返回结果就执行后面的程序。 1.1 同步方式调用代码 相关代码 12345678910111213141516171819202122232425262728@Servicepublic class TaskService &#123; public void doTaskOne() throws Exception &#123; System.out.println(\"开始做任务一\"); long start = System.currentTimeMillis(); Thread.sleep(2000); long end = System.currentTimeMillis(); System.out.println(\"完成任务二，耗时：%s\" , (end - start) + \"毫秒\"); &#125; public void doTaskTwo() throws Exception &#123; System.out.println(\"开始做任务二\"); long start = System.currentTimeMillis(); Thread.sleep(3000); long end = System.currentTimeMillis(); System.out.println(\"完成任务二，耗时：%s\" , (end - start) + \"毫秒\"); &#125; public void doTaskThree() throws Exception &#123; System.out.println(\"开始做任务三\"); long start = System.currentTimeMillis(); Thread.sleep(4000); long end = System.currentTimeMillis(); System.out.println(\"完成任务二，耗时：%s\" , (end - start) + \"毫秒\"); &#125;&#125; 同步调用 1234567891011@Autowiredprivate TaskService task;public String test() &#123; try &#123; task.doTaskOne(); task.doTaskTwo(); task.doTaskThree(); &#125;catch (Exception e)&#123; &#125;&#125; 下面是运行结果，可以看到三个方法是依次执行的，分别耗时2秒、3秒、4秒、总耗时9秒 123456开始做任务一完成任务一，耗时：2001毫秒开始做任务二完成任务二，耗时：3000毫秒开始做任务三完成任务三，耗时：4001毫秒 上面的同步调用，虽然顺利地完成了三个任务，但是执行时间比较长，如果这三个任务没有依赖关系，可以并发执行的话，可以考虑使用异步调用的方法。 2. 异步方式调用代码无返回值 首先在spring中配置相关参数开启异步调用 123456789101112&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;beans xmlns=\"http://www.springframework.org/schema/beans\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns:context=\"http://www.springframework.org/schema/context\" xmlns:task=\"http://www.springframework.org/schema/task\" xsi:schemaLocation=\" http://www.springframework.org/schema/task http://www.springframework.org/schema/task/spring-task.xsd\"&gt; &lt;task:annotation-driven executor=\"taskExecutor\" /&gt; &lt;task:executor id=\"taskExecutor\" pool-size=\"20\" queue-capacity=\"1000\"/&gt;&lt;/beans&gt; 如果直接按照下面的方式配置，则 Spring 会使用默认的线程池 org.springframework.core.task.SimpleAsyncTaskExecutor但这个 SimpleAsyncTaskExecutor 不是真的线程池，这个类不重用线程，每次调用都会创建一个新的线程。 123456789101112131415&lt;task:annotation-driven/&gt;# 详见spring-context-4.1.7.RELEASE.jar/org/springframework/scheduling/config/spring-task-4.0.xsd 的描述&lt;xsd:attribute name=\"executor\" type=\"xsd:string\" use=\"optional\"&gt; &lt;xsd:annotation&gt; &lt;xsd:documentation&gt;&lt;![CDATA[Specifies the java.util.Executor instance to use when invoking asynchronous methods.If not provided, an instance of org.springframework.core.task.SimpleAsyncTaskExecutorwill be used by default.Note that as of Spring 3.1.2, individual @Async methods may qualify which executor touse, meaning that the executor specified here acts as a default for all non-qualified@Async methods. ]]&gt;&lt;/xsd:documentation&gt; &lt;/xsd:annotation&gt; 在方法上加上 @Async 注解就能将同步函数变成异步函数，改造后的代码 123456789101112131415161718192021222324252627282930import java.util.concurrent.Future;@Servicepublic class TaskService &#123; @Async public void doTaskOne() throws Exception &#123; System.out.println(\"开始做任务一\"); long start = System.currentTimeMillis(); Thread.sleep(2000); long end = System.currentTimeMillis(); System.out.println(\"完成任务二，耗时：%s\" , (end - start) + \"毫秒\"); &#125; @Async public void doTaskTwo() throws Exception &#123; System.out.println(\"开始做任务二\"); long start = System.currentTimeMillis(); Thread.sleep(3000); long end = System.currentTimeMillis(); System.out.println(\"完成任务二，耗时：%s\" , (end - start) + \"毫秒\"); &#125; @Async public void doTaskThree() throws Exception &#123; System.out.println(\"开始做任务三\"); long start = System.currentTimeMillis(); Thread.sleep(4000); long end = System.currentTimeMillis(); System.out.println(\"完成任务二，耗时：%s\" , (end - start) + \"毫秒\"); &#125;&#125; 重新调用之后的运行结果如下： 123456开始做任务三开始做任务二开始做任务一完成任务一，耗时：2000毫秒完成任务二，耗时：3001毫秒完成任务三，耗时：4000毫秒 注意事项 @Async 所修饰的函数不要定义为 static 类型，这样异步调用不会生效。 调用方法和异步函数不能在一个 class 中。 可以在使用的时候自定义线程池 @Async(&quot;poolTaskExecutor&quot;) 3. 异步方式调用代码有返回值如果想知道异步函数什么时候执行完，那就需要使用 Future (AsyncResult是Future的子类)来返回异步调用的结果。改造后的代码如下： 12345678910111213141516171819202122232425262728293031323334@Servicepublic class TaskService &#123; @Async public Future&lt;String&gt; doTaskOne() &#123; System.out.println(\"开始做任务一\"); long start = System.currentTimeMillis(); Thread.sleep(2000); long end = System.currentTimeMillis(); System.out.println(\"完成任务二，耗时：%s\" , (end - start) + \"毫秒\"); return new AsyncResult&lt;&gt;(\"任务一完成\"); &#125; @Async public Future&lt;String&gt; doTaskTwo() &#123; System.out.println(\"开始做任务二\"); long start = System.currentTimeMillis(); Thread.sleep(3000); long end = System.currentTimeMillis(); System.out.println(\"完成任务二，耗时：%s\" , (end - start) + \"毫秒\"); return new AsyncResult&lt;&gt;(\"任务二完成\"); &#125; @Async public Future&lt;String&gt; doTaskThree() &#123; System.out.println(\"开始做任务三\"); long start = System.currentTimeMillis(); Thread.sleep(4000); long end = System.currentTimeMillis(); System.out.println(\"完成任务二，耗时：%s\" , (end - start) + \"毫秒\"); return new AsyncResult&lt;&gt;(\"任务三完成\"); &#125;&#125; 异步方式调用代码有返回值 123456789101112131415161718192021222324252627282930@Autowiredprivate TaskService task;public List&lt;String&gt; test() &#123; List&lt;String&gt; result = new ArrayList(16); StopWatch stopWatch = new StopWatch(); stopWatch.start(\"接口速度统计\"); // 3秒超时 int timeout = 3; try &#123; Future&lt;String&gt; task1 = task.doTaskOne(); Future&lt;String&gt; task2 = task.doTaskTwo(); Future&lt;String&gt; task3 = task.doTaskThree(); String taskString1= task1.get(timeout, TimeUnit.SECONDS); String taskString2= task2.get(timeout, TimeUnit.SECONDS); String taskString3= task3.get(timeout, TimeUnit.SECONDS); result.add(taskString1); result.add(taskString2); result.add(taskString3); &#125;catch (TimeoutException | InterruptedException | ExecutionException e)&#123; return result; &#125; finally &#123; stopWatch.stop(); log.info(stopWatch.prettyPrint()); &#125; return result;&#125; 12345678910111213开始做任务三开始做任务二开始做任务一完成任务一，耗时：2001毫秒完成任务二，耗时：3000毫秒完成任务三，耗时：4001毫秒StopWatch '': running time (millis) = 248-----------------------------------------ms % Task name-----------------------------------------04036 100% 接口速度统计 刚开始想利用CountDownLatch来实现等待所有线程结束整合结果，后来调整为 Future 的 get(long timeout, TimeUnit unit) 来实现线程的超时控制，我看有些的例子使用死循环来阻塞整合线程的执行结果，这样做是有些问题的，如果有个线程一直没有结束运行，那就完犊子了！ 这是一种常见的场景将一个大的任务切分为数个子任务，并行处理所有子任务，当所有子任务都成功结束时再继续处理后面的逻辑。还有一种做法是利用CountDownLatch, 主线程构造countDownLatch对象，latch的大小为子任务的总数，每一个任务持有countDownLatch的引用，任务完成时对latch减1，主线程阻塞在countDownLatch.await方法上，当所有子任务都成功执行完后，latch=0, 主线程继续执行。 总结异步调用可以提升接口性能。比如导出下载、发送邮件短信等代码，可以使用异步执行。 参考 https://blog.csdn.net/qqfo24/article/details/81383022","categories":[],"tags":[{"name":"SpringMvc","slug":"SpringMvc","permalink":"https://ydstudios.gitee.io/tags/SpringMvc/"},{"name":"Async","slug":"Async","permalink":"https://ydstudios.gitee.io/tags/Async/"}]},{"title":"在CentOS7上使用yum安装Docker","slug":"在CentOS7上使用yum安装Docker","date":"2019-12-23T22:30:18.000Z","updated":"2020-01-13T14:39:01.000Z","comments":true,"path":"post/ef24ce2b.html","link":"","permalink":"https://ydstudios.gitee.io/post/ef24ce2b.html","excerpt":"目前在 Linux 操作系统上安装 Docker，对系统版本有以下要求： CentOS：7 Debian：7.7（Wheezy LTS）、8.0（Jessie LTS）、9（Stretch） Fedora：24、25 Ubuntu：16.04（Xenial LTS）、14.04（Trusty LTS）、17.04（Zesty）","text":"目前在 Linux 操作系统上安装 Docker，对系统版本有以下要求： CentOS：7 Debian：7.7（Wheezy LTS）、8.0（Jessie LTS）、9（Stretch） Fedora：24、25 Ubuntu：16.04（Xenial LTS）、14.04（Trusty LTS）、17.04（Zesty） 使用 yum 进行安装12345678910111213141516171819202122232425262728293031# step 1: 安装必要的一些系统工具sudo yum install -y yum-utils device-mapper-persistent-data lvm2# Step 2: 添加软件源信息sudo yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo# Step 3: 更新并安装Docker-CEsudo yum makecache fastsudo yum -y install docker-ce# Step 4: 开启Docker服务sudo service docker start# 开机启动systemctl enable docker# 注意：# 官方软件源默认启用了最新的软件，您可以通过编辑软件源的方式获取各个版本的软件包。# 例如官方并没有将测试版本的软件源置为可用，您可以通过以下方式开启。同理可以开启各种测试版本等。# vim /etc/yum.repos.d/docker-ee.repo# 将[docker-ce-test]下方的enabled=0修改为enabled=1## 安装指定版本的Docker-CE:# Step 1: 查找Docker-CE的版本:# yum list docker-ce.x86_64 --showduplicates | sort -r# Loading mirror speeds from cached hostfile# Loaded plugins: branch, fastestmirror, langpacks# docker-ce.x86_64 17.03.1.ce-1.el7.centos docker-ce-stable# docker-ce.x86_64 17.03.1.ce-1.el7.centos @docker-ce-stable# docker-ce.x86_64 17.03.0.ce-1.el7.centos docker-ce-stable# Available Packages# Step2: 安装指定版本的Docker-CE: (VERSION例如上面的17.03.0.ce.1-1.el7.centos)# sudo yum -y install docker-ce-[VERSION] 安装校验12345678910111213141516171819202122232425262728[root@izuf6gp8l1zfgu3hmvia6gz ~]# docker versionClient: Docker Engine - Community Version: 19.03.5 API version: 1.40 Go version: go1.12.12 Git commit: 633a0ea Built: Wed Nov 13 07:25:41 2019 OS/Arch: linux/amd64 Experimental: falseServer: Docker Engine - Community Engine: Version: 19.03.5 API version: 1.40 (minimum version 1.12) Go version: go1.12.12 Git commit: 633a0ea Built: Wed Nov 13 07:24:18 2019 OS/Arch: linux/amd64 Experimental: false containerd: Version: 1.2.10 GitCommit: b34a5c8af56e510852c35414db4c1f4fa6172339 runc: Version: 1.0.0-rc8+dev GitCommit: 3e425f80a8c931f88e6d94a8c831b9d5aa481657 docker-init: Version: 0.18.0 GitCommit: fec3683 至此docker安装完成。 参考资料","categories":[],"tags":[{"name":"docker","slug":"docker","permalink":"https://ydstudios.gitee.io/tags/docker/"},{"name":"centos","slug":"centos","permalink":"https://ydstudios.gitee.io/tags/centos/"}]},{"title":"Mac下安装zookeeper","slug":"Mac下安装zookeeper","date":"2019-12-07T22:15:07.000Z","updated":"2019-12-15T15:50:49.000Z","comments":true,"path":"post/d97b6c63.html","link":"","permalink":"https://ydstudios.gitee.io/post/d97b6c63.html","excerpt":"从zookeeper下载，注意下载带-bin后缀的安装包，不然安装之后启动会出现以下的错误。","text":"从zookeeper下载，注意下载带-bin后缀的安装包，不然安装之后启动会出现以下的错误。 12错误: 找不到或无法加载主类org.apache.zookeeper.server.quorum.QuorumPeerMain 解决方案原来是因为从版本3.5.5开始，带有bin名称的包才是我们想要的下载可以直接使用的里面有编译后的二进制的包，而之前的普通的tar.gz的包里面是只是源码的包无法直接使用。 将下载后的安装包解压到 /usr/local/zookeeper 目录下，进行后续的安装配置。 编辑配置文件在conf目录下拷贝 zoo_sample.cfg 文件新建 zoo.cfg配置文件，修改或者新加以下内容： 1234567891011121314151617181920# The number of milliseconds of each ticktickTime=2000# The number of ticks that the initial # synchronization phase can takeinitLimit=10# The number of ticks that can pass between # sending a request and getting an acknowledgementsyncLimit=5# the directory where the snapshot is stored.# do not use /tmp for storage, /tmp here is just # example sakes.dataDir=/usr/local/zookeeper/data# the port at which the clients will connectclientPort=2181#tickTime: zookeeper中使用的基本时间单位, 毫秒值.#dataDir: 数据目录. 可以是任意目录.#dataLogDir: log目录, 同样可以是任意目录. 如果没有设置该参数, 将使用和#dataDir相同的设置.此目录可以不加#clientPort: 监听client连接的端口号. 将zookeeper加入系统环境变量中12345678910111213141516171819202122sudo vim /etc/profilePATH=$PATH:/usr/local/sphinx/bin:/usr/local/elasticsearch/bin:/usr/local/mycat/bin:/usr/local/zookeeper/binexport PATH#让环境变量立马生效source /etc/profile# 启动Sam-Mac-2:~ Sam$ zkServer.sh start/usr/bin/javaZooKeeper JMX enabled by defaultUsing config: /usr/local/zookeeper/bin/../conf/zoo.cfgStarting zookeeper ... STARTED# 关闭Sam-Mac-2:~ Sam$ zkServer.sh stop/usr/bin/javaZooKeeper JMX enabled by defaultUsing config: /usr/local/zookeeper/bin/../conf/zoo.cfgStopping zookeeper ... STOPPED","categories":[],"tags":[{"name":"zookeeper","slug":"zookeeper","permalink":"https://ydstudios.gitee.io/tags/zookeeper/"},{"name":"mac","slug":"mac","permalink":"https://ydstudios.gitee.io/tags/mac/"}]},{"title":"Hello World","slug":"hello-world","date":"2019-11-10T13:19:35.000Z","updated":"2020-12-06T05:21:18.000Z","comments":true,"path":"post/4a17b156.html","link":"","permalink":"https://ydstudios.gitee.io/post/4a17b156.html","excerpt":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub.","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new \"My New Post\" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","categories":[],"tags":[]},{"title":"解决nested exception is java.lang.IllegalArgumentException Could not resolve placeholder alipay.appId in string value  $ {alipay.appId}","slug":"解决nested-exception-is-java-lang-IllegalArgumentException-Could-not-resolve-placeholder-alipay-appId-in-string-value-alipay-appId","date":"2019-09-18T23:52:05.000Z","updated":"2019-12-09T15:03:26.000Z","comments":true,"path":"post/cf36aee2.html","link":"","permalink":"https://ydstudios.gitee.io/post/cf36aee2.html","excerpt":"开发的时候出现了这种错误 nested exception is java .lang.IllegalArgumentException: Could not resolve placeholder ‘alipay.appId’ in string value “${alipay.appId}”","text":"开发的时候出现了这种错误 nested exception is java .lang.IllegalArgumentException: Could not resolve placeholder ‘alipay.appId’ in string value “${alipay.appId}” 大意是Spring不能处理第二个属性文件中的配置信息，因为Spring不允许定义多个PropertyPlaceholderConfigurer或context:property-placeholder。Spring用反射扫描的发现机制，在探测到Spring容器中有一个org.springframework.beans.factory.config.PropertyPlaceholderConfigurer的Bean就会停止对剩余PropertyPlaceholderConfigurer的扫描（Spring 3.1已经使用PropertySourcesPlaceholderConfigurer替代 PropertyPlaceholderConfigurer了）。换句话说，即Spring容器仅允许最多定义一个PropertyPlaceholderConfigurer(或context:property-placeholder)，其余的会被Spring忽略掉（其实Spring如果提供一个警告就好了）。 问题的解决方案 通配符解决、逗号分隔 使用通配符让spring一次性读取多个属性文件到一个 PropertyPlaceholderConfigurer bean中12&lt;context:property-placeholder location=\"classpath:conf/*.properties\"/&gt; 或者使用 1&lt;context:property-placeholder location=\"classpath:conf/db.properties,conf/alipay.properties\"/&gt; 使用多个context:property-placeholder 分开定义，注意要加上 ignore-unresolvable 属性12&lt;context:property-placeholder location=\"classpath:conf/db.properties\" ignore-unresolvable=\"true\"/&gt;&lt;context:property-placeholder location=\"classpath:conf/alipay.properties\" ignore-unresolvable=\"true\"/&gt; 在每个PropertySourcesPlaceholderConfigurer配置中添加 或者在每个context:property-placeholder中都加上ignore-unresolvable=”true” 因为在你使用@Value(“${xx}”)或在xml中使用${xx}获取属性时，Spring会在第一个读取到的属性文件中去找，如果没有就直接抛出异常，而不会继续去第二个属性文件中找 一个PropertySourcesPlaceholderConfigurer中包含多个属性文件，和方案1原理相同1234567&lt;bean id=\"propertyConfigurer\" class=\"org.springframework.context.support.PropertySourcesPlaceholderConfigurer\"&gt;&lt;property name=\"locations\"&gt; &lt;list&gt; &lt;value&gt;classpath:db.properties&lt;/value&gt; &lt;value&gt;classpath:alipay.properties&lt;/value&gt; &lt;/list&gt;&lt;/property&gt;","categories":[],"tags":[{"name":"java","slug":"java","permalink":"https://ydstudios.gitee.io/tags/java/"},{"name":"spring","slug":"spring","permalink":"https://ydstudios.gitee.io/tags/spring/"}]},{"title":"Java获取一天的最大时间23:59:59和最小时间00:00:00","slug":"Java获取一天的最大时间23-59-59和最小时间00-00-00","date":"2019-09-04T23:49:24.000Z","updated":"2019-12-09T15:03:26.000Z","comments":true,"path":"post/1ef2d20f.html","link":"","permalink":"https://ydstudios.gitee.io/post/1ef2d20f.html","excerpt":"Java获取一天的最大时间23:59:59和最小时间00:00:00","text":"Java获取一天的最大时间23:59:59和最小时间00:00:00 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566public class Test &#123; public static void main(String[] args) &#123; // 加上毫秒数 SimpleDateFormat sDateFormat=new SimpleDateFormat(\"yyyy-MM-dd HH:mm:ss.SSS\"); System.out.println(sDateFormat.format(getAfterYearDateMaxTime(3))); &#125; /** * 若干年之后的23:59:59 * * @param year * @return */ public static Date getAfterYearDateMaxTime(int year)&#123; Calendar calendarEnd = Calendar.getInstance(); calendarEnd.setTime(new Date()); calendarEnd.add(Calendar.YEAR,year); calendarEnd.set(Calendar.HOUR_OF_DAY, 23); calendarEnd.set(Calendar.MINUTE, 59); calendarEnd.set(Calendar.SECOND, 59); // MySQL数据库对于毫秒大于500的数据进行进位，所有就造成的MySQL中的时间多一秒,防止mysql自动加一秒,毫秒设为0 calendarEnd.set(Calendar.MILLISECOND, 0); return calendarEnd.getTime(); &#125; /** * 当天的最大时间23:59:59 * * @return */ public static Date getDayMaxTime()&#123; Calendar calendarEnd = Calendar.getInstance(); calendarEnd.setTime(new Date()); calendarEnd.set(Calendar.HOUR_OF_DAY, 23); calendarEnd.set(Calendar.MINUTE, 59); calendarEnd.set(Calendar.SECOND, 59); // MySQL数据库对于毫秒大于500的数据进行进位，所有就造成的MySQL中的时间多一秒,防止mysql自动加一秒,毫秒设为0 calendarEnd.set(Calendar.MILLISECOND, 0); return calendarEnd.getTime(); &#125; /** * 当天的最小时间00:00:00 * * @return */ public static Date getDayMinTime()&#123; Calendar calendarEnd = Calendar.getInstance(); calendarEnd.setTime(new Date()); calendarEnd.set(Calendar.HOUR_OF_DAY, 00); calendarEnd.set(Calendar.MINUTE, 00); calendarEnd.set(Calendar.SECOND, 00); // MySQL数据库对于毫秒大于500的数据进行进位，所有就造成的MySQL中的时间多一秒,防止mysql自动加一秒,毫秒设为0 calendarEnd.set(Calendar.MILLISECOND, 0); return calendarEnd.getTime(); &#125;&#125; 获取3年后的23:59:59，毫秒数设置成0 12022-09-04 23:59:59.000","categories":[],"tags":[{"name":"java","slug":"java","permalink":"https://ydstudios.gitee.io/tags/java/"}]},{"title":"IDEA下tomcat启动后 server乱码，Tomcat Catalina Log和Tomcat Localhost Log乱码问题的解决 ","slug":"IDEA下tomcat启动后-server乱码，Tomcat-Catalina-Log和Tomcat-Localhost-Log乱码问题的解决","date":"2019-06-03T23:48:03.000Z","updated":"2019-12-09T15:03:26.000Z","comments":true,"path":"post/35c3eddf.html","link":"","permalink":"https://ydstudios.gitee.io/post/35c3eddf.html","excerpt":"前言不改的话，不影响使用，因为报错基本都是英文显示的。但是这几个中文字乱码的看着难受。 乱码的根本原因： Windows系统的cmd是GBK编码的，所以IDEA的下方log输出的部分的编码也是GBK的，然而Tomcat 9.0 版本默认log输出是UTF-8编码的，采用了两种不同的编码方式就会导致乱码。","text":"前言不改的话，不影响使用，因为报错基本都是英文显示的。但是这几个中文字乱码的看着难受。 乱码的根本原因： Windows系统的cmd是GBK编码的，所以IDEA的下方log输出的部分的编码也是GBK的，然而Tomcat 9.0 版本默认log输出是UTF-8编码的，采用了两种不同的编码方式就会导致乱码。 解决方案方法一 修改Tomcat 日志输出为GBK编码先打开tomcat的安装目录 编辑 conf 目录下的 logging.properties,把里面的UTF-8 通通改成 GBK,保存，重启Tomcat，即可生效。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071# Licensed to the Apache Software Foundation (ASF) under one or more# contributor license agreements. See the NOTICE file distributed with# this work for additional information regarding copyright ownership.# The ASF licenses this file to You under the Apache License, Version 2.0# (the \"License\"); you may not use this file except in compliance with# the License. You may obtain a copy of the License at## http://www.apache.org/licenses/LICENSE-2.0## Unless required by applicable law or agreed to in writing, software# distributed under the License is distributed on an \"AS IS\" BASIS,# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.# See the License for the specific language governing permissions and# limitations under the License.handlers = 1catalina.org.apache.juli.FileHandler, 2localhost.org.apache.juli.FileHandler, 3manager.org.apache.juli.FileHandler, 4host-manager.org.apache.juli.FileHandler, java.util.logging.ConsoleHandler.handlers = 1catalina.org.apache.juli.FileHandler, java.util.logging.ConsoleHandler############################################################# Handler specific properties.# Describes specific configuration info for Handlers.############################################################1catalina.org.apache.juli.FileHandler.level = FINE1catalina.org.apache.juli.FileHandler.directory = $&#123;catalina.base&#125;/logs1catalina.org.apache.juli.FileHandler.prefix = catalina.1catalina.org.apache.juli.FileHandler.encoding = UTF-82localhost.org.apache.juli.FileHandler.level = FINE2localhost.org.apache.juli.FileHandler.directory = $&#123;catalina.base&#125;/logs2localhost.org.apache.juli.FileHandler.prefix = localhost.2localhost.org.apache.juli.FileHandler.encoding = UTF-83manager.org.apache.juli.FileHandler.level = FINE3manager.org.apache.juli.FileHandler.directory = $&#123;catalina.base&#125;/logs3manager.org.apache.juli.FileHandler.prefix = manager.3manager.org.apache.juli.FileHandler.encoding = UTF-84host-manager.org.apache.juli.FileHandler.level = FINE4host-manager.org.apache.juli.FileHandler.directory = $&#123;catalina.base&#125;/logs4host-manager.org.apache.juli.FileHandler.prefix = host-manager.4host-manager.org.apache.juli.FileHandler.encoding = UTF-8java.util.logging.ConsoleHandler.level = FINEjava.util.logging.ConsoleHandler.formatter = java.util.logging.SimpleFormatterjava.util.logging.ConsoleHandler.encoding = UTF-8############################################################# Facility specific properties.# Provides extra control for each logger.############################################################org.apache.catalina.core.ContainerBase.[Catalina].[localhost].level = INFOorg.apache.catalina.core.ContainerBase.[Catalina].[localhost].handlers = 2localhost.org.apache.juli.FileHandlerorg.apache.catalina.core.ContainerBase.[Catalina].[localhost].[/manager].level = INFOorg.apache.catalina.core.ContainerBase.[Catalina].[localhost].[/manager].handlers = 3manager.org.apache.juli.FileHandlerorg.apache.catalina.core.ContainerBase.[Catalina].[localhost].[/host-manager].level = INFOorg.apache.catalina.core.ContainerBase.[Catalina].[localhost].[/host-manager].handlers = 4host-manager.org.apache.juli.FileHandler# For example, set the org.apache.catalina.util.LifecycleBase logger to log# each component that extends LifecycleBase changing state:#org.apache.catalina.util.LifecycleBase.level = FINE# To see debug messages in TldLocationsCache, uncomment the following line:#org.apache.jasper.compiler.TldLocationsCache.level = FINEjava.util.logging.ConsoleHandler.encoding = UTF-8 方法二 修改IDEA为UTF-8编码idea.exe.vmoptions、idea64.exe.vmoptions 修改idea.exe.vmoptions和idea64.exe.vmoptions这两个文件 分别在这两个文件的最后，添加一行 -Dfile.encoding=UTF-8 12345678910111213-Xms128m-Xmx750m-XX:ReservedCodeCacheSize=240m-XX:+UseConcMarkSweepGC-XX:SoftRefLRUPolicyMSPerMB=50-ea-Dsun.io.useCanonCaches=false-Djava.net.preferIPv4Stack=true-Djdk.http.auth.tunneling.disabledSchemes=\"\"-XX:+HeapDumpOnOutOfMemoryError-XX:-OmitStackTraceInFastThrow-Dfile.encoding=UTF-8 这个参数的作用是强制系统文件使用UTF-8编码,改完之后保存，重启IDEA即可。","categories":[],"tags":[{"name":"tomcat","slug":"tomcat","permalink":"https://ydstudios.gitee.io/tags/tomcat/"},{"name":"idea","slug":"idea","permalink":"https://ydstudios.gitee.io/tags/idea/"}]},{"title":"JavaScript如何面向对象","slug":"JavaScript如何面向对象","date":"2019-05-23T23:47:08.000Z","updated":"2019-12-09T15:03:26.000Z","comments":true,"path":"post/d7445999.html","link":"","permalink":"https://ydstudios.gitee.io/post/d7445999.html","excerpt":"关于JavaScript编写类的方式，总结一下JavaScript编写类的几种写法以及这几种写法的优缺点。","text":"关于JavaScript编写类的方式，总结一下JavaScript编写类的几种写法以及这几种写法的优缺点。 构造函数方式基本语法： 1234567891011121314 function 类名()&#123; this.属性名;//公共属性 var 属性名;//私有属性 /*凡是定义类的公共属性和公共方法都要使用this*/ //定义类的公共函数 this.函数名=function()&#123; ..... &#125; //定义类的私有函数 function 函数名()&#123; ...... &#125;&#125; 1234567891011121314151617181920212223242526272829303132333435363738394041/*定义一个Person类*/ function Person(_name,_age,_salary)&#123; //Person类的公开属性，类的公开属性的定义方式是：”this.属性名“ this.name=_name; //Person类的私有属性，类的私有属性的定义方式是：”var 属性名“ var age=_age;//私有属性 var salary=_salary;//私有属性 /*定义私有属性Age的对外公开访问方法*/ this.setAge = function(intAge) &#123; age = intAge; &#125; /*定义私有属性Age的对外公开访问方法*/ this.getAge = function() &#123; return age; &#125; //定义Person类的公开方法(特权方法)，类的公开方法的定义方式是：”this.functionName=function()&#123;.....&#125;“ this.Show=function()&#123; document.writeln(\"在公开方法里面访问类的私有属性是允许的，age=\"+age+\"\\t\"+\"salary=\"+salary);//在公开方法里面访问类的私有属性是允许的 &#125; //公共方法 this.publicMethod = function()&#123; document.writeln(\"在公开方法里面访问类的私有方法是允许的\"); privateFn();//在公开方法里面调用类的私有方法 privateFn2();//在公开方法里面调用类的私有方法 &#125; /* 定义Person类的私有方法(内部方法)， 类的私有方法的定义方式是：”function functionName()&#123;.....&#125;“， 或者 var functionName=function()&#123;....&#125; */ function privateFn()&#123; document.writeln(\"我是Person类的私有函数privateFn\"); &#125; var privateFn2=function()&#123; document.writeln(\"我是Person类的私有函数privateFn2\"); &#125; &#125; 测试Person类 1234567891011121314151617181920var p1 = new Person(\"孤傲苍狼\",24,2300); var p2 = new Person(\"白虎神皇\",24,2300); document.write(\"&lt;pre&gt;\"); document.writeln(\"p1 instanceof Person的结果是：\"+(p1 instanceof Person));//p1是Person类的实例，结果是true document.writeln(\"p2 instanceof Person的结果是：\"+(p2 instanceof Person));//p2是Person类的实例，结果是true //当==两边的内容是对象或者是对象的函数属性时，则比较内存地址是否相等 document.writeln(\"当==两边的内容是对象或者是对象的函数属性时，则比较内存地址是否相等\"); document.writeln(\"比较p1和p2这两个对象的show方法的内存地址是否一样：p1.show== p2.show的结果是：\"+(p1.show == p2.show));//false document.writeln(\"p1.show == p2.show的结果是：\"+(p1.show == p2.show)+\"，这证明p1对象和p2对象不是共享一个show方法，在内存中show方法的代码有2份，存放在两块内存区域\"); document.writeln(\"name是Person类定义的public属性，可以使用类的对象去直接访问类的public属性\"); document.writeln(\"p1.name=\"+p1.name);//访问公有属性，这是可以正常访问的 document.writeln(\"age和salary是Person类定义的private属性，不能使用类的对象去直接访问类私有属性，这是访问不了的，结果都是undefined\"); document.writeln(\"p1.age=\"+p1.age+\"，\"+\"p1.salary=\"+p1.salary)//不能使用类的对象去直接访问类私有属性，这是访问不了的，结果都是undefined p1.show();//调用类的公共函数，这次允许的 p1.publicMethod();//调用类的公共函数，这次允许的 p1.setAge(24);//使用public方法setAge方法为私有属性age赋值 document.writeln(\"使用public方法getAge方法获取私有属性age的值，p1.getAge()=\"+p1.getAge());//使用getAge方法获取私有属性age的值 //document.writeln(\"p1.privateFn()：\"+p1.privateFn()+\"&amp;nbsp;p1.privateFn2()：\"+p1.privateFn2());//不能使用类的对象去调用类的私有方法，这里会报错”对象不支持此属性或者方法 document.write(\"&lt;/pre&gt;\"); 这种方式的优点是：可以根据参数来构造不同的对象实例 ，每个对象的属性一般是不相同的，缺点是构造每个实例对象时，方法不能共享，Person类里面定义的那些方法，p1对象有一份，p2也有一份，那么在内存中就得开辟两块内存空间来分别存储p1的方法和p2的方法，这样就造成了内存的浪费。对于一个类的不同实例对象，这些对象的属性一般是不相同的，但是方法是相同的，所以节约内存的做法就是把方法放到内存的一块区域中存放，然后每个实例对象都从这块内存中取出方法。 原型方式需要说明的是，使用原型方式编写JavaScript类是无法给类添加私有属性和私有方法的，使用原型方式添加的属性和方法都是public的。 写法一 123456789101112131415161718192021222324252627/*定义一个Person类*/ function Person(_name,_age,_weight,_height)&#123; this.init(_name,_age,_weight,_height); &#125; /*使用原型的方式定义Person类的public属性：name,age,weight,height，使用原型的方式添加的属性都是public的*/ Person.prototype.name; Person.prototype.age; Person.prototype.weight; Person.prototype.height; /*使用原型的方式给Person类添加public方法，使用原型的方式添加的方法都是public的*/ /*使用原型的方式给Person类添加init方法*/ Person.prototype.init = function(_name,_age,_weight,_height) &#123; if(_name != undefined &amp;&amp; _age!=undefined &amp;&amp; _weight!=undefined &amp;&amp; _height!=undefined)&#123; this.name = _name; this.age = _age; this.weight=_weight; this.height=_height; document.writeln(\"this.name=\"+this.name+\",this.age=\"+this.age+\",this.weight=\"+this.weight+\",this.height=\"+this.height); &#125; &#125; /*使用原型的方式给Person类添加show方法*/ Person.prototype.show = function()&#123; document.writeln(\"show method\"); &#125; 测试 123456789101112131415161718document.write(\"&lt;pre&gt;\"); var p1 = new Person(\"孤傲苍狼\",24,115,160); var p2 = new Person(\"白虎神皇\",25,120,170); var p3 = new Person(); p3.init(\"玄天邪帝\",26,130,180);//调用public方法init初始化p3对象 document.writeln(\"p1 instanceof Person的结果是：\"+(p1 instanceof Person));//p1是Person类的实例，结果是true document.writeln(\"p2 instanceof Person的结果是：\"+(p2 instanceof Person));//p2是Person类的实例，结果是true document.writeln(\"p3 instanceof Person的结果是：\"+(p3 instanceof Person));//p3是Person类的实例，结果是true //当==两边的内容是对象或者是对象的函数属性时，则比较内存地址是否相等 document.writeln(\"当==两边的内容是对象或者是对象的函数属性时，则比较内存地址是否相等\"); document.writeln(\"比较p1和p2这两个对象的show方法的内存地址是否一样：p1.show == p2.show的结果是：\"+(p1.show == p2.show));//true document.writeln(\"p1.show == p2.show的结果是：\"+(p1.show == p2.show)+\"，这证明p1对象和p2对象共享一个show方法，在内存中show方法的代码只有一份，存放在内存的一块区域\");//true document.writeln(\"p1.name=\"+p1.name+\"，p1.age=\"+p1.age+\"，p1.weight=\"+p1.weight+\"，p1.height=\"+p1.height);//访问公有属性，这是可以正常访问的 document.writeln(\"p2.name=\"+p2.name+\"，p2.age=\"+p2.age+\"，p2.weight=\"+p2.weight+\"，p2.height=\"+p2.height);//访问公有属性，这是可以正常访问的 p3.name=\"灭世魔尊\";//为公共属性重新赋值 document.writeln(\"p3.name=\"+p3.name);//访问公有属性，这是可以正常访问的 p1.show();//调用类的公共函数，这次允许的 document.write(\"&lt;/pre&gt;\"); 写法二使用原型方式给类定义public属性和public方法更加优雅的写法，我个人推荐使用这种方式，这种方式看起来比较舒服 123456789101112131415161718192021222324/*定义类Person2*/ function Person2()&#123; &#125; /*使用原型方式给类定义public属性和public方法更加优雅的写法*/ Person2.prototype = &#123; name:\"\",//public属性 age:0,//public属性 weight:0,//public属性 height:0,//public属性 /*public方法*/ init:function(_name,_age,_weight,_height) &#123; this.name = _name; this.age = _age; this.weight=_weight; this.height=_height; document.writeln(\"this.name=\"+this.name+\",this.age=\"+this.age+\",this.weight=\"+this.weight+\",this.height=\"+this.height); &#125;, /*public方法*/ show:function()&#123; document.writeln(\"show method\"); &#125; &#125;; 测试代码： 1234567891011121314151617document.write(\"&lt;pre&gt;\"); var p2_1 = new Person2(); var p2_2 = new Person2(); p2_1.init(\"孤傲苍狼\",24,115,160); p2_2.init(\"白虎神皇\",25,120,170); document.writeln(\"p2_1.name=\"+p2_1.name+\"，p2_1.age=\"+p2_1.age+\"，p2_1.weight=\"+p2_1.weight+\"，p2_1.height=\"+p2_1.height);//访问公有属性，这是可以正常访问的 document.writeln(\"p2_2.name=\"+p2_2.name+\"，p2_2.age=\"+p2_2.age+\"，p2_2.weight=\"+p2_2.weight+\"，p2_2.height=\"+p2_2.height);//访问公有属性，这是可以正常访问的 document.writeln(\"p2_1 instanceof Person2的结果是：\"+(p2_1 instanceof Person2));//p2_1是Person2类的实例，结果是true document.writeln(\"p2_2 instanceof Person2的结果是：\"+(p2_2 instanceof Person2));//p2_2是Person2类的实例，结果是true //当==两边的内容是对象或者是对象的函数属性时，则比较内存地址是否相等 document.writeln(\"当==两边的内容是对象或者是对象的函数属性时，则比较内存地址是否相等\"); document.writeln(\"比较p2_1和p2_2这两个对象的init方法的内存地址是否一样：p2_1.init == p2_2.init的结果是：\"+(p2_1.init == p2_2.init));//true p2_1.name=\"灭世魔尊\";//为公共属性重新赋值 document.writeln(\"p2_1.name=\"+p2_1.name);//访问公有属性，这是可以正常访问的 p2_1.show();//调用类的公共函数，这次允许的 document.write(\"&lt;/pre&gt;\"); 构造函数+原型 构造函数方式和原型方式都有各自的优缺点，因此可以把这两种方式合并起来，用构造函数方式来定义类的属性(public属性，private属性)，用原型方式来定义类的方法(public方法)。互补不足，这就有了第三种写法。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849/*定义一个Person类*/ function Person(_name,_age,_salary)&#123; //在Person类内部定义类的public属性和private属性以及private方法 //Person类的公开属性，类的公开属性的定义方式是：”this.属性名“ this.name=_name; //Person类的私有属性，类的私有属性的定义方式是：”var 属性名“ var age=_age;//私有属性，只能在类内部使用 var salary=_salary;//私有属性，只能在类内部使用 /* 定义Person类的私有方法(内部方法)，只能在类内部使用 类的私有方法的定义方式是：”function functionName()&#123;.....&#125;“， 或者 var functionName=function()&#123;....&#125; */ function privateFn()&#123; document.write(\"&lt;pre&gt;\"); document.writeln(\"我是Person类的私有属性age，只能在Person类内部使用，初始化后age=\"+age); document.writeln(\"我是Person类的私有函数privateFn，只能在Person类内部使用\"); document.write(\"&lt;/pre&gt;\"); &#125; var privateFn2=function()&#123; document.write(\"&lt;pre&gt;\"); document.writeln(\"我是Person类的私有属性salary，只能在Person类内部使用，初始化后salary=\"+salary); document.writeln(\"我是Person类的私有函数privateFn2，只能在Person类内部使用\"); document.write(\"&lt;/pre&gt;\"); &#125; privateFn();//在Person类内部调用私有方法 privateFn2();//在Person类内部调用私有方法 &#125; //使用prototype原型方式定义的方法(public方法)是无法访问类的私有属性和私有方法的 //使用prototype原型方式定义Person类的方public方法 Person.prototype=&#123; setName:function(_name)&#123; this.name = _name; //privateFn();//不能调用Person类定义的私有方法privateFn()，会报错：缺少对象 &#125;, getName:function()&#123; return this.name; &#125;, show:function()&#123; document.writeln(\"公开方法show\"); &#125;, //公共方法 publicMethod:function()&#123; document.writeln(\"公开方法publicMethod\"); &#125; &#125;; 测试代码： 12345678910111213141516171819var p1 = new Person(\"孤傲苍狼\",24,2300); var p2 = new Person(\"白虎神皇\",25,3000); document.write(\"&lt;pre&gt;\"); document.writeln(\"p1 instanceof Person的结果是：\"+(p1 instanceof Person));//p1是Person类的实例，结果是true document.writeln(\"p2 instanceof Person的结果是：\"+(p2 instanceof Person));//p2是Person类的实例，结果是true //当==两边的内容是对象或者是对象的函数属性时，则比较内存地址是否相等 document.writeln(\"当==两边的内容是对象或者是对象的函数属性时，则比较内存地址是否相等\"); document.writeln(\"比较p1和p2这两个对象的show方法的内存地址是否一样：p1.show== p2.show的结果是：\"+(p1.show == p2.show));//true document.writeln(\"p1.show == p2.show的结果是：\"+(p1.show == p2.show)+\"，这证明p1对象和p2对象共享一个show方法，在内存中show方法的代码有1份，存放在1块内存区域\"); document.writeln(\"name是Person类定义的public属性，可以使用类的对象去直接访问类的public属性\"); document.writeln(\"p1.name=\"+p1.name);//访问公有属性，这是可以正常访问的 document.writeln(\"age和salary是Person类定义的private属性，不能使用类的对象去直接访问类私有属性，这是访问不了的，结果都是undefined\"); document.writeln(\"p1.age=\"+p1.age+\"，\"+\"p1.salary=\"+p1.salary)//不能使用类的对象去直接访问类私有属性，这是访问不了的，结果都是undefined p1.show();//调用类的公共函数，这次允许的 p1.publicMethod();//调用类的公共函数，这次允许的 p1.setName(\"玄天邪帝\");//调用类的公共函数设置为name属性重新赋值 document.writeln(\"p1.getName=\"+p1.getName()); //document.writeln(\"p1.privateFn()：\"+p1.privateFn()+\"&amp;nbsp;p1.privateFn2()：\"+p1.privateFn2());//不能使用类的对象去调用类的私有方法，这里会报错”对象不支持此属性或者方法 document.write(\"&lt;/pre&gt;\"); 第三种方式通过前两种方式的结合，算是达到了一个比较理想的写法了，可以通过传参构造对象实例，对象实例都共享同一份方法不造成内存浪费。第三种方式在开发中用得最多，我本人也是采用这种方式来编写JavaScript类。 查看原文","categories":[],"tags":[{"name":"JavaScript","slug":"JavaScript","permalink":"https://ydstudios.gitee.io/tags/JavaScript/"},{"name":"oop","slug":"oop","permalink":"https://ydstudios.gitee.io/tags/oop/"}]},{"title":"Dubbo消费者consumer捕捉服务提供者provider抛出的自定义异常","slug":"Dubbo消费者consumer捕捉服务提供者provider抛出的自定义异常","date":"2019-05-20T23:44:33.000Z","updated":"2020-09-05T02:01:16.000Z","comments":true,"path":"post/2e0453a7.html","link":"","permalink":"https://ydstudios.gitee.io/post/2e0453a7.html","excerpt":"我们在自己的业务系统中，通常会用到自定义的业务异常类，这个异常会继承extends RuntimeException，当发生业务限制的时候，会throw出来。但是在使用dubbo进行soa治理的时候，会发现provider抛出的异常，在custom端并不能正确的捕获。即便我们在provider和custom都有导入相同framework.jar下面的BusinessException异常，并且抛出这个异常，在Consumer端只会打印一行异常信息，获取不到正常的异常。百度才知道Dubbo对抛出得异常用一个ExceptionFilter的类进行拦截。","text":"我们在自己的业务系统中，通常会用到自定义的业务异常类，这个异常会继承extends RuntimeException，当发生业务限制的时候，会throw出来。但是在使用dubbo进行soa治理的时候，会发现provider抛出的异常，在custom端并不能正确的捕获。即便我们在provider和custom都有导入相同framework.jar下面的BusinessException异常，并且抛出这个异常，在Consumer端只会打印一行异常信息，获取不到正常的异常。百度才知道Dubbo对抛出得异常用一个ExceptionFilter的类进行拦截。 在一个公共包里定义一个异常类，继承自RuntimeException。 12345678910111213141516171819202122232425262728293031323334353637383940414243public class BusinessException extends RuntimeException &#123; /** * 异常code */ private String code; public BusinessException() &#123; super(); &#125; public BusinessException(String message) &#123; super(message); &#125; public BusinessException(String code, String message) &#123; super(message); this.code = code; &#125; public BusinessException(String message, Throwable cause) &#123; super(message, cause); &#125; public BusinessException(String code, String message, Throwable cause) &#123; super(message, cause); this.code = code; &#125; public BusinessException(Throwable cause) &#123; super(cause); &#125; public String getCode() &#123; return code; &#125; public void setCode(String code) &#123; this.code = code; &#125;&#125; 在服务提供者的Dubbo配置文件中添加下面的内容，主要就是自定义exceptionFilter，然后排出dubbo自带的exceptionFilter 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;beans xmlns=\"http://www.springframework.org/schema/beans\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns:dubbo=\"http://code.alibabatech.com/schema/dubbo\" xsi:schemaLocation=\"http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://code.alibabatech.com/schema/dubbo http://code.alibabatech.com/schema/dubbo/dubbo.xsd\"&gt; &lt;!-- http://dubbo.apache.org/schema/dubbo/dubbo.xsd 上面配置为这个一直报错，改为 http://code.alibabatech.com/schema/dubbo/dubbo.xsd --&gt; &lt;!--用于配置当前应用信息，不管该应用是提供者还是消费者 --&gt; &lt;dubbo:application name=\"dubbo-web-provide\"/&gt; &lt;!-- 用于配置连接注册中心相关信息 --&gt; &lt;dubbo:registry address=\"zookeeper://localhost:2181\" timeout=\"30000\"&gt; &lt;!--配置redis连接参数 --&gt; &lt;!--具体参数配置见com.alibaba.dubbo.registry.redis.RedisRegistry.class --&gt; &lt;dubbo:parameter key=\"max.idle\" value=\"10\" /&gt; &lt;dubbo:parameter key=\"min.idle\" value=\"5\" /&gt; &lt;dubbo:parameter key=\"max.active\" value=\"20\" /&gt; &lt;dubbo:parameter key=\"max.total\" value=\"100\" /&gt; &lt;/dubbo:registry&gt; &lt;!-- 最重要的就是下面的配置：自定义exceptionFilter，然后排出dubbo自带的exceptionFilter payload 设置传输的最大值 --&gt; &lt;dubbo:provider filter=\"dubboExceptionFilter,-exception\" payload=\"123886080\" &gt;&lt;/dubbo:provider&gt; &lt;!-- 用于配置提供服务的协议信息，协议由提供方指定，消费方被动接受 --&gt; &lt;dubbo:protocol name=\"dubbo\" port=\"20880\" accesslog=\"true\" serialization=\"hessian2\" /&gt; &lt;!-- 实现类 --&gt; &lt;bean id=\"helloService\" class=\"cn.ydstudio.dubbo.web.provide.serviceimpl.HelloServiceImpl\" /&gt; &lt;bean id=\"goodsService\" class=\"cn.ydstudio.dubbo.web.provide.serviceimpl.GoodsServiceImpl\" /&gt; &lt;bean id=\"goodsService2\" class=\"cn.ydstudio.dubbo.web.provide.serviceimpl.GoodsServiceImpl2\" /&gt; &lt;bean id=\"goodsOrderServiceRemoteImpl\" class=\"cn.ydstudio.dubbo.web.provide.serviceimpl.GoodsOrderServiceRemoteImpl\" /&gt; &lt;!--定义暴露服务的接口，用于暴露一个服务，定义服务的元信息，一个服务可以用多个协议暴露，一个服务也可以注册到多个注册中心 --&gt; &lt;!--每个接口都应定义版本号，为后续不兼容升级提供可能 --&gt; &lt;!--ref：服务的真正实现类 --&gt; &lt;dubbo:service interface=\"cn.ydstudio.dubbo.rpc.service.HelloService\" ref=\"helloService\" version=\"1.0.0\" retries=\"1\"/&gt; &lt;dubbo:service interface=\"cn.ydstudio.dubbo.rpc.service.GoodsService\" ref=\"goodsService\" version=\"1.0.0\" retries=\"1\"/&gt; &lt;dubbo:service interface=\"cn.ydstudio.dubbo.rpc.service.GoodsService\" ref=\"goodsService2\" version=\"2.0.0\" retries=\"1\"/&gt; &lt;dubbo:service interface=\"cn.ydstudio.dubbo.rpc.service.GoodsOrderServiceRemote\" ref=\"goodsOrderServiceRemoteImpl\" version=\"1.0.0\" retries=\"0\" &gt; &lt;dubbo:method name=\"querGoodsOrderList\" timeout=\"1000000\"/&gt; &lt;/dubbo:service&gt; &lt;!--监控中心配置 监控中心协议，如果为protocol=\"registry\"，表示从注册中心发现监控中心地址，否则直连监控中心。 --&gt; &lt;!--&lt;dubbo:monitor protocol=\"registry\"&gt;&lt;/dubbo:monitor&gt;--&gt; &lt;!-- 直连监控中心服务器地址 --&gt; &lt;!-- &lt;dubbo:monitor address=\"localhost:6379\"&gt;&lt;/dubbo:monitor&gt; --&gt; &lt;/beans&gt; 最重要的继承ExceptionFilter，然后重写invoke方法 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495package cn.ydstudio.dubbo.web.provide.filter;import cn.ydstudio.common.tools.exception.BizException;import cn.ydstudio.common.tools.exception.BusinessException;import com.alibaba.dubbo.common.logger.Logger;import com.alibaba.dubbo.common.logger.LoggerFactory;import com.alibaba.dubbo.common.utils.ReflectUtils;import com.alibaba.dubbo.common.utils.StringUtils;import com.alibaba.dubbo.rpc.*;import com.alibaba.dubbo.rpc.filter.ExceptionFilter;import com.alibaba.dubbo.rpc.service.GenericService;import java.lang.reflect.Method;/** * 功能描述:&lt;br/&gt; * * @Author Sam * @Date 2019/4/30 18:02 */public class DubboExceptionFilter extends ExceptionFilter &#123; private final Logger logger; public DubboExceptionFilter() &#123; this(LoggerFactory.getLogger(com.alibaba.dubbo.rpc.filter.ExceptionFilter.class)); &#125; public DubboExceptionFilter(Logger logger) &#123; this.logger = logger; &#125; @Override public Result invoke(Invoker&lt;?&gt; invoker, Invocation invocation) throws RpcException &#123; try &#123; Result result = invoker.invoke(invocation); if (result.hasException() &amp;&amp; GenericService.class != invoker.getInterface()) &#123; try &#123; Throwable exception = result.getException(); // 自定义的异常 if (exception instanceof BizException || exception instanceof BusinessException)&#123; return result; &#125; if (!(exception instanceof RuntimeException) &amp;&amp; exception instanceof Exception) &#123; return result; &#125; else &#123; try &#123; Method method = invoker.getInterface().getMethod(invocation.getMethodName(), invocation.getParameterTypes()); Class&lt;?&gt;[] exceptionClassses = method.getExceptionTypes(); Class[] arr$ = exceptionClassses; int len$ = exceptionClassses.length; for (int i$ = 0; i$ &lt; len$; ++i$) &#123; Class&lt;?&gt; exceptionClass = arr$[i$]; if (exception.getClass().equals(exceptionClass)) &#123; return result; &#125; &#125; &#125; catch (NoSuchMethodException var11) &#123; return result; &#125; this.logger.error(\"Got unchecked and undeclared exception which called by \" + RpcContext.getContext().getRemoteHost() + \". service: \" + invoker.getInterface().getName() + \", method: \" + invocation.getMethodName() + \", exception: \" + exception.getClass().getName() + \": \" + exception.getMessage(), exception); String serviceFile = ReflectUtils.getCodeBase(invoker.getInterface()); String exceptionFile = ReflectUtils.getCodeBase(exception.getClass()); if (serviceFile != null &amp;&amp; exceptionFile != null &amp;&amp; !serviceFile.equals(exceptionFile)) &#123; String className = exception.getClass().getName(); if (!className.startsWith(\"java.\") &amp;&amp; !className.startsWith(\"javax.\")) &#123; return (Result) (exception instanceof RpcException ? result : new RpcResult(new RuntimeException(StringUtils.toString(exception)))); &#125; else &#123; return result; &#125; &#125; else &#123; return result; &#125; &#125; &#125; catch (Throwable var12) &#123; this.logger.warn(\"Fail to ExceptionFilter when called by \" + RpcContext.getContext().getRemoteHost() + \". service: \" + invoker.getInterface().getName() + \", method: \" + invocation.getMethodName() + \", exception: \" + var12.getClass().getName() + \": \" + var12.getMessage(), var12); return result; &#125; &#125; else &#123; return result; &#125; &#125; catch (RuntimeException var13) &#123; this.logger.error(\"Got unchecked and undeclared exception which called by \" + RpcContext.getContext().getRemoteHost() + \". service: \" + invoker.getInterface().getName() + \", method: \" + invocation.getMethodName() + \", exception: \" + var13.getClass().getName() + \": \" + var13.getMessage(), var13); throw var13; &#125; &#125;&#125; 然后在provider可以直接抛出异常，在consumer可以直接捕捉到。consumer服务消费端可以新建一个全局异常处理的控制器，可以格式化后输出给前端。","categories":[],"tags":[{"name":"dubbo","slug":"dubbo","permalink":"https://ydstudios.gitee.io/tags/dubbo/"}]},{"title":"小试阿里巴巴EasyExcel导出Excel","slug":"小试阿里巴巴EasyExcel导出Excel","date":"2019-05-17T23:43:39.000Z","updated":"2019-12-09T15:03:26.000Z","comments":true,"path":"post/76eb7bfd.html","link":"","permalink":"https://ydstudios.gitee.io/post/76eb7bfd.html","excerpt":"添加easyexcel的maven依赖12345&lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;easyexcel&lt;/artifactId&gt; &lt;version&gt;1.1.1&lt;/version&gt; &lt;/dependency&gt;","text":"添加easyexcel的maven依赖12345&lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;easyexcel&lt;/artifactId&gt; &lt;version&gt;1.1.1&lt;/version&gt; &lt;/dependency&gt; 导出的模型需要继承BaseRowModel，然后可以在每个属性上设置所在的列和一些format12345678910111213141516171819202122232425262728293031323334public class GoodsOrderExportDto extends BaseRowModel &#123; @ExcelProperty(value = \"订单序号\",index = 0) private Long orderId; @ExcelProperty(value = \"订单编号\",index = 1) private String orderCode; @ExcelProperty(value = \"商品序号\",index = 2) private Long goodsId; @ExcelProperty(value = \"商品名称\",index = 3) private String goodsTitle; @ExcelProperty(value = \"商品链接\",index = 4) private String goodsDetailsUrl; @ExcelProperty(value = \"商品图片\",index = 5) private String goodsPhoto; @ExcelProperty(value = \"商品数量\",index = 6) private Long goodsNum; @ExcelProperty(value = \"商品价格\",index = 7) private BigDecimal goodsAmt; @ExcelProperty(value = \"区域公司\",index = 8) private Long areaComp; @ExcelProperty(value = \"付款时间\",index = 12,format = \"yy-MM-dd hh:mm:ss\") private Date payTime;&#125; 最后在控制器输出12345678910111213141516171819202122232425262728293031323334@RequestMapping(value = \"export\",method = RequestMethod.GET) public void exportExcel(HttpServletRequest request, HttpServletResponse response) throws IOException &#123; List&lt;GoodsOrderDto&gt; orderDtoList = goodsOrderServiceRemote.querGoodsOrderList(); List&lt;GoodsOrderExportDto&gt; orderExportDtoList = Collections.emptyList(); if (CollectionUtils.isNotEmpty(orderDtoList))&#123; orderExportDtoList = new ArrayList&lt;&gt;(orderDtoList.size()); for (GoodsOrderDto goodsOrderDto : orderDtoList) &#123; GoodsOrderExportDto goodsOrderExportDto = new GoodsOrderExportDto(); BeanUtils.copyProperties(goodsOrderDto,goodsOrderExportDto); orderExportDtoList.add(goodsOrderExportDto); &#125; orderDtoList.clear(); &#125; // 设定输出文件头 response.setHeader(\"Content-disposition\", \"attachment; filename=\" + URLEncoder.encode(\"订单导出.xls\", \"UTF-8\")); // 定义输出类型 response.setContentType(\"application/msexcel\"); OutputStream outputStream = response.getOutputStream(); ExcelWriter excelWriter = new ExcelWriter(outputStream, ExcelTypeEnum.XLS); Sheet sheet = new Sheet(1, 0, GoodsOrderExportDto.class); excelWriter.write(orderExportDtoList,sheet); outputStream.flush(); excelWriter.finish(); outputStream.close(); &#125;","categories":[],"tags":[{"name":"EasyExcel","slug":"EasyExcel","permalink":"https://ydstudios.gitee.io/tags/EasyExcel/"}]},{"title":"查看依赖第三方jar最低要求运行的jdk版本","slug":"查看依赖第三方jar最低要求运行的jdk版本","date":"2019-05-17T23:42:49.000Z","updated":"2019-12-09T15:03:26.000Z","comments":true,"path":"post/58af8f82.html","link":"","permalink":"https://ydstudios.gitee.io/post/58af8f82.html","excerpt":"最近在比较阿里巴巴的easyexcel和easypoi的excel导出功能哪个强大。两者的demo我都是写在同一个工程里面的，easyexcel的demo是先写的，使用起来方便简洁。然后开始写easypoi的demo时，首先出现了依赖下载不了，依赖能下载之后，还出现了下面的乱七八糟的问题：","text":"最近在比较阿里巴巴的easyexcel和easypoi的excel导出功能哪个强大。两者的demo我都是写在同一个工程里面的，easyexcel的demo是先写的，使用起来方便简洁。然后开始写easypoi的demo时，首先出现了依赖下载不了，依赖能下载之后，还出现了下面的乱七八糟的问题： 123Handler dispatch failed; nested exception is java.lang.NoClassDefFoundError: cn/afterturn/easypoi/excel/entity/ExportParamsUnsupported major.minor version 52.0 刚开始我是以为easypoi的依赖包没有完整下下来，后来检查发现应该全部下载了。后来我想是不是依赖没有添加全，去easypoi的官方网站看，我的依赖使用应该是没有问题的，但是我还是把easypoi的版本从4.0.0降到了3.30，但是demo运行起来还是报错。后来我想到easyexcel也是依赖poi的，是不是他俩又版本依赖的冲突。于是我把easyexcel给注释了，于是我就成功了！~我嘞个去！！！！！出现第一个错误就是因为easyexcel和easypoi的依赖有冲突。出现第二个错误是英文，使用的easypoi的版本过高，我使用的是easypoi4.0.0,他需要的jdk的版本是jdk8以上的。 那如何查看第三方依赖最低要求运行的jdk版本呢？ 在第三方jar包下会存在一个META-INF目录，该目录下有一个MANIFEST.MF文件，这个文件以键值对的形式存储一些关于jar包的关键信息。下面我以easypoi-base的MANIFEST.MF文件做为示例： 12345Manifest-Version: 1.0Archiver-Version: Plexus ArchiverBuilt-By: jueyueCreated-By: Apache Maven 3.1.1Build-Jdk: 1.8.0_144 可是一个.jar文件中，如果没有这个字段，就必须通过下面一种方法来查看 通过反编译.class文件来查看解压.jar包，能得到.class文件。用JDK自带的javap反编译.class文件，用如下命令： 1234javap -verbose Configuration.class#查看得到的信息中，major version属性的内容，如下major version: 52 说明这个.class文件是由JDK1.8编译得到的。 Java 9 uses major version 53 J2SE 8.0 = 52(0x33 hex) J2SE 7.0 = 51(0x32 hex) J2SE 6.0 = 50 (0x32 hex) J2SE 5.0 = 49 (0x31 hex) JDK 1.4 = 48 (0x30 hex) JDK 1.3 = 47 (0x2F hex) JDK 1.2 = 46 (0x2E hex) JDK 1.1 = 45 (0x2D hex) 注意：一个.jar包中可能有多个.class文件，每个.class的JDK版本可能会不一样（编译器多个项目设置不同） 我电脑上安装的是jdk7。所以解决办法是删掉easyexcel的依赖，使用easypoi运行和自己jdk相同版本的就行了","categories":[],"tags":[{"name":"java","slug":"java","permalink":"https://ydstudios.gitee.io/tags/java/"}]},{"title":"Dubbo出现ExceedPayloadLimitException: Data length too large的错误","slug":"Dubbo出现ExceedPayloadLimitException-Data-length-too-large的错误","date":"2019-05-14T23:41:47.000Z","updated":"2019-12-09T15:03:26.000Z","comments":true,"path":"post/45135675.html","link":"","permalink":"https://ydstudios.gitee.io/post/45135675.html","excerpt":"在使用dubbo的时候，一次请求的数据量过大的时候，会出现下面的问题。","text":"在使用dubbo的时候，一次请求的数据量过大的时候，会出现下面的问题。 123456789101112131415161718192021222324252627com.alibaba.dubbo.remoting.transport.ExceedPayloadLimitException: Data length too large: 12470880, max payload: 8388608, channel: NettyChannel [channel=[id: 0x78767b1e, /10.23.144.194:54414 =&gt; /10.23.144.194:20880]] at com.alibaba.dubbo.remoting.transport.AbstractCodec.checkPayload(AbstractCodec.java:44) ~[dubbo-2.6.2.jar:2.6.2] at com.alibaba.dubbo.remoting.exchange.codec.ExchangeCodec.encodeResponse(ExchangeCodec.java:288) [dubbo-2.6.2.jar:2.6.2] at com.alibaba.dubbo.remoting.exchange.codec.ExchangeCodec.encode(ExchangeCodec.java:73) [dubbo-2.6.2.jar:2.6.2] at com.alibaba.dubbo.rpc.protocol.dubbo.DubboCountCodec.encode(DubboCountCodec.java:38) [dubbo-2.6.2.jar:2.6.2] at com.alibaba.dubbo.remoting.transport.netty.NettyCodecAdapter$InternalEncoder.encode(NettyCodecAdapter.java:80) [dubbo-2.6.2.jar:2.6.2] at org.jboss.netty.handler.codec.oneone.OneToOneEncoder.handleDownstream(OneToOneEncoder.java:66) [netty-3.2.5.Final.jar:na] at org.jboss.netty.channel.DefaultChannelPipeline.sendDownstream(DefaultChannelPipeline.java:591) [netty-3.2.5.Final.jar:na] at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendDownstream(DefaultChannelPipeline.java:776) [netty-3.2.5.Final.jar:na] at org.jboss.netty.channel.SimpleChannelHandler.writeRequested(SimpleChannelHandler.java:304) [netty-3.2.5.Final.jar:na] at com.alibaba.dubbo.remoting.transport.netty.NettyHandler.writeRequested(NettyHandler.java:98) [dubbo-2.6.2.jar:2.6.2] at org.jboss.netty.channel.SimpleChannelHandler.handleDownstream(SimpleChannelHandler.java:266) [netty-3.2.5.Final.jar:na] at org.jboss.netty.channel.DefaultChannelPipeline.sendDownstream(DefaultChannelPipeline.java:591) [netty-3.2.5.Final.jar:na] at org.jboss.netty.channel.DefaultChannelPipeline.sendDownstream(DefaultChannelPipeline.java:582) [netty-3.2.5.Final.jar:na] at org.jboss.netty.channel.Channels.write(Channels.java:611) [netty-3.2.5.Final.jar:na] at org.jboss.netty.channel.Channels.write(Channels.java:578) [netty-3.2.5.Final.jar:na] at org.jboss.netty.channel.AbstractChannel.write(AbstractChannel.java:251) [netty-3.2.5.Final.jar:na] at com.alibaba.dubbo.remoting.transport.netty.NettyChannel.send(NettyChannel.java:100) [dubbo-2.6.2.jar:2.6.2] at com.alibaba.dubbo.remoting.transport.AbstractPeer.send(AbstractPeer.java:53) [dubbo-2.6.2.jar:2.6.2] at com.alibaba.dubbo.remoting.exchange.support.header.HeaderExchangeHandler.received(HeaderExchangeHandler.java:173) [dubbo-2.6.2.jar:2.6.2] at com.alibaba.dubbo.remoting.transport.DecodeHandler.received(DecodeHandler.java:51) [dubbo-2.6.2.jar:2.6.2] at com.alibaba.dubbo.remoting.transport.dispatcher.ChannelEventRunnable.run(ChannelEventRunnable.java:80) [dubbo-2.6.2.jar:2.6.2] at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) [na:1.7.0_80] at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [na:1.7.0_80] at java.lang.Thread.run(Thread.java:745) [na:1.7.0_80]2019-05-14 10:27:30.194 WARN 10424 --- [:20880-thread-4] c.a.d.r.exchange.codec.ExchangeCodec : [DUBBO] Data length too large: 12470880, max payload: 8388608, channel: NettyChannel [channel=[id: 0x78767b1e, /10.23.144.194:54414 =&gt; /10.23.144.194:20880]], dubbo version: 2.6.2, current host: 10.23.144.194 当Dubbo服务提供者向消费者传输大数据容量的对象时，会受到Dubbo的限制，抛出上面的Data length too large异常编辑dubbo.xml,添加payload配置，默认是8M，我给改成80M。这样做简单粗暴，其实不太符合Dubbo的设计理念。 1&lt;dubbo:provider filter=\"dubboExceptionFilter,-exception\" payload=\"83886080\" &gt;&lt;/dubbo:provider&gt;","categories":[],"tags":[{"name":"dubbo","slug":"dubbo","permalink":"https://ydstudios.gitee.io/tags/dubbo/"}]},{"title":"MyBatis Generator配置文件--指定生成实体类使用实际的表列名作为实体类的属性名","slug":"MyBatis-Generator配置文件-指定生成实体类使用实际的表列名作为实体类的属性名","date":"2019-05-13T23:40:33.000Z","updated":"2019-12-09T15:03:26.000Z","comments":true,"path":"post/f1ed0391.html","link":"","permalink":"https://ydstudios.gitee.io/post/f1ed0391.html","excerpt":"今天在用MyBatis Generator生成表实体的时候，发现数据库中的表字段使用的是小驼峰命名规则，生成出来的实体属性都是小写了，没有转成小驼峰，一番搜索得知可以在MyBatis Generator的配置文件中修改。","text":"今天在用MyBatis Generator生成表实体的时候，发现数据库中的表字段使用的是小驼峰命名规则，生成出来的实体属性都是小写了，没有转成小驼峰，一番搜索得知可以在MyBatis Generator的配置文件中修改。 table标签下的设置属性useActualColumnNames用于指定生成实体类时是否使用实际的列名作为实体类的属性名，取值true或false。 true：MyBatis Generator会使用数据库中实际的字段名字作为生成的实体类的属性名。 false：这是默认值。如果设置为false,则MyBatis Generator会将数据库中实际的字段名字转换为Camel Case风格作为生成的实体类的属性名。 如果明确的使用columnOverride元素指定了字段对应的实体的属性名,那么useActualColumnNames会被忽略。 假设表有一个字段名为start_date,如果这个属性设置为true,则生成的实体类的属性名为start_date,生成的setter/getter为 setStart_date/getStart_date。如果useActualColumnNames设置为false,则生成的实体类的属性名为startDate,生成的setter/getter为setStartDate/getStartDate。 123456&lt;table tableName=\"dobbo_goods_order\" domainObjectName=\"GoodsOrder\" enableCountByExample=\"false\" enableUpdateByExample=\"false\" enableDeleteByExample=\"false\" enableSelectByExample=\"false\" selectByExampleQueryId=\"false\" &gt; &lt;!-- table标签下的设置属性useActualColumnNames用于指定生成实体类时是否使用实际的列名作为实体类的属性名，取值true或false--&gt; &lt;property name=\"useActualColumnNames\" value=\"true\" /&gt; &lt;/table&gt;","categories":[],"tags":[{"name":"mybatis","slug":"mybatis","permalink":"https://ydstudios.gitee.io/tags/mybatis/"}]},{"title":"Dubbo由于连接不上monitor监控中心报错","slug":"Dubbo由于连接不上monitor监控中心报错","date":"2019-05-08T23:38:49.000Z","updated":"2019-12-09T15:03:26.000Z","comments":true,"path":"post/9008dc8b.html","link":"","permalink":"https://ydstudios.gitee.io/post/9008dc8b.html","excerpt":"dubbbo使用时候没有启动监控中心，不能配了监控地址。","text":"dubbbo使用时候没有启动监控中心，不能配了监控地址。 12345678910111213141516com.alibaba.dubbo.rpc.RpcException: Failed to invoke the method collect in the service com.alibaba.dubbo.monitor.MonitorService. No provider available for the service com.alibaba.dubbo.monitor.MonitorService from registry localhost:2181 on the consumer 10.23.144.194 using the dubbo version 2.6.2. Please check if the providers have been started and registered. at com.alibaba.dubbo.rpc.cluster.support.AbstractClusterInvoker.checkInvokers(AbstractClusterInvoker.java:257) ~[dubbo-2.6.2.jar:2.6.2] at com.alibaba.dubbo.rpc.cluster.support.FailoverClusterInvoker.doInvoke(FailoverClusterInvoker.java:56) ~[dubbo-2.6.2.jar:2.6.2] at com.alibaba.dubbo.rpc.cluster.support.AbstractClusterInvoker.invoke(AbstractClusterInvoker.java:238) ~[dubbo-2.6.2.jar:2.6.2] at com.alibaba.dubbo.rpc.cluster.support.wrapper.MockClusterInvoker.invoke(MockClusterInvoker.java:75) ~[dubbo-2.6.2.jar:2.6.2] at com.alibaba.dubbo.rpc.proxy.InvokerInvocationHandler.invoke(InvokerInvocationHandler.java:52) ~[dubbo-2.6.2.jar:2.6.2] at com.alibaba.dubbo.common.bytecode.proxy0.collect(proxy0.java) ~[dubbo-2.6.2.jar:2.6.2] at com.alibaba.dubbo.monitor.dubbo.DubboMonitor.send(DubboMonitor.java:112) ~[dubbo-2.6.2.jar:2.6.2] at com.alibaba.dubbo.monitor.dubbo.DubboMonitor$1.run(DubboMonitor.java:69) ~[dubbo-2.6.2.jar:2.6.2] at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471) [na:1.7.0_80] at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:304) [na:1.7.0_80] at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:178) [na:1.7.0_80] at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293) [na:1.7.0_80] at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) [na:1.7.0_80] at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [na:1.7.0_80] at java.lang.Thread.run(Thread.java:745) [na:1.7.0_80] 解决方案：这是因为没有启动监控中心，却配了监控地址，把监控中心启动，或者把xml配置中的&lt;dubbo:monitor protocol=”registry”&gt;或properties配置中的dubbo.monitor.protocol=registry去掉，即可。","categories":[],"tags":[{"name":"dubbo","slug":"dubbo","permalink":"https://ydstudios.gitee.io/tags/dubbo/"}]},{"title":"Java中ListIterator和Iterator的异同","slug":"Java中ListIterator和Iterator的异同","date":"2019-05-08T23:37:09.000Z","updated":"2019-12-09T15:03:26.000Z","comments":true,"path":"post/5da32797.html","link":"","permalink":"https://ydstudios.gitee.io/post/5da32797.html","excerpt":"在使用Java集合的时候，都需要使用Iterator。但是java集合中还有一个迭代器ListIterator,在使用List、ArrayList、LinkedList和Vector的时候可以使用。这两种迭代器有什么区别呢？下面我们详细分析。这里有一点需要明确的时候，迭代器指向的位置是元素之前的位置。","text":"在使用Java集合的时候，都需要使用Iterator。但是java集合中还有一个迭代器ListIterator,在使用List、ArrayList、LinkedList和Vector的时候可以使用。这两种迭代器有什么区别呢？下面我们详细分析。这里有一点需要明确的时候，迭代器指向的位置是元素之前的位置。 首先看一下Iterator和ListIterator迭代器的方法有哪些。 Iterator迭代器包含的方法有： hasNext()：如果迭代器指向位置后面还有元素，则返回 true，否则返回false next()：返回集合中Iterator指向位置后面的元素 remove()：删除集合中Iterator指向位置后面的元素 ListIterator迭代器包含的方法有： add(E e): 将指定的元素插入列表，插入位置为迭代器当前位置之前 hasNext()：以正向遍历列表时，如果列表迭代器后面还有元素，则返回 true，否则返回false hasPrevious():如果以逆向遍历列表，列表迭代器前面还有元素，则返回 true，否则返回false next()：返回列表中ListIterator指向位置后面的元素 nextIndex():返回列表中ListIterator所需位置后面元素的索引 previous():返回列表中ListIterator指向位置前面的元素 previousIndex()：返回列表中ListIterator所需位置前面元素的索引 remove():从列表中删除next()或previous()返回的最后一个元素（有点拗口，意思就是对迭代器使用hasNext()方法时，删除ListIterator指向位置后面的元素；当对迭代器使用hasPrevious()方法时，删除ListIterator指向位置前面的元素） set(E e)：从列表中将next()或previous()返回的最后一个元素返回的最后一个元素更改为指定元素e 相同点都是迭代器，当需要对集合中元素进行遍历不需要干涉其遍历过程时，这两种迭代器都可以使用。 不同点 使用范围不同，Iterator可以应用于所有的集合，Set、List和Map和这些集合的子类型。而ListIterator只能用于List及其子类型。 ListIterator有add方法，可以向List中添加对象，而Iterator不能。 ListIterator和Iterator都有hasNext()和next()方法，可以实现顺序向后遍历，但是ListIterator有hasPrevious()和previous()方法，可以实现逆向（顺序向前）遍历。Iterator不可以。 ListIterator可以定位当前索引的位置，nextIndex()和previousIndex()可以实现。Iterator没有此功能。 都可实现删除操作，但是ListIterator可以实现对象的修改，set()方法可以实现。Iterator仅能遍历，不能修改。 1234567891011121314151617181920212223ArrayList&lt;String&gt; stringArrayList1 = new ArrayList&lt;String&gt;(); ArrayList&lt;String&gt; stringArrayList2 = new ArrayList&lt;String&gt;(); stringArrayList1.add(\"ok\"); stringArrayList1.add(\"hello\"); stringArrayList1.add(\"world\"); stringArrayList2.add(\"好的\"); stringArrayList2.add(\"你好\"); stringArrayList2.add(\"世界\"); stringArrayList1.addAll(stringArrayList2); ListIterator&lt;String&gt; iterator = stringArrayList1.listIterator(); System.out.println(\"从前往后输出:\"); while (iterator.hasNext())&#123; System.out.println(\"next=\"+iterator.next()); &#125; System.out.println(\"\\r\\n从后往前输出:\"); while (iterator.hasPrevious())&#123; System.out.println(\"previous=\"+iterator.previous()); &#125; 注意：一定要先进行由前向后输出，之后才能进行由后向前的输出。","categories":[],"tags":[{"name":"java","slug":"java","permalink":"https://ydstudios.gitee.io/tags/java/"},{"name":"ListIterator","slug":"ListIterator","permalink":"https://ydstudios.gitee.io/tags/ListIterator/"},{"name":"Iterator","slug":"Iterator","permalink":"https://ydstudios.gitee.io/tags/Iterator/"}]},{"title":"深入理解spring注解之@Bean注解","slug":"深入理解spring注解之-Bean注解","date":"2019-04-23T23:36:01.000Z","updated":"2019-12-09T15:03:26.000Z","comments":true,"path":"post/2d1810c4.html","link":"","permalink":"https://ydstudios.gitee.io/post/2d1810c4.html","excerpt":"@Bean是一个方法级别上的注解，主要用在@Configuration注解的类里，也可以用在@Component注解的类里。添加的bean的id为方法名。","text":"@Bean是一个方法级别上的注解，主要用在@Configuration注解的类里，也可以用在@Component注解的类里。添加的bean的id为方法名。 12345678910111213141516171819202122@Target(&#123;ElementType.METHOD, ElementType.ANNOTATION_TYPE&#125;)@Retention(RetentionPolicy.RUNTIME)@Documentedpublic @interface Bean &#123; @AliasFor(\"name\") String[] value() default &#123;&#125;; @AliasFor(\"value\") String[] name() default &#123;&#125;; Autowire autowire() default Autowire.NO; String initMethod() default \"\"; String destroyMethod() default \"(inferred)\";&#125;value -- bean别名和name是相互依赖关联的，value,name如果都使用的话值必须要一致name -- bean名称，如果不写会默认为注解的方法名称autowire -- 自定装配默认是不开启的，建议尽量不要开启，因为自动装配不能装配基本数据类型、字符串、数组等，这是自动装配设计的局限性，以及自动装配不如显示依赖注入精确initMethod -- bean的初始化之前的执行方法，该参数一般不怎么用，因为可以完全可以在代码中实现destroyMethod -- bean销毁执行的方法 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190package net.ydstudio.practise.bean;import org.springframework.context.annotation.*;/** * 功能描述:&lt;br/&gt; * @Bean意思是定义一个bean，该注解是一个方法级别上的注解，可以使用在@Configuration的类里，也可以 * 在@Component注解的类里，添加的bean的id为方法的名称 * * @Date 2019/4/23 19:36 */@Configurationpublic class AppConfig &#123; /** * * 下面的代码就相当于在xml之中添加下面的配置 * &lt;beans&gt; * &lt;bean id=\"transferService\" class=\"net.ydstudio.practise.bean.TransferServiceImpl“ &gt;&lt;/bean&gt; * &lt;/beans&gt; * * 还能够使用@Scope注解来指定使用@Bean定义的bean * singleton : 单例，整个应用中只存在一个实例bean * prototype ： 与单例相对，每次getBean都会重新生成一个Bean。 * request ： web环境下，每个请求都会创建一个bean，在一次请求中只存在一个Bean，不同request的bean不同 * session ： web环境下，session生命周期下，获取的是同一个bean * * 默认情况下，所有单实例bean都会在创建spring容器的时候创建， * 如果在bean第一次使用的时候创建，我们称为懒加载 * 配置很简单，在创建bean的方法上添加@Lazy注解即可 * * @return net.ydstudio.practise.bean.TransferService * @date 2019/4/23 19:41 */ @Bean @Lazy @Scope(\"singleton\") public TransferService transferService()&#123; return new TransferServiceImpl(); &#125; /** * @Bean可以依赖其他任意数量的bean，如果TransferService依赖Foo，我们可以通过方法参数实现这个依赖 * * @param: foo * @return net.ydstudio.practise.bean.TransferService * @date 2019/4/24 11:23 */ @Bean public TransferService transferService(Foo foo)&#123; return new TransferServiceImpl(foo); &#125; /** * * 任何使用@Bean定义的bean，也可以执行生命周期的回调函数 * 类似@PostConstruct and @PreDestroy的方法 * * @return net.ydstudio.practise.bean.Foo * @date 2019/4/23 19:49 */ @Bean(initMethod = \"init\") public Foo foo()&#123; return new Foo(); &#125; /** * 任何使用@Bean定义的bean，也可以执行生命周期的回调函数 * 类似@PostConstruct and @PreDestroy的方法 * @param: * @return net.ydstudio.practise.bean.Bar * @date 2019/4/23 19:50 */ @Bean(destroyMethod = \"cleanUp\") public Bar bar()&#123; return new Bar(); &#125; /** * * 默认情况下bean的名称和方法名称相同，你也可以使用 * 注解中的name属性进行指定 * * @param: * @return net.ydstudio.practise.bean.Foo * @date 2019/4/23 19:52 */ @Bean(name = \"myFoo\") public Foo getFoo()&#123; return new Foo(); &#125; /** * 可以通过某些条件，来选择是否注册Bean，通过@Condition注解来实现 * 实现Condition接口，并重写matches方法，根据该方法返回的布尔值来决定是否注册Bean * @return net.ydstudio.practise.bean.Book * @date 2019/4/24 11:54 */ @Bean @Conditional(MyCondition.class) public Book book()&#123; return new Book(); &#125;&#125;public class Bar &#123; public void cleanUp()&#123; System.out.println(\"---111111cleanUp---\"); &#125; @Override public String toString() &#123; final StringBuffer sb = new StringBuffer(\"Bar&#123;\"); sb.append('&#125;'); return sb.toString(); &#125;&#125;public class Book &#123; private String name; private BigDecimal price; private Date publishDate; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; public BigDecimal getPrice() &#123; return price; &#125; public void setPrice(BigDecimal price) &#123; this.price = price; &#125; public Date getPublishDate() &#123; return publishDate; &#125; public void setPublishDate(Date publishDate) &#123; this.publishDate = publishDate; &#125;&#125;public class Foo &#123; public void init()&#123; System.out.println(\"初始化之前---init---初始化之前\"); &#125; @Override public String toString() &#123; final StringBuffer sb = new StringBuffer(\"Foo&#123;\"); sb.append('&#125;'); return sb.toString(); &#125;&#125;public class MyCondition implements Condition &#123; @Override public boolean matches(ConditionContext conditionContext, AnnotatedTypeMetadata annotatedTypeMetadata) &#123; /** * 根据环境变量是否存在my.env=hello的属性来决定是否创建， * 可以通过启动参数指定-Dmy.env=hello来测试。 **/ Environment environment = conditionContext.getEnvironment(); String property = environment.getProperty(\"my.env\"); if (\"hello\".equals(property)) &#123; return true; &#125; return false; &#125;&#125;public interface TransferService &#123;&#125;public class TransferServiceImpl implements TransferService&#123; private Foo foo; public TransferServiceImpl(Foo foo) &#123; this.foo = foo; &#125; public TransferServiceImpl() &#123; &#125;&#125;","categories":[],"tags":[{"name":"spring","slug":"spring","permalink":"https://ydstudios.gitee.io/tags/spring/"},{"name":"bean","slug":"bean","permalink":"https://ydstudios.gitee.io/tags/bean/"}]},{"title":"idea自动生成方法注释（含参数及返回值）","slug":"idea自动生成方法注释（含参数及返回值）","date":"2019-04-22T23:35:02.000Z","updated":"2020-12-06T14:02:12.000Z","comments":true,"path":"post/526abd46.html","link":"","permalink":"https://ydstudios.gitee.io/post/526abd46.html","excerpt":"程序员到新的公司上班，拿到新电脑当然要第一时间配置好各种开发环境和开发工具。","text":"程序员到新的公司上班，拿到新电脑当然要第一时间配置好各种开发环境和开发工具。 在idea中打开设置，选择File-&gt;settings-&gt;Editor-&gt;Live Templates点击”+”-&gt;选择template group ，然后输入group的name，如annotation。然后，选中刚才创建的myGroup，然后再次点击右侧的绿色+，这次选择的是Live Template。取名（Abbreviation）为*。Description中填写”方法注释” 配置模板选择刚刚添加的Live Template，在下面的Text Template中添加下面的内容，注意不要整理下面的格式。 12345678* * * $VAR1$ $params$ * @return $returns$ * @author 19037900@xx.cn * @date $date$ $time$ */ 注意：此处一定要为刚刚的模板指定使用范围,选择全部就行了。 配置variables，点击 Edit variablesVAR1参数不用管params (网上好多都不行，这个是我正在用的) 1groovyScript(\"def result=''; def params=\\\"$&#123;_1&#125;\\\".replaceAll('[\\\\\\\\[|\\\\\\\\]|\\\\\\\\s]', '').split(',').toList(); for(i = 0; i &lt; params.size(); i++) &#123;result+='* @'+'param: ' + params[i] + ((i &lt; params.size() - 1) ? '\\\\n'+'\\\\b' : '')&#125;; return result\", methodParameters()) returns参数可以下拉，选择methodReturnType();date参数可以下拉，选择date();time参数可以下拉，选择time(); 配置快捷键，点击Edit variables下面的Options中的Expand with下拉选择Enter。开始使用输入 /**，然后Enter，大功告成","categories":[],"tags":[{"name":"idea","slug":"idea","permalink":"https://ydstudios.gitee.io/tags/idea/"}]},{"title":"Windows下搭建Redis高可用集群-哨兵模式（Redis-Sentinel）","slug":"Windows下搭建Redis高可用集群-哨兵模式（Redis-Sentinel","date":"2019-02-28T23:32:15.000Z","updated":"2020-10-02T03:11:00.000Z","comments":true,"path":"post/bdbc3b8a.html","link":"","permalink":"https://ydstudios.gitee.io/post/bdbc3b8a.html","excerpt":"Sentinel的分布式特性Redis Sentinel 是一个分布式系统， 你可以在一个架构中运行多个 Sentinel 进程（progress）， 这些进程使用流言协议（gossip protocols)来接收关于主服务器是否下线的信息， 并使用投票协议（agreement protocols）来决定是否执行自动故障迁移， 以及选择哪个从服务器作为新的主服务器。","text":"Sentinel的分布式特性Redis Sentinel 是一个分布式系统， 你可以在一个架构中运行多个 Sentinel 进程（progress）， 这些进程使用流言协议（gossip protocols)来接收关于主服务器是否下线的信息， 并使用投票协议（agreement protocols）来决定是否执行自动故障迁移， 以及选择哪个从服务器作为新的主服务器。 单个sentinel进程来监控redis集群是不可靠的，当sentinel进程宕掉后(sentinel本身也有单点问题，single-point-of-failure)整个集群系统将无法按照预期的方式运行。所以有必要将sentinel集群，这样有几个好处： 有一些sentinel进程宕掉了，依然可以进行redis集群的主备切换； 如果只有一个sentinel进程，如果这个进程运行出错，或者是网络堵塞，那么将无法实现redis集群的主备切换（单点问题）； 如果有多个sentinel，redis的客户端可以随意地连接任意一个sentinel来获得关于redis集群中的信息； 一个健壮的部署至少需要三个哨兵实例。三个哨兵实例应该放置在客户使用独立方式确认故障的计算机或虚拟机中。例如不同的物理机或不同可用区域的虚拟机。 开始准备环境搭建：本次在window上搭建哨兵模式的Redis集群。按照上一篇文章Redis主从复制的指导，先搭建好Redis的集群，具体情况如下：在git上下载好window下的Redis免安装的包，解压配置好之后，再复制两份，端口号分别为6380和6381，Redis安装目录则分别为redis6379、redis6380、redis6381. 127.0.0.1 6379 master 127.0.0.1 6380 slave 127.0.0.1 6381 slave 127.0.0.1:26379 127.0.0.1:26380 127.0.0.1:26381 sentinel Redis的密码都设置成requirepass 123456 配置Sentinel在上述三个Redis安装目录下新建sentinel.conf文件，在其中写入以下内容： 1234567891011121314# 这个是Redis6379配置内容，其他文件同理新增然后改一下端口即可，26380，和 26381。#当前Sentinel服务运行的端口port 26379 # 哨兵监听的主服务器 sentinel monitor mymaster 127.0.0.1 6379 2# 3s内mymaster无响应，则认为mymaster宕机了sentinel down-after-milliseconds mymaster 3000#如果10秒后,mysater仍没启动过来，则启动failover sentinel failover-timeout mymaster 10000 # 执行故障转移时， 最多有1个从服务器同时对新的主服务器进行同步sentinel parallel-syncs mymaster 1# sentinel auth-pass &lt;master-name&gt; &lt;password&gt;sentinel auth-pass mymaster 123456 在另外的两个Redis目录里面分别创建sentinel.conf，注意修改其中port端口分26380和26381。配置文件只需要配置master的信息就好啦，不用配置slave的信息，因为slave能够被自动检测到(master节点中有关于slave的消息)。 为了更清楚每一行配置的含义，对每个选项的含义进行简单介绍： 1sentinel monitor [master-group-name] [ip] [port] [quorum] master-group-name：master名称（可以自定义） ip port : IP地址和端口号 quorum：票数，Sentinel需要协商同意master是否可到达的数量。 第一行配置指示 Sentinel 去监视一个名为 mymaster 的主服务器， 这个主服务器的 IP 地址为 127.0.0.1 ， 端口号为 6379 ， 而将这个主服务器判断为失效至少需要 2 个 Sentinel 同意 （只要同意 Sentinel 的数量不达标，自动故障迁移就不会执行）。票数在本文中：redis集群中有3个sentinel实例，其中master挂掉啦，这里设置票数为2，表示有2个sentinel认为master挂掉啦，才能被认为是正真的挂掉啦。 1sentinel &lt;选项的名字&gt; &lt;主服务器的名字&gt; &lt;选项的值&gt; down-after-milliseconds 选项指定了 Sentinel 认为服务器已经断线所需的毫秒数。 如果服务器在给定的毫秒数之内， 没有返回 Sentinel 发送的 PING 命令的回复， 或者返回一个错误， 那么 Sentinel 将这个服务器标记为主观下线（subjectively down，简称 SDOWN ）。不过只有一个 Sentinel 将服务器标记为主观下线并不一定会引起服务器的自动故障迁移： 只有在足够数量的 Sentinel 都将一个服务器标记为主观下线之后， 服务器才会被标记为客观下线（objectively down， 简称 ODOWN ）， 这时自动故障迁移才会执行。将服务器标记为客观下线所需的 Sentinel 数量由对主服务器的配置决定。 parallel-syncs 选项指定了在执行故障转移时， 最多可以有多少个从服务器同时对新的主服务器进行同步， 这个数字越小， 完成故障转移所需的时间就越长。 新增Redis启动脚本：startRedisServer.bat123@echo offredis-server.exe redis.conf@pause 新增Redis-Sentinel启动脚本：startRedisSentinel.bat123@echo offredis-server.exe sentinel.conf --sentinel @pause 另外两个进行同样的操作，创建新增Redis启动脚本和Redis-Sentinel启动脚本。 开始启动环境 点击startRedis.bat，启动Redis集群 点击startRedisSentinel.bat，启动哨兵实例 搭建中出现的问题 sentinel配置文件的顺序问题 12345678[3968] 01 Mar 11:06:38.314 #*** FATAL CONFIG FILE ERROR ***[3968] 01 Mar 11:06:38.314 # Reading the configuration file, at line 10[3968] 01 Mar 11:06:38.314 # &gt;&gt;&gt; 'sentinel down-after-milliseconds mymaster 3000'[3968] 01 Mar 11:06:38.314 # No such master with specified name. 解决方法：哨兵监听的主服务器的配置 1sentinel monitor mymaster 127.0.0.1 6379 2 要尽量放到sentinel配置的前面 配置文件的空格问题 1Invalid argument during startup: unknown conf file parameter : 解决方法：删除或者更改配置注意不要留有空格 未完","categories":[],"tags":[{"name":"Windows","slug":"Windows","permalink":"https://ydstudios.gitee.io/tags/Windows/"},{"name":"Redis","slug":"Redis","permalink":"https://ydstudios.gitee.io/tags/Redis/"},{"name":"Redis-Sentinel","slug":"Redis-Sentinel","permalink":"https://ydstudios.gitee.io/tags/Redis-Sentinel/"}]},{"title":"Linux下搭建Redis主从复制","slug":"Linux下搭建Redis主从复制","date":"2019-02-27T23:30:02.000Z","updated":"2019-12-09T15:03:26.000Z","comments":true,"path":"post/c1608171.html","link":"","permalink":"https://ydstudios.gitee.io/post/c1608171.html","excerpt":"最近忙着准备面试，就把之前用到的Redis的主从配置和哨兵模式配置都重新复习一遍。这篇文章先复习配置Redis的主从，下一篇复习Redis的哨兵模式。Redis的主从复制配置起来比较简单，主从配置有以下特点：","text":"最近忙着准备面试，就把之前用到的Redis的主从配置和哨兵模式配置都重新复习一遍。这篇文章先复习配置Redis的主从，下一篇复习Redis的哨兵模式。Redis的主从复制配置起来比较简单，主从配置有以下特点： 一个Master可以有多个slave主机，支持链式复制 Master以非阻塞方式同步数据至slave主机 在一台节点上配置文件中定义自己是谁的从节点，并且启用主节点密码认证即可。下面使用3台主机配置一主两从的结构，redis使用一主多从的结构时还可以实现像mysql MHA那样的复制集群，当master节点宕机后，可以在两个slave节点中根据优先级选举新的master。 安装Redis本次采用源码编译的方式安装Redis，执行相关的命令如下：123456789101112131415161718192021#下载redis安装包cd /usr/local/srcwget http://download.redis.io/releases/redis-5.0.3.tar.gz#解压安装包tar -zxvf redis-5.0.3.tar.gz#拷贝到/usr/local/redis目录mkdir -p /usr/local/redis#编译安装make &amp; make install#redis安装完成后，redis-server、redis-cli等程序默认安装在/usr/local/redis/src目录中，我们需要将它安装到系统搜索路径，并安装为系统服务，方便在任何终端访问，并且开机自动启动。cd /usr/local/src/redis/utils./install_server.sh#重复上述安装操作，安装redis到6380和6381端口./install_server.sh 环境准备好之后，具体情况如下： 三个Redis实例分别安装以下IP地址和端口，角色分别如下 192.168.1.103 6379 master192.168.1.103 6380 slave192.168.1.103 6381 slave 配置Master节点1234bind 0.0.0.0 #绑定地址 (绑定在127.0.0.1，只有本机客户端可以访问，其他服务器无法访问，绑定在0.0.0.0上，如果没有端口限制，那么其他服务器则可以连接该服务器的该端口)requirepass 123456 #启用密码认证#默认master节点修改这两项就可以了，也可以进行其他设置 配置Slave节点 1234567bind 0.0.0.0 #定义master信息slaveof 192.168.1.103 6379 #认证masterauth 123456 #从节点上配置这三项页就可以，其他参数可以根据情况选择调整即可。 修改后记得重启服务，可以登录到master节点查看信息。 首先使用client list命令查看 12345678910[root@192 bin]# redis-cli -p 6379 -a 123456Warning: Using a password with '-a' or '-u' option on the command line interface may not be safe.127.0.0.1:6379&gt; client listid=3 addr=127.0.0.1:44273 fd=7 name= age=2456 idle=1 flags=S db=0 sub=0 psub=0 multi=-1 qbuf=0 qbuf-free=0 obl=0 oll=0 omem=0 events=r cmd=replconfid=4 addr=127.0.0.1:45342 fd=8 name= age=2453 idle=1 flags=S db=0 sub=0 psub=0 multi=-1 qbuf=0 qbuf-free=0 obl=0 oll=0 omem=0 events=r cmd=replconfid=6 addr=127.0.0.1:49306 fd=9 name= age=6 idle=0 flags=N db=0 sub=0 psub=0 multi=-1 qbuf=26 qbuf-free=32742 obl=0 oll=0 omem=0 events=r cmd=client127.0.0.1:6379&gt; cmd=client 表示是mastercmd=replconf 表示是slave 首先使用info replication 命令查看123456789101112131415127.0.0.1:6379&gt; info replication# Replicationrole:master #自己的角色connected_slaves:2 #从节点数量slave0:ip=127.0.0.1,port=6380,state=online,offset=3738,lag=0 #从节点信息slave1:ip=127.0.0.1,port=6381,state=online,offset=3738,lag=1 #从节点信息master_replid:061bcdd36e3b8d9a0c38d29dc762e908445baad6master_replid2:0000000000000000000000000000000000000000master_repl_offset:3738second_repl_offset:-1repl_backlog_active:1repl_backlog_size:1048576repl_backlog_first_byte_offset:1repl_backlog_histlen:3738127.0.0.1:6379&gt; 可以看到主从差不多已经配置好了，接下来可以进行简单的验证复制，在master节点设置一个key，看两个slave节点复制情况。 Master节点创建一个key: 12345[root@192 bin]# redis-cli -p 6379 -a 123456Warning: Using a password with '-a' or '-u' option on the command line interface may not be safe.127.0.0.1:6379&gt; set jobNo 123456OK127.0.0.1:6379&gt; 从slave 6380 : 12345[root@192 bin]# redis-cli -p 6380 -a 123456Warning: Using a password with '-a' or '-u' option on the command line interface may not be safe.127.0.0.1:6380&gt; get jobNo\"123456\"127.0.0.1:6380&gt; 从slave 6381： 12345[root@192 bin]# redis-cli -p 6381 -a 123456Warning: Using a password with '-a' or '-u' option on the command line interface may not be safe.127.0.0.1:6381&gt; get jobNo\"123456\"127.0.0.1:6381&gt; 可以看出在master上设置的key已经同步到了两个slave上。 slave节点定义也可以通过指令设置，设置后立即生效，并且会被保存至配置文件中，指令配置方式如下： #配置slave节点 12redis-cli&gt; SLAVEOF &lt;MASTER_IP&gt; &lt;MASTER_PORT&gt;redis-cli&gt; CONFIG SET masterauth &lt;PASSWORD&gt; redis主从复制相关配置下面是redis主从复制场景的一些可调参数，需要根据实际环境调整 slave-serve-stale-data yes ： 是否可以把不新鲜的数据服务与客户端 slave-read-only yes ： 从节点只读，启用slaveof定义后才生效 repl-diskless-sync no ：是否同时向多个从节点同时发数据 repl-diskless-sync-delay 5 ：发送的延迟时间 repl-ping-slave-period 10 探测从节点状态 repl-timeout 60 探测节点超时时间 repl-disable-tcp-nodelay no ： 启用nodelay repl-backlog-size 1mb slave-priority 100 ： 从节点优先级,复制集群中，主节点故障时，sentinel应用场景中的主节点选举时使用的优先级；数字越小优先级越高，但0表示不参与选举； min-slaves-to-write 3：主节点仅允许其能够通信的从节点数量大于等于此处的值时接受写操作； min-slaves-max-lag 10：从节点延迟时长超出此处指定的时长时，主节点会拒绝写入操作； 查看原文","categories":[],"tags":[{"name":"linux","slug":"linux","permalink":"https://ydstudios.gitee.io/tags/linux/"},{"name":"redis","slug":"redis","permalink":"https://ydstudios.gitee.io/tags/redis/"},{"name":"主从复制","slug":"主从复制","permalink":"https://ydstudios.gitee.io/tags/%E4%B8%BB%E4%BB%8E%E5%A4%8D%E5%88%B6/"}]},{"title":"Nginx代理时header头中带_的信息丢失的问题","slug":"Nginx代理时header头中带-的信息丢失的问题","date":"2019-02-01T23:28:37.000Z","updated":"2019-12-09T15:03:26.000Z","comments":true,"path":"post/a6b6fb0a.html","link":"","permalink":"https://ydstudios.gitee.io/post/a6b6fb0a.html","excerpt":"开发网关项目时，在请求时往请求头header中放入了签名sign_key信息，在接收请求时再从header中拿出，在本地调试时是可以的，但上线之后通过Nginx代理之后发现拿不到。","text":"开发网关项目时，在请求时往请求头header中放入了签名sign_key信息，在接收请求时再从header中拿出，在本地调试时是可以的，但上线之后通过Nginx代理之后发现拿不到。 123456789101112location / &#123; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header Host $http_host; proxy_set_header X-Nginx-Proxy true; add_header Proxy-Node $upstream_addr; add_header Proxy-Status $upstream_status; proxy_http_version 1.1; proxy_pass http://tianusa; &#125; 后来找到原因是因为Nginx对header有所限制，下划线（_）不支持 1.不用下划线把下划线_改成其他的，如sign_key改成sign-key 2.从根本解除Nginx的限制Nginx默认request的header的那么中包含’_’时，会自动忽略掉。解决方法是：在nginx里的nginx.conf配置文件中的http部分中添加如下配置： 1underscores_in_headers on; （默认 underscores_in_headers 为off）","categories":[],"tags":[{"name":"linux","slug":"linux","permalink":"https://ydstudios.gitee.io/tags/linux/"},{"name":"nginx","slug":"nginx","permalink":"https://ydstudios.gitee.io/tags/nginx/"}]},{"title":"Mybatis Generator最完整配置详解","slug":"Mybatis-Generator最完整配置详解","date":"2019-01-29T23:27:40.000Z","updated":"2019-12-09T15:03:26.000Z","comments":true,"path":"post/a9e1e2fd.html","link":"","permalink":"https://ydstudios.gitee.io/post/a9e1e2fd.html","excerpt":"这是一份Mybatis Generator最完整配置详解，大家可以好好看看。","text":"这是一份Mybatis Generator最完整配置详解，大家可以好好看看。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;!DOCTYPE generatorConfiguration PUBLIC \"-//mybatis.org//DTD MyBatis Generator Configuration 1.0//EN\"\"http://mybatis.org/dtd/mybatis-generator-config_1_0.dtd\"&gt;&lt;!-- 配置生成器 --&gt;&lt;generatorConfiguration&gt;&lt;!-- 可以用于加载配置项或者配置文件，在整个配置文件中就可以使用$&#123;propertyKey&#125;的方式来引用配置项 resource：配置资源加载地址，使用resource，MBG从classpath开始找，比如com/myproject/generatorConfig.properties url：配置资源加载地质，使用URL的方式，比如file:///C:/myfolder/generatorConfig.properties. 注意，两个属性只能选址一个; 另外，如果使用了mybatis-generator-maven-plugin，那么在pom.xml中定义的properties都可以直接在generatorConfig.xml中使用&lt;properties resource=\"\" url=\"\" /&gt; --&gt; &lt;!-- 在MBG工作的时候，需要额外加载的依赖包 location属性指明加载jar/zip包的全路径&lt;classPathEntry location=\"/Program Files/IBM/SQLLIB/java/db2java.zip\" /&gt; --&gt; &lt;!-- context:生成一组对象的环境 id:必选，上下文id，用于在生成错误时提示 defaultModelType:指定生成对象的样式 1，conditional：类似hierarchical； 2，flat：所有内容（主键，blob）等全部生成在一个对象中； 3，hierarchical：主键生成一个XXKey对象(key class)，Blob等单独生成一个对象，其他简单属性在一个对象中(record class) targetRuntime: 1，MyBatis3：默认的值，生成基于MyBatis3.x以上版本的内容，包括XXXBySample； 2，MyBatis3Simple：类似MyBatis3，只是不生成XXXBySample； introspectedColumnImpl：类全限定名，用于扩展MBG--&gt;&lt;context id=\"mysql\" defaultModelType=\"hierarchical\" targetRuntime=\"MyBatis3Simple\" &gt; &lt;!-- 自动识别数据库关键字，默认false，如果设置为true，根据SqlReservedWords中定义的关键字列表； 一般保留默认值，遇到数据库关键字（Java关键字），使用columnOverride覆盖 --&gt; &lt;property name=\"autoDelimitKeywords\" value=\"false\"/&gt; &lt;!-- 生成的Java文件的编码 --&gt; &lt;property name=\"javaFileEncoding\" value=\"UTF-8\"/&gt; &lt;!-- 格式化java代码 --&gt; &lt;property name=\"javaFormatter\" value=\"org.mybatis.generator.api.dom.DefaultJavaFormatter\"/&gt; &lt;!-- 格式化XML代码 --&gt; &lt;property name=\"xmlFormatter\" value=\"org.mybatis.generator.api.dom.DefaultXmlFormatter\"/&gt; &lt;!-- beginningDelimiter和endingDelimiter：指明数据库的用于标记数据库对象名的符号，比如ORACLE就是双引号，MYSQL默认是`反引号； --&gt; &lt;property name=\"beginningDelimiter\" value=\"`\"/&gt; &lt;property name=\"endingDelimiter\" value=\"`\"/&gt; &lt;!-- 必须要有的，使用这个配置链接数据库 @TODO:是否可以扩展 --&gt; &lt;jdbcConnection driverClass=\"com.mysql.jdbc.Driver\" connectionURL=\"jdbc:mysql:///pss\" userId=\"root\" password=\"admin\"&gt; &lt;!-- 这里面可以设置property属性，每一个property属性都设置到配置的Driver上 --&gt; &lt;/jdbcConnection&gt; &lt;!-- java类型处理器 用于处理DB中的类型到Java中的类型，默认使用JavaTypeResolverDefaultImpl； 注意一点，默认会先尝试使用Integer，Long，Short等来对应DECIMAL和 NUMERIC数据类型； --&gt; &lt;javaTypeResolver type=\"org.mybatis.generator.internal.types.JavaTypeResolverDefaultImpl\"&gt; &lt;!-- true：使用BigDecimal对应DECIMAL和 NUMERIC数据类型 false：默认, scale&gt;0;length&gt;18：使用BigDecimal; scale=0;length[10,18]：使用Long； scale=0;length[5,9]：使用Integer； scale=0;length&lt;5：使用Short； --&gt; &lt;property name=\"forceBigDecimals\" value=\"false\"/&gt; &lt;/javaTypeResolver&gt; &lt;!-- java模型创建器，是必须要的元素 负责：1，key类（见context的defaultModelType）；2，java类；3，查询类 targetPackage：生成的类要放的包，真实的包受enableSubPackages属性控制； targetProject：目标项目，指定一个存在的目录下，生成的内容会放到指定目录中，如果目录不存在，MBG不会自动建目录 --&gt; &lt;javaModelGenerator targetPackage=\"com._520it.mybatis.domain\" targetProject=\"src/main/java\"&gt; &lt;!-- for MyBatis3/MyBatis3Simple 自动为每一个生成的类创建一个构造方法，构造方法包含了所有的field；而不是使用setter； --&gt; &lt;property name=\"constructorBased\" value=\"false\"/&gt; &lt;!-- 在targetPackage的基础上，根据数据库的schema再生成一层package，最终生成的类放在这个package下，默认为false --&gt; &lt;property name=\"enableSubPackages\" value=\"true\"/&gt; &lt;!-- for MyBatis3 / MyBatis3Simple 是否创建一个不可变的类，如果为true， 那么MBG会创建一个没有setter方法的类，取而代之的是类似constructorBased的类 --&gt; &lt;property name=\"immutable\" value=\"false\"/&gt; &lt;!-- 设置一个根对象， 如果设置了这个根对象，那么生成的keyClass或者recordClass会继承这个类；在Table的rootClass属性中可以覆盖该选项 注意：如果在key class或者record class中有root class相同的属性，MBG就不会重新生成这些属性了，包括： 1，属性名相同，类型相同，有相同的getter/setter方法； --&gt; &lt;property name=\"rootClass\" value=\"com._520it.mybatis.domain.BaseDomain\"/&gt; &lt;!-- 设置是否在getter方法中，对String类型字段调用trim()方法 --&gt; &lt;property name=\"trimStrings\" value=\"true\"/&gt; &lt;/javaModelGenerator&gt; &lt;!-- 生成SQL map的XML文件生成器， 注意，在Mybatis3之后，我们可以使用mapper.xml文件+Mapper接口（或者不用mapper接口）， 或者只使用Mapper接口+Annotation，所以，如果 javaClientGenerator配置中配置了需要生成XML的话，这个元素就必须配置 targetPackage/targetProject:同javaModelGenerator --&gt; &lt;sqlMapGenerator targetPackage=\"com._520it.mybatis.mapper\" targetProject=\"src/main/resources\"&gt; &lt;!-- 在targetPackage的基础上，根据数据库的schema再生成一层package，最终生成的类放在这个package下，默认为false --&gt; &lt;property name=\"enableSubPackages\" value=\"true\"/&gt; &lt;/sqlMapGenerator&gt; &lt;!-- 对于mybatis来说，即生成Mapper接口，注意，如果没有配置该元素，那么默认不会生成Mapper接口 targetPackage/targetProject:同javaModelGenerator type：选择怎么生成mapper接口（在MyBatis3/MyBatis3Simple下）： 1，ANNOTATEDMAPPER：会生成使用Mapper接口+Annotation的方式创建（SQL生成在annotation中），不会生成对应的XML； 2，MIXEDMAPPER：使用混合配置，会生成Mapper接口，并适当添加合适的Annotation，但是XML会生成在XML中； 3，XMLMAPPER：会生成Mapper接口，接口完全依赖XML； 注意，如果context是MyBatis3Simple：只支持ANNOTATEDMAPPER和XMLMAPPER --&gt; &lt;javaClientGenerator targetPackage=\"com._520it.mybatis.mapper\" type=\"ANNOTATEDMAPPER\" targetProject=\"src/main/java\"&gt; &lt;!-- 在targetPackage的基础上，根据数据库的schema再生成一层package，最终生成的类放在这个package下，默认为false --&gt; &lt;property name=\"enableSubPackages\" value=\"true\"/&gt; &lt;!-- 可以为所有生成的接口添加一个父接口，但是MBG只负责生成，不负责检查 &lt;property name=\"rootInterface\" value=\"\"/&gt; --&gt; &lt;/javaClientGenerator&gt; &lt;!-- 选择一个table来生成相关文件，可以有一个或多个table，必须要有table元素 选择的table会生成一下文件： 1，SQL map文件 2，生成一个主键类； 3，除了BLOB和主键的其他字段的类； 4，包含BLOB的类； 5，一个用户生成动态查询的条件类（selectByExample, deleteByExample），可选； 6，Mapper接口（可选） tableName（必要）：要生成对象的表名； 注意：大小写敏感问题。正常情况下，MBG会自动的去识别数据库标识符的大小写敏感度，在一般情况下，MBG会 根据设置的schema，catalog或tablename去查询数据表，按照下面的流程： 1，如果schema，catalog或tablename中有空格，那么设置的是什么格式，就精确的使用指定的大小写格式去查询； 2，否则，如果数据库的标识符使用大写的，那么MBG自动把表名变成大写再查找； 3，否则，如果数据库的标识符使用小写的，那么MBG自动把表名变成小写再查找； 4，否则，使用指定的大小写格式查询； 另外的，如果在创建表的时候，使用的\"\"把数据库对象规定大小写，就算数据库标识符是使用的大写，在这种情况下也会使用给定的大小写来创建表名； 这个时候，请设置delimitIdentifiers=\"true\"即可保留大小写格式； 可选： 1，schema：数据库的schema； 2，catalog：数据库的catalog； 3，alias：为数据表设置的别名，如果设置了alias，那么生成的所有的SELECT SQL语句中，列名会变成：alias_actualColumnName 4，domainObjectName：生成的domain类的名字，如果不设置，直接使用表名作为domain类的名字；可以设置为somepck.domainName，那么会自动把domainName类再放到somepck包里面； 5，enableInsert（默认true）：指定是否生成insert语句； 6，enableSelectByPrimaryKey（默认true）：指定是否生成按照主键查询对象的语句（就是getById或get）； 7，enableSelectByExample（默认true）：MyBatis3Simple为false，指定是否生成动态查询语句； 8，enableUpdateByPrimaryKey（默认true）：指定是否生成按照主键修改对象的语句（即update)； 9，enableDeleteByPrimaryKey（默认true）：指定是否生成按照主键删除对象的语句（即delete）； 10，enableDeleteByExample（默认true）：MyBatis3Simple为false，指定是否生成动态删除语句； 11，enableCountByExample（默认true）：MyBatis3Simple为false，指定是否生成动态查询总条数语句（用于分页的总条数查询）； 12，enableUpdateByExample（默认true）：MyBatis3Simple为false，指定是否生成动态修改语句（只修改对象中不为空的属性）； 13，modelType：参考context元素的defaultModelType，相当于覆盖； 14，delimitIdentifiers：参考tableName的解释，注意，默认的delimitIdentifiers是双引号，如果类似MYSQL这样的数据库，使用的是`（反引号，那么还需要设置context的beginningDelimiter和endingDelimiter属性） 15，delimitAllColumns：设置是否所有生成的SQL中的列名都使用标识符引起来。默认为false，delimitIdentifiers参考context的属性 注意，table里面很多参数都是对javaModelGenerator，context等元素的默认属性的一个复写； --&gt; &lt;table tableName=\"userinfo\" &gt; &lt;!-- 参考 javaModelGenerator 的 constructorBased属性--&gt; &lt;property name=\"constructorBased\" value=\"false\"/&gt; &lt;!-- 默认为false，如果设置为true，在生成的SQL中，table名字不会加上catalog或schema； --&gt; &lt;property name=\"ignoreQualifiersAtRuntime\" value=\"false\"/&gt; &lt;!-- 参考 javaModelGenerator 的 immutable 属性 --&gt; &lt;property name=\"immutable\" value=\"false\"/&gt; &lt;!-- 指定是否只生成domain类，如果设置为true，只生成domain类，如果还配置了sqlMapGenerator，那么在mapper XML文件中，只生成resultMap元素 --&gt; &lt;property name=\"modelOnly\" value=\"false\"/&gt; &lt;!-- 参考 javaModelGenerator 的 rootClass 属性 &lt;property name=\"rootClass\" value=\"\"/&gt; --&gt; &lt;!-- 参考javaClientGenerator 的 rootInterface 属性 &lt;property name=\"rootInterface\" value=\"\"/&gt; --&gt; &lt;!-- 如果设置了runtimeCatalog，那么在生成的SQL中，使用该指定的catalog，而不是table元素上的catalog &lt;property name=\"runtimeCatalog\" value=\"\"/&gt; --&gt; &lt;!-- 如果设置了runtimeSchema，那么在生成的SQL中，使用该指定的schema，而不是table元素上的schema &lt;property name=\"runtimeSchema\" value=\"\"/&gt; --&gt; &lt;!-- 如果设置了runtimeTableName，那么在生成的SQL中，使用该指定的tablename，而不是table元素上的tablename &lt;property name=\"runtimeTableName\" value=\"\"/&gt; --&gt; &lt;!-- 注意，该属性只针对MyBatis3Simple有用； 如果选择的runtime是MyBatis3Simple，那么会生成一个SelectAll方法，如果指定了selectAllOrderByClause，那么会在该SQL中添加指定的这个order条件； --&gt; &lt;property name=\"selectAllOrderByClause\" value=\"age desc,username asc\"/&gt; &lt;!-- 如果设置为true，生成的model类会直接使用column本身的名字，而不会再使用驼峰命名方法，比如BORN_DATE，生成的属性名字就是BORN_DATE,而不会是bornDate --&gt; &lt;property name=\"useActualColumnNames\" value=\"false\"/&gt; &lt;!-- generatedKey用于生成生成主键的方法， 如果设置了该元素，MBG会在生成的&lt;insert&gt;元素中生成一条正确的&lt;selectKey&gt;元素，该元素可选 column:主键的列名； sqlStatement：要生成的selectKey语句，有以下可选项： Cloudscape:相当于selectKey的SQL为： VALUES IDENTITY_VAL_LOCAL() DB2 :相当于selectKey的SQL为： VALUES IDENTITY_VAL_LOCAL() DB2_MF :相当于selectKey的SQL为：SELECT IDENTITY_VAL_LOCAL() FROM SYSIBM.SYSDUMMY1 Derby :相当于selectKey的SQL为：VALUES IDENTITY_VAL_LOCAL() HSQLDB :相当于selectKey的SQL为：CALL IDENTITY() Informix :相当于selectKey的SQL为：select dbinfo('sqlca.sqlerrd1') from systables where tabid=1 MySql :相当于selectKey的SQL为：SELECT LAST_INSERT_ID() SqlServer :相当于selectKey的SQL为：SELECT SCOPE_IDENTITY() SYBASE :相当于selectKey的SQL为：SELECT @@IDENTITY JDBC :相当于在生成的insert元素上添加useGeneratedKeys=\"true\"和keyProperty属性 &lt;generatedKey column=\"\" sqlStatement=\"\"/&gt; --&gt; &lt;!-- 该元素会在根据表中列名计算对象属性名之前先重命名列名，非常适合用于表中的列都有公用的前缀字符串的时候， 比如列名为：CUST_ID,CUST_NAME,CUST_EMAIL,CUST_ADDRESS等； 那么就可以设置searchString为\"^CUST_\"，并使用空白替换，那么生成的Customer对象中的属性名称就不是 custId,custName等，而是先被替换为ID,NAME,EMAIL,然后变成属性：id，name，email； 注意，MBG是使用java.util.regex.Matcher.replaceAll来替换searchString和replaceString的， 如果使用了columnOverride元素，该属性无效； &lt;columnRenamingRule searchString=\"\" replaceString=\"\"/&gt; --&gt; &lt;!-- 用来修改表中某个列的属性，MBG会使用修改后的列来生成domain的属性； column:要重新设置的列名； 注意，一个table元素中可以有多个columnOverride元素哈~ --&gt; &lt;columnOverride column=\"username\"&gt; &lt;!-- 使用property属性来指定列要生成的属性名称 --&gt; &lt;property name=\"property\" value=\"userName\"/&gt; &lt;!-- javaType用于指定生成的domain的属性类型，使用类型的全限定名 &lt;property name=\"javaType\" value=\"\"/&gt; --&gt; &lt;!-- jdbcType用于指定该列的JDBC类型 &lt;property name=\"jdbcType\" value=\"\"/&gt; --&gt; &lt;!-- typeHandler 用于指定该列使用到的TypeHandler，如果要指定，配置类型处理器的全限定名 注意，mybatis中，不会生成到mybatis-config.xml中的typeHandler 只会生成类似：where id = #&#123;id,jdbcType=BIGINT,typeHandler=com._520it.mybatis.MyTypeHandler&#125;的参数描述 &lt;property name=\"jdbcType\" value=\"\"/&gt; --&gt; &lt;!-- 参考table元素的delimitAllColumns配置，默认为false &lt;property name=\"delimitedColumnName\" value=\"\"/&gt; --&gt; &lt;/columnOverride&gt; &lt;!-- ignoreColumn设置一个MGB忽略的列，如果设置了改列，那么在生成的domain中，生成的SQL中，都不会有该列出现 column:指定要忽略的列的名字； delimitedColumnName：参考table元素的delimitAllColumns配置，默认为false 注意，一个table元素中可以有多个ignoreColumn元素 &lt;ignoreColumn column=\"deptId\" delimitedColumnName=\"\"/&gt; --&gt; &lt;/table&gt; &lt;/context&gt;","categories":[],"tags":[{"name":"java","slug":"java","permalink":"https://ydstudios.gitee.io/tags/java/"},{"name":"mybatis","slug":"mybatis","permalink":"https://ydstudios.gitee.io/tags/mybatis/"}]},{"title":"Java实现二分查找算法","slug":"Java实现二分查找算法","date":"2019-01-25T23:26:42.000Z","updated":"2019-12-09T15:03:26.000Z","comments":true,"path":"post/e486e255.html","link":"","permalink":"https://ydstudios.gitee.io/post/e486e255.html","excerpt":"二分法查找，也称为折半法，是一种在有序数组中查找特定元素的搜索算法。所以在采用二分法查找时，数据需是有序不重复的，如果是无序的也可通过选择排序、冒泡排序等数组排序方法进行排序之后，就可以使用二分法查找。 基本思想：假设数据是按升序排序的，对于给定值 x，从序列的中间位置开始比较，如果当前位置值等于 x，则查找成功；若 x 小于当前位置值，则在数列的前半段中查找；若 x 大于当前位置值则在数列的后半段中继续查找，直到找到为止，但是如果当前段的索引最大值小于当前段索引最小值，说明查找的值不存在，返回-1，不继续查找。","text":"二分法查找，也称为折半法，是一种在有序数组中查找特定元素的搜索算法。所以在采用二分法查找时，数据需是有序不重复的，如果是无序的也可通过选择排序、冒泡排序等数组排序方法进行排序之后，就可以使用二分法查找。 基本思想：假设数据是按升序排序的，对于给定值 x，从序列的中间位置开始比较，如果当前位置值等于 x，则查找成功；若 x 小于当前位置值，则在数列的前半段中查找；若 x 大于当前位置值则在数列的后半段中继续查找，直到找到为止，但是如果当前段的索引最大值小于当前段索引最小值，说明查找的值不存在，返回-1，不继续查找。 下面贴出代码实现： 123456789101112131415161718192021222324252627282930313233343536373839404142import java.util.ArrayList;import java.util.LinkedList;import java.util.List;/** * Created by Sam on 18/12/9. */public class Test &#123; public static void main(String[] args) &#123; int[] array = &#123;1,4,7,9,12,56,78,89,120,179,180,200,290&#125;; System.out.println(\"index=\"+binarySearch(array,290)); &#125; public static int binarySearch(int[] array,int searchNumber)&#123; int minIndex = 0; int maxIndex = array.length - 1; int searchIndex = (minIndex + maxIndex) &gt;&gt; 1 ; int count = 0; while (array[searchIndex] != searchNumber)&#123; System.out.printf(\"第次%d次运算\\n\", ++count); if (array[searchIndex] &gt; searchNumber)&#123; maxIndex = searchIndex - 1 ; &#125;else &#123; minIndex = searchIndex + 1 ; &#125; if (minIndex&gt;maxIndex)&#123; return -1; &#125; searchIndex = (minIndex + maxIndex) &gt;&gt; 1 ; &#125; return searchIndex; &#125;&#125;","categories":[],"tags":[{"name":"java","slug":"java","permalink":"https://ydstudios.gitee.io/tags/java/"},{"name":"二分法","slug":"二分法","permalink":"https://ydstudios.gitee.io/tags/%E4%BA%8C%E5%88%86%E6%B3%95/"}]},{"title":"PowerDesigner生成SQL时注释为name和comment合并后的内容","slug":"PowerDesigner生成SQL时注释为name和comment合并后的内容","date":"2019-01-18T23:25:42.000Z","updated":"2019-12-09T15:03:26.000Z","comments":true,"path":"post/26129f65.html","link":"","permalink":"https://ydstudios.gitee.io/post/26129f65.html","excerpt":"PowerDesigner生成SQL时的注释默认是没有name字段的，网上讲的设置方法大部分都是只能用name或者comment，无法将两者合并使用。但是有时候我们需要name和comment字段合并起来加到注释中，则可以通过下面的方法来实现。","text":"PowerDesigner生成SQL时的注释默认是没有name字段的，网上讲的设置方法大部分都是只能用name或者comment，无法将两者合并使用。但是有时候我们需要name和comment字段合并起来加到注释中，则可以通过下面的方法来实现。 第一步：修改SQL的生成脚本 选择菜单：database -&gt; edit current DBMS… 选择general选项卡：script -&gt; objects -&gt; column -&gt; columncommnet 修改value值为： 重点就是下面这个脚本了，本人对脚本语法不熟悉，经过各种尝试，最终脚本如下123456alter table %TABLE% modify column [%QUALIFIER%] %TABLE%.%COLUMN% %DATATYPE%[%Unsigned%? unsigned][%ZeroFill%? zerofill][.Z:[ %NOTNULL%][%R%?[%PRIMARY%]][%IDENTITY%? auto_increment:[ default %DEFAULT%]]] comment.if (%COMMENT%==%COLNNAME%) '%COLNNAME%'.else '%COLNNAME% %COMMENT%'.endif(\\n) 第二步：修改生成规则当第一步的脚本修改后就可以显示name为注释了，但是comment为空的则不能生成注释，需要修改生成规则。 选择菜单：database -&gt; generate database… 选择format选项卡，选中”generate name is empty comment”选项 点“应用”按钮保存设置 第一次实现这种效果，不是通过这种方式，换了新的机器之后，之前那种方法找不到了，就通过查找资料用这种方式实现了","categories":[],"tags":[{"name":"PowerDesigner","slug":"PowerDesigner","permalink":"https://ydstudios.gitee.io/tags/PowerDesigner/"}]},{"title":"Snowflake算法生成分布式系统唯一ID","slug":"Snowflake算法生成分布式系统唯一ID","date":"2019-01-06T23:23:41.000Z","updated":"2019-12-09T15:03:26.000Z","comments":true,"path":"post/4f74853e.html","link":"","permalink":"https://ydstudios.gitee.io/post/4f74853e.html","excerpt":"在复杂的系统中唯一ID是我们在设计的时候常常会遇见的问题，生成ID的方法有很多，适应不同的场景、需求以及性能要求。所以有些比较复杂的系统会有多个ID生成的策略，下面就介绍一些常见的ID生成策略。","text":"在复杂的系统中唯一ID是我们在设计的时候常常会遇见的问题，生成ID的方法有很多，适应不同的场景、需求以及性能要求。所以有些比较复杂的系统会有多个ID生成的策略，下面就介绍一些常见的ID生成策略。 1. 数据库自增长序列或字段最常见的方式。利用数据库，全数据库唯一。 优点： 简单，代码方便，性能可以接受。 数字ID天然排序，对分页或者需要排序的结果很有帮助。 缺点： 不同数据库语法和实现不同，数据库迁移的时候或多数据库版本支持的时候需要处理。 在单个数据库或读写分离或一主多从的情况下，只有一个主库可以生成。有单点故障的风险。 在性能达不到要求的情况下，比较难于扩展。 如果遇见多个系统需要合并或者涉及到数据迁移会相当痛苦。 优化方案： 针对主库单点，如果有多个Master库，则每个Master库设置的起始数字不一样，步长一样，可以是Master的个数。比如：Master1 生成的是 1，4，7，10，Master2生成的是2,5,8,11 Master3生成的是 3,6,9,12。这样就可以有效生成集群中的唯一ID，也可以大大降低ID生成数据库操作的负载。 2. UUID UUID(Universally Unique Identifier)的标准型式包含32个16进制数字，以连字号分为五段，形式为8-4-4-4-12的36个字符，示例：550e8400-e29b-41d4-a716-446655440000，到目前为止业界一共有5种方式生成UUID。 优点： 简单，代码方便。 生成ID性能非常好，基本不会有性能问题，本地生成，没有网络消耗。 全球唯一，在遇见数据迁移，系统数据合并，或者数据库变更等情况下，可以从容应对。 缺点： 不易于存储，UUID太长，16字节128位，通常以36长度的字符串表示，很多场景不适用。MySQL官方有明确的建议主键要尽量越短越好 信息不安全：基于MAC地址生成UUID的算法可能会造成MAC地址泄露，这个漏洞曾被用于寻找梅丽莎病毒的制作者位置。 生成ID无序对MySQL索引不利：如果作为数据库主键，在InnoDB引擎下，UUID的无序性可能会引起数据位置频繁变动，严重影响性能。 3. snowflake方案大致来说是一种以划分命名空间（UUID也算，由于比较常见，所以单独分析）来生成ID的一种算法，这种方案把64-bit分别划分成多段，分开来标示机器、时间等，比如在snowflake中的64-bit分别表示如下图所示： 10 - 0000000000 0000000000 0000000000 0000000000 0 - 00000 - 00000 - 000000000000 1bit 表示符号位，表示是正数还是负数，设为正数固定为0 41bit 的时间戳 可以表示( 1L&lt;&lt;41 ) / ( 1000L * 3600 * 24 *365 )=69年的时间。 10bit机器可以分别表示1024台机器。如果我们对IDC划分有需求，还可以将10bit分5bit给IDC，分5bit给工作机器。这样就可以表示32个IDC，每个IDC下可以有32台机器，可以根据自身需求定义。 12bit可以表示的最大正整数是2^12-1=4095，即可以用0、1、2、3、….4094这4095个数字，来表示同一机器同一时间截（毫秒)内产生的4095个ID序号，这种分配方式可以保证在任何一个IDC的任何一台机器在任意毫秒内生成的ID都是不同的。 优点： 毫秒数在高位，自增序列在低位，整个ID都是趋势递增的。 不依赖数据库等第三方系统，以服务的方式部署，稳定性更高，生成ID的性能也是非常高的。 可以根据自身业务特性分配bit位，非常灵活。 缺点： 强依赖机器时钟，在单机上是递增的，但是由于涉及到分布式环境，每台机器上的时钟不可能完全同步，还有闰秒的存在，会导致重复或者服务会处于不可用状态。 附上相关的代码： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150package com.dm.tool.util;/** * 通过 snowFlake 雪花算法生成唯一且在时间段内趋势递增的 * 分布式ID * @author Nick * @projectName dm-mt * @package com.dm.tool.util * @createDate 2019/01/17 09:17 * @updateDate 2019/01/17 09:17 */public class SnowFlake&#123; private static volatile SnowFlake instance ; /** * 起始的时间戳 * 从 2019/01/01 00:00:00 开始 */ private final static long START_STMP = 1546272000000L; /** * 序列号占用的位数 */ private final static long SEQUENCE_BIT = 12; /** * 机器标识占用的位数 */ private final static long MACHINE_BIT = 5; /** * 数据中心占用的位数 */ private final static long DATACENTER_BIT = 5; /** * 每一部分的最大值 */ private final static long MAX_DATACENTER_NUM = -1L ^ (-1L &lt;&lt; DATACENTER_BIT); private final static long MAX_MACHINE_NUM = -1L ^ (-1L &lt;&lt; MACHINE_BIT); private final static long MAX_SEQUENCE = -1L ^ (-1L &lt;&lt; SEQUENCE_BIT); /** * 每一部分向左的位移 */ private final static long MACHINE_LEFT = SEQUENCE_BIT; private final static long DATACENTER_LEFT = SEQUENCE_BIT + MACHINE_BIT; private final static long TIMESTMP_LEFT = DATACENTER_LEFT + DATACENTER_BIT; /** * 数据中心 */ private long datacenterId; /** * 机器标识 */ private long machineId; /** * 序列号 */ private long sequence = 0L; /** * 上一次时间戳 */ private long lastStmp = -1L; /** * * @param datacenterId 数据中心ID (0~31) * @param machineId workerId 工作ID (0~31) */ private SnowFlake(long datacenterId, long machineId) &#123; if (datacenterId &gt; MAX_DATACENTER_NUM || datacenterId &lt; 0) &#123; throw new IllegalArgumentException(String.format(\"datacenterId can't be greater than %d or less than 0\",MAX_DATACENTER_NUM)); &#125; if (machineId &gt; MAX_MACHINE_NUM || machineId &lt; 0) &#123; throw new IllegalArgumentException(String.format(\"machineId can't be greater than %d or less than 0\",MAX_MACHINE_NUM)); &#125; this.datacenterId = datacenterId; this.machineId = machineId; &#125; public static SnowFlake getInstance(long datacenterId, long machineId)&#123; if (instance == null)&#123; synchronized (SnowFlake.class)&#123; if (instance == null)&#123; instance = new SnowFlake(datacenterId,machineId); &#125; &#125; &#125; return instance; &#125; /** * 产生下一个ID * * @return */ protected synchronized long nextId() &#123; long currStmp = getNewTimestamp(); if (currStmp &lt; lastStmp) &#123; String msg = String.format(\"Clock moved backwards. Refusing to generate id! currStmp=%d,lastStmp=%d\",currStmp,lastStmp); throw new RuntimeException(msg); &#125; if (currStmp == lastStmp) &#123; // 相同毫秒内，序列号自增 sequence = (sequence + 1) &amp; MAX_SEQUENCE; // 同一毫秒的序列数已经达到最大 if (sequence == 0L) &#123; currStmp = getNextMill(); &#125; &#125; else &#123; // 不同毫秒内，序列号置为0 sequence = 0L; &#125; lastStmp = currStmp; // 时间戳 41 数据中心 5 机器标识 5 序列号 12 return (currStmp - START_STMP) &lt;&lt; TIMESTMP_LEFT | datacenterId &lt;&lt; DATACENTER_LEFT | machineId &lt;&lt; MACHINE_LEFT | sequence; &#125; private long getNextMill() &#123; long mill = getNewTimestamp(); while (mill &lt;= lastStmp) &#123; mill = getNewTimestamp(); &#125; return mill; &#125; private long getNewTimestamp() &#123; return System.currentTimeMillis(); &#125; public static long getSequenceBit() &#123; return SEQUENCE_BIT; &#125; public static long getMachineBit() &#123; return MACHINE_BIT; &#125; public static long getDatacenterBit() &#123; return DATACENTER_BIT; &#125; public static long getStartStmp() &#123; return START_STMP; &#125;&#125; 工具类如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192package com.dm.tool.util;import org.apache.commons.lang.StringUtils;import java.math.BigInteger;import java.util.HashSet;import java.util.Set;/** * * snowFlake工具类 * * @author Nick * @projectName dm-mt * @package com.dm.tool.util * @createDate 2019/01/16 09:30 * @updateDate 2019/01/16 09:30 */public class SnowFlakeIDUtil &#123; private static final int radix = 2; /** * 数据中心ID 0，机器ID 0 * @param datacenterId * @param machineId * @return */ public static long getNextId(long datacenterId, long machineId)&#123; return SnowFlake.getInstance(datacenterId,machineId).nextId(); &#125; /** * 获得订单ID生成时的时间戳 * @param id * @return */ public static long getIDTimestamp(long id)&#123; return (id &gt;&gt; SnowFlake.getTimestmpLeft() &amp; ~(-1L &lt;&lt; 41))+SnowFlake.getStartStmp(); &#125; /** * 获取数据中心 ID * @param id * @return */ public static long getDatacenterId(long id)&#123; return id &gt;&gt; SnowFlake.getDatacenterLeft() &amp; ~(-1L &lt;&lt; SnowFlake.getDatacenterBit()); &#125; /** * 获得机器ID * @param id * @return */ public static long getMachineId(long id)&#123; return id &gt;&gt; SnowFlake.getMachineLeft() &amp; ~(-1L &lt;&lt; SnowFlake.getMachineBit()); &#125; /** * 获取序列ID * @param id * @return */ public static long getSequence(long id)&#123; return id &amp; ~(-1L &lt;&lt; SnowFlake.getSequenceBit()); &#125; public static void main(String[] args)&#123; MyThread thread1 = new MyThread(); MyThread thread2 = new MyThread(); MyThread thread3 = new MyThread(); MyThread thread4 = new MyThread(); thread1.start(); thread2.start(); thread3.start(); thread4.start(); &#125; static class MyThread extends Thread&#123; @Override public void run() &#123; while (true)&#123; long id = SnowFlakeIDUtil.getNextId(0,0); System.out.println(\"ID=\"+id+\"时间戳= \"+getIDTimestamp(id)+\" DatacenterId= \"+getDatacenterId(id)+\" MachineId=\"+getMachineId(id)+\" Sequence=\"+getSequence(id)); &#125; &#125; &#125;&#125;","categories":[],"tags":[{"name":"java","slug":"java","permalink":"https://ydstudios.gitee.io/tags/java/"},{"name":"Snowflake","slug":"Snowflake","permalink":"https://ydstudios.gitee.io/tags/Snowflake/"}]},{"title":"使用FastJson进行对象和JSON转换属性命名规则为下划线和驼峰的问题","slug":"使用FastJson进行对象和JSON转换属性命名规则为下划线和驼峰的问题","date":"2018-12-24T23:20:33.000Z","updated":"2019-12-09T15:03:26.000Z","comments":true,"path":"post/ff52aa1f.html","link":"","permalink":"https://ydstudios.gitee.io/post/ff52aa1f.html","excerpt":"最近在对接支付宝，在编写支付需要的bean的时候，发现支付宝需要传递的参数命名方式都是下划线，自己idea又装了阿里巴巴的编程规约，代码老是有黄色提示，显得特别难看。于是乎我把属性改成了驼峰的命名方式，然后下意识的在上面加了一个@JsonProperty注解实现相互转换过程中属性命名方式也能自动转换。可是我运行代码时发现代码报错运行不起来了。主要代码如下：","text":"最近在对接支付宝，在编写支付需要的bean的时候，发现支付宝需要传递的参数命名方式都是下划线，自己idea又装了阿里巴巴的编程规约，代码老是有黄色提示，显得特别难看。于是乎我把属性改成了驼峰的命名方式，然后下意识的在上面加了一个@JsonProperty注解实现相互转换过程中属性命名方式也能自动转换。可是我运行代码时发现代码报错运行不起来了。主要代码如下： 12345678910111213141516171819202122232425262728public class AliPayParam &#123; @JsonProperty(name=\"out_trade_no\") private String outTradeNo; @JsonProperty(name=\"total_amount\") private String totalAmount; /** * 公共回传参数 */ @JsonProperty(name=\"passback_params\") private String passbackParams; private String subject; private String body; @JsonProperty(name=\"product_code\") private String productCode; /** * 该参数在请求到支付宝时开始计时,该笔订单允许的最晚付款时间，逾期将关闭交易。 * 取值范围：1m～15d。m-分钟，h-小时，d-天，1c-当天 * （1c-当天的情况下，无论交易何时创建，都在0点关闭）。 * 该参数数值不接受小数点， 如 1.5h，可转换为 90m。 */ @JsonProperty(name=\"timeout_express\") private String timeoutExpress; ｝ 突然发现这个@JsonProperty注解是Spring框架自带jackson的注解，不是阿里FastJson的注解，于是乎我看了看找到了下面的这个注解： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253public @interface JSONField &#123; /** * config encode/decode ordinal * @since 1.1.42 * @return */ // 配置序列化和反序列化的顺序，1.1.42版本之后才支持 int ordinal() default 0; // 指定字段的名称 String name() default \"\"; // 指定字段的格式，对日期格式有用 String format() default \"\"; // 是否序列化 boolean serialize() default true; // 是否反序列化 boolean deserialize() default true; SerializerFeature[] serialzeFeatures() default &#123;&#125;; Feature[] parseFeatures() default &#123;&#125;; String label() default \"\"; /** * @since 1.2.12 */ boolean jsonDirect() default false; /** * Serializer class to use for serializing associated value. * * @since 1.2.16 */ Class&lt;?&gt; serializeUsing() default Void.class; /** * Deserializer class to use for deserializing associated value. * * @since 1.2.16 */ Class&lt;?&gt; deserializeUsing() default Void.class; /** * @since 1.2.21 * @return the alternative names of the field when it is deserialized */ String[] alternateNames() default &#123;&#125;; /** * @since 1.2.31 */ boolean unwrapped() default false;&#125; 知道这个注解之后，把@JsonProperty替换成@JSONField注解就行了，转换成Json属性名就会从outTradeNo变成out_trade_no","categories":[],"tags":[{"name":"java","slug":"java","permalink":"https://ydstudios.gitee.io/tags/java/"},{"name":"fastJson","slug":"fastJson","permalink":"https://ydstudios.gitee.io/tags/fastJson/"}]},{"title":"谈谈Java集合ArrayList扩容","slug":"谈谈Java集合ArrayList扩容","date":"2018-12-13T23:19:26.000Z","updated":"2019-12-09T15:03:26.000Z","comments":true,"path":"post/3d08b907.html","link":"","permalink":"https://ydstudios.gitee.io/post/3d08b907.html","excerpt":"一 先从 ArrayList 的构造函数说起ArrayList有三种方式来初始化，构造方法源码如下：","text":"一 先从 ArrayList 的构造函数说起ArrayList有三种方式来初始化，构造方法源码如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647/** * 默认初始容量大小 */ private static final int DEFAULT_CAPACITY = 10; private static final Object[] DEFAULTCAPACITY_EMPTY_ELEMENTDATA = &#123;&#125;; /** *默认构造函数，使用初始容量10构造一个空列表(无参数构造) */ public ArrayList() &#123; this.elementData = DEFAULTCAPACITY_EMPTY_ELEMENTDATA; &#125; /** * 带初始容量参数的构造函数。（用户自己指定容量） */ public ArrayList(int initialCapacity) &#123; if (initialCapacity &gt; 0) &#123;//初始容量大于0 //创建initialCapacity大小的数组 this.elementData = new Object[initialCapacity]; &#125; else if (initialCapacity == 0) &#123;//初始容量等于0 //创建空数组 this.elementData = EMPTY_ELEMENTDATA; &#125; else &#123;//初始容量小于0，抛出异常 throw new IllegalArgumentException(\"Illegal Capacity: \"+ initialCapacity); &#125; &#125;/** *构造包含指定collection元素的列表，这些元素利用该集合的迭代器按顺序返回 *如果指定的集合为null，throws NullPointerException。 */ public ArrayList(Collection&lt;? extends E&gt; c) &#123; elementData = c.toArray(); if ((size = elementData.length) != 0) &#123; // c.toArray might (incorrectly) not return Object[] (see 6260652) if (elementData.getClass() != Object[].class) elementData = Arrays.copyOf(elementData, size, Object[].class); &#125; else &#123; // replace with empty array. this.elementData = EMPTY_ELEMENTDATA; &#125; &#125; 细心的同学一定会发现 ：以无参数构造方法创建 ArrayList 时，实际上初始化赋值的是一个空数组。当真正对数组进行添加元素操作时，才真正分配容量。即向数组中添加第一个元素时，数组容量扩为10。 下面在我们分析 ArrayList 扩容时会降到这一点内容！ 二 一步一步分析 ArrayList 扩容机制这里以无参构造函数创建的 ArrayList 为例分析 1. 先来看 add 方法12345678910 /** * 将指定的元素追加到此列表的末尾。 */ public boolean add(E e) &#123;//添加元素之前，先调用ensureCapacityInternal方法 ensureCapacityInternal(size + 1); // Increments modCount!! //这里看到ArrayList添加元素的实质就相当于为数组赋值 elementData[size++] = e; return true; &#125; 2. 再来看看 ensureCapacityInternal() 方法可以看到 add 方法 首先调用了ensureCapacityInternal(size + 1) 123456789//得到最小扩容量 private void ensureCapacityInternal(int minCapacity) &#123; if (elementData == DEFAULTCAPACITY_EMPTY_ELEMENTDATA) &#123; // 获取默认的容量和传入参数的较大值 minCapacity = Math.max(DEFAULT_CAPACITY, minCapacity); &#125; ensureExplicitCapacity(minCapacity); &#125; 当 要 add 进第1个元素时，minCapacity为1，在Math.max()方法比较后，minCapacity 为10。 3. ensureExplicitCapacity() 方法如果调用 ensureCapacityInternal() 方法就一定会进过（执行）这个方法，下面我们来研究一下这个方法的源码！ 123456789//判断是否需要扩容 private void ensureExplicitCapacity(int minCapacity) &#123; modCount++; // overflow-conscious code if (minCapacity - elementData.length &gt; 0) //调用grow方法进行扩容，调用此方法代表已经开始扩容了 grow(minCapacity); &#125; 我们来仔细分析一下： 当我们要 add 进第1个元素到 ArrayList 时，elementData.length 为0 （因为还是一个空的 list），因为执行了 ensureCapacityInternal() 方法 ，所以 minCapacity 此时为10。此时，minCapacity - elementData.length &gt; 0成立，所以会进入 grow(minCapacity) 方法。 当add第2个元素时，minCapacity 为2，此时e lementData.length(容量)在添加第一个元素后扩容成 10 了。此时，minCapacity - elementData.length &gt; 0 不成立，所以不会进入 （执行）grow(minCapacity) 方法。 添加第3、4···到第10个元素时，依然不会执行grow方法，数组容量都为10。 直到添加第11个元素，minCapacity(为11)比elementData.length（为10）要大。进入grow方法进行扩容。 4. grow() 方法123456789101112131415161718192021222324/** * 要分配的最大数组大小 */private static final int MAX_ARRAY_SIZE = Integer.MAX_VALUE - 8;/** * ArrayList扩容的核心方法。 */private void grow(int minCapacity) &#123; // oldCapacity为旧容量，newCapacity为新容量 int oldCapacity = elementData.length; //将oldCapacity 右移一位，其效果相当于oldCapacity /2， //我们知道位运算的速度远远快于整除运算，整句运算式的结果就是将新容量更新为旧容量的1.5倍， int newCapacity = oldCapacity + (oldCapacity &gt;&gt; 1); //然后检查新容量是否大于最小需要容量，若还是小于最小需要容量，那么就把最小需要容量当作数组的新容量， if (newCapacity - minCapacity &lt; 0) newCapacity = minCapacity; // 如果新容量大于 MAX_ARRAY_SIZE,进入(执行) `hugeCapacity()` 方法来比较 minCapacity 和 MAX_ARRAY_SIZE， //如果minCapacity大于最大容量，则新容量则为`Integer.MAX_VALUE`，否则，新容量大小则为 MAX_ARRAY_SIZE 即为 `Integer.MAX_VALUE - 8`。 if (newCapacity - MAX_ARRAY_SIZE &gt; 0) newCapacity = hugeCapacity(minCapacity); // minCapacity is usually close to size, so this is a win: elementData = Arrays.copyOf(elementData, newCapacity);&#125; int newCapacity = oldCapacity + (oldCapacity &gt;&gt; 1),所以 ArrayList 每次扩容之后容量都会变为原来的 1.5 倍！ 记清楚了！不是网上很多人说的 1.5 倍+1！ “&gt;&gt;”（移位运算符）：&gt;&gt;1 右移一位相当于除2，右移n位相当于除以 2 的 n 次方。这里 oldCapacity 明显右移了1位所以相当于oldCapacity /2。对于大数据的2进制运算,位移运算符比那些普通运算符的运算要快很多,因为程序仅仅移动一下而已,不去计算,这样提高了效率,节省了资源 我们再来通过例子探究一下grow() 方法 ： 当add第1个元素时，oldCapacity 为0，经比较后第一个if判断成立，newCapacity = minCapacity(为10)。但是第二个if判断不会成立，即newCapacity 不比 MAX_ARRAY_SIZE大，则不会进入 hugeCapacity 方法。数组容量为10，add方法中 return true,size增为1。 当add第11个元素进入grow方法时，newCapacity为15，比minCapacity（为11）大，第一个if判断不成立。新容量没有大于数组最大size，不会进入hugeCapacity方法。数组容量扩为15，add方法中return true,size增为11。 以此类推······ 这里补充一点比较重要，但是容易被忽视掉的知识点： java 中的 length属性是针对数组说的,比如说你声明了一个数组,想知道这个数组的长度则用到了 length 这个属性. java 中的 length() 方法是针对字符串说的,如果想看这个字符串的长度则用到 length() 这个方法. java 中的 size() 方法是针对泛型集合说的,如果想看这个泛型有多少个元素,就调用此方法来查看! 5. hugeCapacity() 方法。从上面 grow() 方法源码我们知道： 如果新容量大于 MAX_ARRAY_SIZE,进入(执行) hugeCapacity() 方法来比较 minCapacity 和 MAX_ARRAY_SIZE，如果minCapacity大于最大容量，则新容量则为Integer.MAX_VALUE，否则，新容量大小则为 MAX_ARRAY_SIZE 即为 Integer.MAX_VALUE - 8。 1234567891011private static int hugeCapacity(int minCapacity) &#123; if (minCapacity &lt; 0) // overflow throw new OutOfMemoryError(); //对minCapacity和MAX_ARRAY_SIZE进行比较 //若minCapacity大，将Integer.MAX_VALUE作为新数组的大小 //若MAX_ARRAY_SIZE大，将MAX_ARRAY_SIZE作为新数组的大小 //MAX_ARRAY_SIZE = Integer.MAX_VALUE - 8; return (minCapacity &gt; MAX_ARRAY_SIZE) ? Integer.MAX_VALUE : MAX_ARRAY_SIZE;&#125; 三 System.arraycopy() 和 Arrays.copyOf()方法阅读源码的话，我们就会发现 ArrayList 中大量调用了这两个方法。比如：我们上面讲的扩容操作以及add(int index, E element)、toArray() 等方法中都用到了该方法！ 3.1 System.arraycopy() 方法123456789101112131415/** * 在此列表中的指定位置插入指定的元素。 *先调用 rangeCheckForAdd 对index进行界限检查；然后调用 ensureCapacityInternal 方法保证capacity足够大； *再将从index开始之后的所有成员后移一个位置；将element插入index位置；最后size加1。 */public void add(int index, E element) &#123; rangeCheckForAdd(index); ensureCapacityInternal(size + 1); // Increments modCount!! //arraycopy()方法实现数组自己复制自己 //elementData:源数组;index:源数组中的起始位置;elementData：目标数组；index + 1：目标数组中的起始位置； size - index：要复制的数组元素的数量； System.arraycopy(elementData, index, elementData, index + 1, size - index); elementData[index] = element; size++;&#125; 我们写一个简单的方法测试以下： 1234567891011121314151617public class ArraycopyTest &#123; public static void main(String[] args) &#123; // TODO Auto-generated method stub int[] a = new int[10]; a[0] = 0; a[1] = 1; a[2] = 2; a[3] = 3; System.arraycopy(a, 2, a, 3, 3); a[2]=99; for (int i = 0; i &lt; a.length; i++) &#123; System.out.println(a[i]); &#125; &#125;&#125; 结果： 10 1 99 2 3 0 0 0 0 0 3.2 Arrays.copyOf()方法1234567/** 以正确的顺序返回一个包含此列表中所有元素的数组（从第一个到最后一个元素）; 返回的数组的运行时类型是指定数组的运行时类型。 */ public Object[] toArray() &#123; //elementData：要复制的数组；size：要复制的长度 return Arrays.copyOf(elementData, size); &#125; 个人觉得使用 Arrays.copyOf()方法主要是为了给原有数组扩容，测试代码如下： 1234567891011public class ArrayscopyOfTest &#123; public static void main(String[] args) &#123; int[] a = new int[3]; a[0] = 0; a[1] = 1; a[2] = 2; int[] b = Arrays.copyOf(a, 10); System.out.println(\"b.length\"+b.length); &#125;&#125; 结果： 110 3.3 两者联系和区别联系： 看两者源代码可以发现 copyOf() 内部实际调用了 System.arraycopy() 方法 区别： arraycopy() 需要目标数组，将原数组拷贝到你自己定义的数组里或者原数组，而且可以选择拷贝的起点和长度以及放入新数组中的位置 copyOf() 是系统自动在内部新建一个数组，并返回该数组。 四 ensureCapacity方法ArrayList 源码中有一个 ensureCapacity 方法不知道大家注意到没有，这个方法 ArrayList 内部没有被调用过，所以很显然是提供给用户调用的，那么这个方法有什么作用呢？ 1234567891011121314151617/**如有必要，增加此 ArrayList 实例的容量，以确保它至少可以容纳由minimum capacity参数指定的元素数。 * * @param minCapacity 所需的最小容量 */public void ensureCapacity(int minCapacity) &#123; int minExpand = (elementData != DEFAULTCAPACITY_EMPTY_ELEMENTDATA) // any size if not default element table ? 0 // larger than default for default empty table. It's already // supposed to be at default size. : DEFAULT_CAPACITY; if (minCapacity &gt; minExpand) &#123; ensureExplicitCapacity(minCapacity); &#125;&#125; 最好在 add 大量元素之前用 ensureCapacity 方法，以减少增量从新分配的次数 我们通过下面的代码实际测试以下这个方法的效果： 123456789101112131415161718192021public class EnsureCapacityTest &#123; public static void main(String[] args) &#123; ArrayList&lt;Object&gt; list = new ArrayList&lt;Object&gt;(); final int N = 10000000; long startTime = System.currentTimeMillis(); for (int i = 0; i &lt; N; i++) &#123; list.add(i); &#125; long endTime = System.currentTimeMillis(); System.out.println(\"使用ensureCapacity方法前：\"+(endTime - startTime)); list = new ArrayList&lt;Object&gt;(); long startTime1 = System.currentTimeMillis(); list.ensureCapacity(N); for (int i = 0; i &lt; N; i++) &#123; list.add(i); &#125; long endTime1 = System.currentTimeMillis(); System.out.println(\"使用ensureCapacity方法后：\"+(endTime1 - startTime1)); &#125;&#125; 运行结果： 12使用ensureCapacity方法前：4637使用ensureCapacity方法后：241 通过运行结果，我们可以很明显的看出向 ArrayList 添加大量元素之前最好先使用ensureCapacity 方法，以减少增量从新分配的次数 查看原文","categories":[],"tags":[{"name":"java","slug":"java","permalink":"https://ydstudios.gitee.io/tags/java/"},{"name":"arrayList","slug":"arrayList","permalink":"https://ydstudios.gitee.io/tags/arrayList/"}]},{"title":"谈谈Java集合ArrayList","slug":"谈谈Java集合ArrayList","date":"2018-11-30T23:17:57.000Z","updated":"2019-12-09T15:03:26.000Z","comments":true,"path":"post/e6d29090.html","link":"","permalink":"https://ydstudios.gitee.io/post/e6d29090.html","excerpt":"ArrayList简介 ArrayList核心源码 ArrayList源码分析 System.arraycopy()和Arrays.copyOf()方法 两者联系与区别 ArrayList核心扩容技术 内部类 ArrayList经典Demo ArrayList简介 ArrayList 的底层是数组队列，相当于动态数组。与 Java 中的数组相比，它的容量能动态增长。在添加大量元素前，应用程序可以使用ensureCapacity操作来增加 ArrayList 实例的容量。这可以减少递增式再分配的数量。","text":"ArrayList简介 ArrayList核心源码 ArrayList源码分析 System.arraycopy()和Arrays.copyOf()方法 两者联系与区别 ArrayList核心扩容技术 内部类 ArrayList经典Demo ArrayList简介 ArrayList 的底层是数组队列，相当于动态数组。与 Java 中的数组相比，它的容量能动态增长。在添加大量元素前，应用程序可以使用ensureCapacity操作来增加 ArrayList 实例的容量。这可以减少递增式再分配的数量。 它继承于 AbstractList，实现了 List, RandomAccess, Cloneable, java.io.Serializable 这些接口。 在我们学数据结构的时候就知道了线性表的顺序存储，插入删除元素的时间复杂度为O（n）,求表长以及增加元素，取第 i 元素的时间复杂度为O（1） ArrayList 继承了AbstractList，实现了List。它是一个数组队列，提供了相关的添加、删除、修改、遍历等功能。 ArrayList 实现了RandomAccess 接口，即提供了随机访问功能。RandomAccess 是 Java 中用来被 List 实现，为 List 提供快速访问功能的。在 ArrayList 中，我们即可以通过元素的序号快速获取元素对象，这就是快速随机访问。 ArrayList 实现了Cloneable 接口，即覆盖了函数 clone()，能被克隆。 ArrayList 实现java.io.Serializable 接口，这意味着ArrayList支持序列化，能通过序列化去传输。 和 Vector 不同，ArrayList 中的操作不是线程安全的！所以，建议在单线程中才使用 ArrayList，而在多线程中可以选择 Vector 或者 CopyOnWriteArrayList。 ArrayList核心源码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382383384385386387388389390391392393394395396397398399400401402403404405406407408409410411412413414415416417418419420421422423424425426427428429430431432433434435436437438439440441442443444445446447448449450451452453454455456457458459460461462463464465466467468469470471472473474475476477478479480481482483484485486487488489490491492493494495496497498package java.util;import java.util.function.Consumer;import java.util.function.Predicate;import java.util.function.UnaryOperator;public class ArrayList&lt;E&gt; extends AbstractList&lt;E&gt; implements List&lt;E&gt;, RandomAccess, Cloneable, java.io.Serializable&#123; private static final long serialVersionUID = 8683452581122892189L; /** * 默认初始容量大小 */ private static final int DEFAULT_CAPACITY = 10; /** * 空数组（用于空实例）。 */ private static final Object[] EMPTY_ELEMENTDATA = &#123;&#125;; //用于默认大小空实例的共享空数组实例。 //我们把它从EMPTY_ELEMENTDATA数组中区分出来，以知道在添加第一个元素时容量需要增加多少。 private static final Object[] DEFAULTCAPACITY_EMPTY_ELEMENTDATA = &#123;&#125;; /** * 保存ArrayList数据的数组 */ transient Object[] elementData; // non-private to simplify nested class access /** * ArrayList 所包含的元素个数 */ private int size; /** * 带初始容量参数的构造函数。（用户自己指定容量） */ public ArrayList(int initialCapacity) &#123; if (initialCapacity &gt; 0) &#123; //创建initialCapacity大小的数组 this.elementData = new Object[initialCapacity]; &#125; else if (initialCapacity == 0) &#123; //创建空数组 this.elementData = EMPTY_ELEMENTDATA; &#125; else &#123; throw new IllegalArgumentException(\"Illegal Capacity: \"+ initialCapacity); &#125; &#125; /** *默认构造函数，其默认初始容量为10 */ public ArrayList() &#123; this.elementData = DEFAULTCAPACITY_EMPTY_ELEMENTDATA; &#125; /** * 构造一个包含指定集合的元素的列表，按照它们由集合的迭代器返回的顺序。 */ public ArrayList(Collection&lt;? extends E&gt; c) &#123; // elementData = c.toArray(); //如果指定集合元素个数不为0 if ((size = elementData.length) != 0) &#123; // c.toArray 可能返回的不是Object类型的数组所以加上下面的语句用于判断， //这里用到了反射里面的getClass()方法 if (elementData.getClass() != Object[].class) elementData = Arrays.copyOf(elementData, size, Object[].class); &#125; else &#123; // 用空数组代替 this.elementData = EMPTY_ELEMENTDATA; &#125; &#125; /** * 修改这个ArrayList实例的容量是列表的当前大小。 应用程序可以使用此操作来最小化ArrayList实例的存储。 */ public void trimToSize() &#123; modCount++; if (size &lt; elementData.length) &#123; elementData = (size == 0) ? EMPTY_ELEMENTDATA : Arrays.copyOf(elementData, size); &#125; &#125;//下面是ArrayList的扩容机制//ArrayList的扩容机制提高了性能，如果每次只扩充一个，//那么频繁的插入会导致频繁的拷贝，降低性能，而ArrayList的扩容机制避免了这种情况。 /** * 如有必要，增加此ArrayList实例的容量，以确保它至少能容纳元素的数量 * @param minCapacity 所需的最小容量 */ public void ensureCapacity(int minCapacity) &#123; int minExpand = (elementData != DEFAULTCAPACITY_EMPTY_ELEMENTDATA) // any size if not default element table ? 0 // larger than default for default empty table. It's already // supposed to be at default size. : DEFAULT_CAPACITY; if (minCapacity &gt; minExpand) &#123; ensureExplicitCapacity(minCapacity); &#125; &#125; //得到最小扩容量 private void ensureCapacityInternal(int minCapacity) &#123; if (elementData == DEFAULTCAPACITY_EMPTY_ELEMENTDATA) &#123; // 获取默认的容量和传入参数的较大值 minCapacity = Math.max(DEFAULT_CAPACITY, minCapacity); &#125; ensureExplicitCapacity(minCapacity); &#125; //判断是否需要扩容 private void ensureExplicitCapacity(int minCapacity) &#123; modCount++; // overflow-conscious code if (minCapacity - elementData.length &gt; 0) //调用grow方法进行扩容，调用此方法代表已经开始扩容了 grow(minCapacity); &#125; /** * 要分配的最大数组大小 */ private static final int MAX_ARRAY_SIZE = Integer.MAX_VALUE - 8; /** * ArrayList扩容的核心方法。 */ private void grow(int minCapacity) &#123; // oldCapacity为旧容量，newCapacity为新容量 int oldCapacity = elementData.length; //将oldCapacity 右移一位，其效果相当于oldCapacity /2， //我们知道位运算的速度远远快于整除运算，整句运算式的结果就是将新容量更新为旧容量的1.5倍， int newCapacity = oldCapacity + (oldCapacity &gt;&gt; 1); //然后检查新容量是否大于最小需要容量，若还是小于最小需要容量，那么就把最小需要容量当作数组的新容量， if (newCapacity - minCapacity &lt; 0) newCapacity = minCapacity; //再检查新容量是否超出了ArrayList所定义的最大容量， //若超出了，则调用hugeCapacity()来比较minCapacity和 MAX_ARRAY_SIZE， //如果minCapacity大于最大容量，则新容量则为ArrayList定义的最大容量，否则，新容量大小则为 minCapacity。 if (newCapacity - MAX_ARRAY_SIZE &gt; 0) newCapacity = hugeCapacity(minCapacity); // minCapacity is usually close to size, so this is a win: elementData = Arrays.copyOf(elementData, newCapacity); &#125; //比较minCapacity和 MAX_ARRAY_SIZE private static int hugeCapacity(int minCapacity) &#123; if (minCapacity &lt; 0) // overflow throw new OutOfMemoryError(); return (minCapacity &gt; MAX_ARRAY_SIZE) ? Integer.MAX_VALUE : MAX_ARRAY_SIZE; &#125; /** *返回此列表中的元素数。 */ public int size() &#123; return size; &#125; /** * 如果此列表不包含元素，则返回 true 。 */ public boolean isEmpty() &#123; //注意=和==的区别 return size == 0; &#125; /** * 如果此列表包含指定的元素，则返回true 。 */ public boolean contains(Object o) &#123; //indexOf()方法：返回此列表中指定元素的首次出现的索引，如果此列表不包含此元素，则为-1 return indexOf(o) &gt;= 0; &#125; /** *返回此列表中指定元素的首次出现的索引，如果此列表不包含此元素，则为-1 */ public int indexOf(Object o) &#123; if (o == null) &#123; for (int i = 0; i &lt; size; i++) if (elementData[i]==null) return i; &#125; else &#123; for (int i = 0; i &lt; size; i++) //equals()方法比较 if (o.equals(elementData[i])) return i; &#125; return -1; &#125; /** * 返回此列表中指定元素的最后一次出现的索引，如果此列表不包含元素，则返回-1。. */ public int lastIndexOf(Object o) &#123; if (o == null) &#123; for (int i = size-1; i &gt;= 0; i--) if (elementData[i]==null) return i; &#125; else &#123; for (int i = size-1; i &gt;= 0; i--) if (o.equals(elementData[i])) return i; &#125; return -1; &#125; /** * 返回此ArrayList实例的浅拷贝。 （元素本身不被复制。） */ public Object clone() &#123; try &#123; ArrayList&lt;?&gt; v = (ArrayList&lt;?&gt;) super.clone(); //Arrays.copyOf功能是实现数组的复制，返回复制后的数组。参数是被复制的数组和复制的长度 v.elementData = Arrays.copyOf(elementData, size); v.modCount = 0; return v; &#125; catch (CloneNotSupportedException e) &#123; // 这不应该发生，因为我们是可以克隆的 throw new InternalError(e); &#125; &#125; /** *以正确的顺序（从第一个到最后一个元素）返回一个包含此列表中所有元素的数组。 *返回的数组将是“安全的”，因为该列表不保留对它的引用。 （换句话说，这个方法必须分配一个新的数组）。 *因此，调用者可以自由地修改返回的数组。 此方法充当基于阵列和基于集合的API之间的桥梁。 */ public Object[] toArray() &#123; return Arrays.copyOf(elementData, size); &#125; /** * 以正确的顺序返回一个包含此列表中所有元素的数组（从第一个到最后一个元素）; *返回的数组的运行时类型是指定数组的运行时类型。 如果列表适合指定的数组，则返回其中。 *否则，将为指定数组的运行时类型和此列表的大小分配一个新数组。 *如果列表适用于指定的数组，其余空间（即数组的列表数量多于此元素），则紧跟在集合结束后的数组中的元素设置为null 。 *（这仅在调用者知道列表不包含任何空元素的情况下才能确定列表的长度。） */ @SuppressWarnings(\"unchecked\") public &lt;T&gt; T[] toArray(T[] a) &#123; if (a.length &lt; size) // 新建一个运行时类型的数组，但是ArrayList数组的内容 return (T[]) Arrays.copyOf(elementData, size, a.getClass()); //调用System提供的arraycopy()方法实现数组之间的复制 System.arraycopy(elementData, 0, a, 0, size); if (a.length &gt; size) a[size] = null; return a; &#125; // Positional Access Operations @SuppressWarnings(\"unchecked\") E elementData(int index) &#123; return (E) elementData[index]; &#125; /** * 返回此列表中指定位置的元素。 */ public E get(int index) &#123; rangeCheck(index); return elementData(index); &#125; /** * 用指定的元素替换此列表中指定位置的元素。 */ public E set(int index, E element) &#123; //对index进行界限检查 rangeCheck(index); E oldValue = elementData(index); elementData[index] = element; //返回原来在这个位置的元素 return oldValue; &#125; /** * 将指定的元素追加到此列表的末尾。 */ public boolean add(E e) &#123; ensureCapacityInternal(size + 1); // Increments modCount!! //这里看到ArrayList添加元素的实质就相当于为数组赋值 elementData[size++] = e; return true; &#125; /** * 在此列表中的指定位置插入指定的元素。 *先调用 rangeCheckForAdd 对index进行界限检查；然后调用 ensureCapacityInternal 方法保证capacity足够大； *再将从index开始之后的所有成员后移一个位置(本人修改，移动成员包括index所在元素)；将element插入index位置；最后size加1。 */ public void add(int index, E element) &#123; rangeCheckForAdd(index); ensureCapacityInternal(size + 1); // Increments modCount!! //arraycopy()这个实现数组之间复制的方法一定要看一下，下面就用到了arraycopy()方法实现数组自己复制自己 System.arraycopy(elementData, index, elementData, index + 1, size - index); elementData[index] = element; size++; &#125; /** * 删除该列表中指定位置的元素。 将任何后续元素移动到左侧（从其索引中减去一个元素）。 */ public E remove(int index) &#123; rangeCheck(index); modCount++; E oldValue = elementData(index); int numMoved = size - index - 1; if (numMoved &gt; 0) System.arraycopy(elementData, index+1, elementData, index, numMoved); elementData[--size] = null; // clear to let GC do its work //从列表中删除的元素 return oldValue; &#125; /** * 从列表中删除指定元素的第一个出现（如果存在）。 如果列表不包含该元素，则它不会更改。 *返回true，如果此列表包含指定的元素 */ public boolean remove(Object o) &#123; if (o == null) &#123; for (int index = 0; index &lt; size; index++) if (elementData[index] == null) &#123; fastRemove(index); return true; &#125; &#125; else &#123; for (int index = 0; index &lt; size; index++) if (o.equals(elementData[index])) &#123; fastRemove(index); return true; &#125; &#125; return false; &#125; /* * Private remove method that skips bounds checking and does not * return the value removed. */ private void fastRemove(int index) &#123; modCount++; int numMoved = size - index - 1; if (numMoved &gt; 0) System.arraycopy(elementData, index+1, elementData, index, numMoved); elementData[--size] = null; // clear to let GC do its work &#125; /** * 从列表中删除所有元素。 */ public void clear() &#123; modCount++; // 把数组中所有的元素的值设为null for (int i = 0; i &lt; size; i++) elementData[i] = null; size = 0; &#125; /** * 按指定集合的Iterator返回的顺序将指定集合中的所有元素追加到此列表的末尾。 */ public boolean addAll(Collection&lt;? extends E&gt; c) &#123; Object[] a = c.toArray(); int numNew = a.length; ensureCapacityInternal(size + numNew); // Increments modCount System.arraycopy(a, 0, elementData, size, numNew); size += numNew; return numNew != 0; &#125; /** * 将指定集合中的所有元素插入到此列表中，从指定的位置开始。 */ public boolean addAll(int index, Collection&lt;? extends E&gt; c) &#123; rangeCheckForAdd(index); Object[] a = c.toArray(); int numNew = a.length; ensureCapacityInternal(size + numNew); // Increments modCount int numMoved = size - index; if (numMoved &gt; 0) System.arraycopy(elementData, index, elementData, index + numNew, numMoved); System.arraycopy(a, 0, elementData, index, numNew); size += numNew; return numNew != 0; &#125; /** * 从此列表中删除所有索引为fromIndex （含）和toIndex之间的元素。 *将任何后续元素移动到左侧（减少其索引）。 */ protected void removeRange(int fromIndex, int toIndex) &#123; modCount++; int numMoved = size - toIndex; System.arraycopy(elementData, toIndex, elementData, fromIndex, numMoved); // clear to let GC do its work int newSize = size - (toIndex-fromIndex); for (int i = newSize; i &lt; size; i++) &#123; elementData[i] = null; &#125; size = newSize; &#125; /** * 检查给定的索引是否在范围内。 */ private void rangeCheck(int index) &#123; if (index &gt;= size) throw new IndexOutOfBoundsException(outOfBoundsMsg(index)); &#125; /** * add和addAll使用的rangeCheck的一个版本 */ private void rangeCheckForAdd(int index) &#123; if (index &gt; size || index &lt; 0) throw new IndexOutOfBoundsException(outOfBoundsMsg(index)); &#125; /** * 返回IndexOutOfBoundsException细节信息 */ private String outOfBoundsMsg(int index) &#123; return \"Index: \"+index+\", Size: \"+size; &#125; /** * 从此列表中删除指定集合中包含的所有元素。 */ public boolean removeAll(Collection&lt;?&gt; c) &#123; Objects.requireNonNull(c); //如果此列表被修改则返回true return batchRemove(c, false); &#125; /** * 仅保留此列表中包含在指定集合中的元素。 *换句话说，从此列表中删除其中不包含在指定集合中的所有元素。 */ public boolean retainAll(Collection&lt;?&gt; c) &#123; Objects.requireNonNull(c); return batchRemove(c, true); &#125; /** * 从列表中的指定位置开始，返回列表中的元素（按正确顺序）的列表迭代器。 *指定的索引表示初始调用将返回的第一个元素为next 。 初始调用previous将返回指定索引减1的元素。 *返回的列表迭代器是fail-fast 。 */ public ListIterator&lt;E&gt; listIterator(int index) &#123; if (index &lt; 0 || index &gt; size) throw new IndexOutOfBoundsException(\"Index: \"+index); return new ListItr(index); &#125; /** *返回列表中的列表迭代器（按适当的顺序）。 *返回的列表迭代器是fail-fast 。 */ public ListIterator&lt;E&gt; listIterator() &#123; return new ListItr(0); &#125; /** *以正确的顺序返回该列表中的元素的迭代器。 *返回的迭代器是fail-fast 。 */ public Iterator&lt;E&gt; iterator() &#123; return new Itr(); &#125; ArrayList源码分析System.arraycopy()和Arrays.copyOf()方法 通过上面源码我们发现这两个实现数组复制的方法被广泛使用而且很多地方都特别巧妙。比如下面add(int index, E element)方法就很巧妙的用到了arraycopy()方法让数组自己复制自己实现让index开始之后的所有成员后移一个位置: 123456789101112131415/** * 在此列表中的指定位置插入指定的元素。 *先调用 rangeCheckForAdd 对index进行界限检查；然后调用 ensureCapacityInternal 方法保证capacity足够大； *再将从index开始之后的所有成员后移一个位置；将element插入index位置；最后size加1。 */public void add(int index, E element) &#123; rangeCheckForAdd(index); ensureCapacityInternal(size + 1); // Increments modCount!! //arraycopy()方法实现数组自己复制自己 //elementData:源数组;index:源数组中的起始位置;elementData：目标数组；index + 1：目标数组中的起始位置； size - index：要复制的数组元素的数量； System.arraycopy(elementData, index, elementData, index + 1, size - index); elementData[index] = element; size++;&#125; 又如toArray()方法中用到了copyOf()方法 12345678910/** *以正确的顺序（从第一个到最后一个元素）返回一个包含此列表中所有元素的数组。 *返回的数组将是“安全的”，因为该列表不保留对它的引用。 （换句话说，这个方法必须分配一个新的数组）。 *因此，调用者可以自由地修改返回的数组。 此方法充当基于阵列和基于集合的API之间的桥梁。 */public Object[] toArray() &#123;//elementData：要复制的数组；size：要复制的长度 return Arrays.copyOf(elementData, size);&#125; 两者联系与区别联系：看两者源代码可以发现copyOf()内部调用了System.arraycopy()方法区别： arraycopy()需要目标数组，将原数组拷贝到你自己定义的数组里，而且可以选择拷贝的起点和长度以及放入新数组中的位置 copyOf()是系统自动在内部新建一个数组，并返回该数组。ArrayList 核心扩容技术1234567891011121314151617181920212223242526272829303132333435363738//下面是ArrayList的扩容机制//ArrayList的扩容机制提高了性能，如果每次只扩充一个，//那么频繁的插入会导致频繁的拷贝，降低性能，而ArrayList的扩容机制避免了这种情况。 /** * 如有必要，增加此ArrayList实例的容量，以确保它至少能容纳元素的数量 * @param minCapacity 所需的最小容量 */ public void ensureCapacity(int minCapacity) &#123; int minExpand = (elementData != DEFAULTCAPACITY_EMPTY_ELEMENTDATA) // any size if not default element table ? 0 // larger than default for default empty table. It's already // supposed to be at default size. : DEFAULT_CAPACITY; if (minCapacity &gt; minExpand) &#123; ensureExplicitCapacity(minCapacity); &#125; &#125; //得到最小扩容量 private void ensureCapacityInternal(int minCapacity) &#123; if (elementData == DEFAULTCAPACITY_EMPTY_ELEMENTDATA) &#123; // 获取默认的容量和传入参数的较大值 minCapacity = Math.max(DEFAULT_CAPACITY, minCapacity); &#125; ensureExplicitCapacity(minCapacity); &#125; //判断是否需要扩容,上面两个方法都要调用 private void ensureExplicitCapacity(int minCapacity) &#123; modCount++; // 如果说minCapacity也就是所需的最小容量大于保存ArrayList数据的数组的长度的话，就需要调用grow(minCapacity)方法扩容。 //这个minCapacity到底为多少呢？举个例子在添加元素(add)方法中这个minCapacity的大小就为现在数组的长度加1 if (minCapacity - elementData.length &gt; 0) //调用grow方法进行扩容，调用此方法代表已经开始扩容了 grow(minCapacity); &#125; 12345678910111213141516171819202122/** * ArrayList扩容的核心方法。 */private void grow(int minCapacity) &#123; //elementData为保存ArrayList数据的数组 ///elementData.length求数组长度elementData.size是求数组中的元素个数 // oldCapacity为旧容量，newCapacity为新容量 int oldCapacity = elementData.length; //将oldCapacity 右移一位，其效果相当于oldCapacity /2， //我们知道位运算的速度远远快于整除运算，整句运算式的结果就是将新容量更新为旧容量的1.5倍， int newCapacity = oldCapacity + (oldCapacity &gt;&gt; 1); //然后检查新容量是否大于最小需要容量，若还是小于最小需要容量，那么就把最小需要容量当作数组的新容量， if (newCapacity - minCapacity &lt; 0) newCapacity = minCapacity; //再检查新容量是否超出了ArrayList所定义的最大容量， //若超出了，则调用hugeCapacity()来比较minCapacity和 MAX_ARRAY_SIZE， //如果minCapacity大于最大容量，则新容量则为ArrayList定义的最大容量，否则，新容量大小则为 minCapacity。 if (newCapacity - MAX_ARRAY_SIZE &gt; 0) newCapacity = hugeCapacity(minCapacity); // minCapacity is usually close to size, so this is a win: elementData = Arrays.copyOf(elementData, newCapacity);&#125; 扩容机制代码已经做了详细的解释。另外值得注意的是大家很容易忽略的一个运算符：移位运算符 简介：移位运算符就是在二进制的基础上对数字进行平移。按照平移的方向和填充数字的规则分为三种:&lt;&lt;(左移)、&gt;&gt;(带符号右移)和&gt;&gt;&gt;(无符号右移)。 作用：对于大数据的2进制运算,位移运算符比那些普通运算符的运算要快很多,因为程序仅仅移动一下而已,不去计算,这样提高了效率,节省了资源 比如这里：int newCapacity = oldCapacity + (oldCapacity &gt;&gt; 1);右移一位相当于除2，右移n位相当于除以 2 的 n 次方。这里 oldCapacity 明显右移了1位所以相当于oldCapacity /2。 另外需要注意的是： java 中的length 属性是针对数组说的,比如说你声明了一个数组,想知道这个数组的长度则用到了 length 这个属性. java 中的length()方法是针对字 符串String说的,如果想看这个字符串的长度则用到 length()这个方法. .java 中的size()方法是针对泛型集合说的,如果想看这个泛型有多少个元素,就调用此方法来查看! 内部类123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778 (1)private class Itr implements Iterator&lt;E&gt; (2)private class ListItr extends Itr implements ListIterator&lt;E&gt; (3)private class SubList extends AbstractList&lt;E&gt; implements RandomAccess (4)static final class ArrayListSpliterator&lt;E&gt; implements Spliterator&lt;E&gt; ``` ArrayList有四个内部类，其中的**Itr是实现了Iterator接口**，同时重写了里面的**hasNext()**，**next()**，**remove()**等方法；其中的**ListItr**继承**Itr**，实现了**ListIterator接口**，同时重写了**hasPrevious()**，**nextIndex()**，**previousIndex()**，**previous()**，**set(E e)**，**add(E e)**等方法，所以这也可以看出了**Iterator和ListIterator的区别:**ListIterator在Iterator的基础上增加了添加对象，修改对象，逆向遍历等方法，这些是Iterator不能实现的。### ArrayList经典Demo```javapackage list;import java.util.ArrayList;import java.util.Iterator;public class ArrayListDemo &#123; public static void main(String[] srgs)&#123; ArrayList&lt;Integer&gt; arrayList = new ArrayList&lt;Integer&gt;(); System.out.printf(\"Before add:arrayList.size() = %d\\n\",arrayList.size()); arrayList.add(1); arrayList.add(3); arrayList.add(5); arrayList.add(7); arrayList.add(9); System.out.printf(\"After add:arrayList.size() = %d\\n\",arrayList.size()); System.out.println(\"Printing elements of arrayList\"); // 三种遍历方式打印元素 // 第一种：通过迭代器遍历 System.out.print(\"通过迭代器遍历:\"); Iterator&lt;Integer&gt; it = arrayList.iterator(); while(it.hasNext())&#123; System.out.print(it.next() + \" \"); &#125; System.out.println(); // 第二种：通过索引值遍历 System.out.print(\"通过索引值遍历:\"); for(int i = 0; i &lt; arrayList.size(); i++)&#123; System.out.print(arrayList.get(i) + \" \"); &#125; System.out.println(); // 第三种：for循环遍历 System.out.print(\"for循环遍历:\"); for(Integer number : arrayList)&#123; System.out.print(number + \" \"); &#125; // toArray用法 // 第一种方式(最常用) Integer[] integer = arrayList.toArray(new Integer[0]); // 第二种方式(容易理解) Integer[] integer1 = new Integer[arrayList.size()]; arrayList.toArray(integer1); // 抛出异常，java不支持向下转型 //Integer[] integer2 = new Integer[arrayList.size()]; //integer2 = arrayList.toArray(); System.out.println(); // 在指定位置添加元素 arrayList.add(2,2); // 删除指定位置上的元素 arrayList.remove(2); // 删除指定元素 arrayList.remove((Object)3); // 判断arrayList是否包含5 System.out.println(\"ArrayList contains 5 is: \" + arrayList.contains(5)); // 清空ArrayList arrayList.clear(); // 判断ArrayList是否为空 System.out.println(\"ArrayList is empty: \" + arrayList.isEmpty()); &#125;&#125; 查看原文","categories":[],"tags":[{"name":"java","slug":"java","permalink":"https://ydstudios.gitee.io/tags/java/"},{"name":"arrayList","slug":"arrayList","permalink":"https://ydstudios.gitee.io/tags/arrayList/"}]},{"title":"使用Nginx做反向代理,设置请求返回时带上被代理机器的信息","slug":"使用Nginx做反向代理-设置请求返回时带上被代理机器的信息","date":"2018-11-08T23:16:02.000Z","updated":"2019-12-09T15:03:26.000Z","comments":true,"path":"post/8f4c3efa.html","link":"","permalink":"https://ydstudios.gitee.io/post/8f4c3efa.html","excerpt":"现在的大家经常使用Nginx做代理，例如用Nginx去代理Node。如果代理的Node过多，Node一旦出现问题我们怎么知道到底是哪个出了问题呢？于是就有了今天的文章，我们可以设置请求返回时带上被代理机器的一些信息。","text":"现在的大家经常使用Nginx做代理，例如用Nginx去代理Node。如果代理的Node过多，Node一旦出现问题我们怎么知道到底是哪个出了问题呢？于是就有了今天的文章，我们可以设置请求返回时带上被代理机器的一些信息。 Nginx的配置123456789101112131415161718192021222324upstream usa &#123; server 127.0.0.1:3001; &#125;server &#123; listen 80 ; server_name xxx.com ; error_log /var/log/nginx/tianxingusa_error.log error; access_log /var/log/nginx/tianxingusa_accss.log main; location / &#123; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header Host $http_host; proxy_set_header X-Nginx-Proxy true; add_header Proxy-Node $upstream_addr; add_header Proxy-Status $upstream_status; proxy_http_version 1.1; proxy_pass http://usa; &#125; #省略部分信息&#125; 上面配置中12add_header Proxy-Node $upstream_addr;add_header Proxy-Status $upstream_status; 设置了Proxy-Node和Proxy-Status两个header，Proxy-Node显示的是被代理的节点，Proxy-Status显示的是被代理节点的状态，配置好之后重载Nginx的配置文件。我们就可以在 Response Headers 中看到相关的信息：12345678910111213HTTP/1.1 200 OKServer: nginxDate: Fri, 09 Nov 2018 03:07:55 GMTContent-Type: text/html; charset=UTF-8Content-Length: 1227Connection: keep-aliveX-Powered-By: ExpressAccept-Ranges: bytesCache-Control: public, max-age=0Last-Modified: Fri, 26 Oct 2018 10:11:49 GMTETag: W/\"4cb-166afdbcd67\"Proxy-Node: 127.0.0.1:3001Proxy-Status: 200","categories":[],"tags":[{"name":"nginx","slug":"nginx","permalink":"https://ydstudios.gitee.io/tags/nginx/"},{"name":"proxy","slug":"proxy","permalink":"https://ydstudios.gitee.io/tags/proxy/"}]},{"title":"Spring Boot 1.x 中整合Sharding-JDBC实现读写分离","slug":"Spring-Boot-1-x-中整合Sharding-JDBC实现读写分离","date":"2018-10-06T23:14:30.000Z","updated":"2019-12-09T15:03:26.000Z","comments":true,"path":"post/6d3d362b.html","link":"","permalink":"https://ydstudios.gitee.io/post/6d3d362b.html","excerpt":"国庆期间研究了数据库中间件例如Mycat和Sharding JDBC，Sharding-jdbc和Mycat使用不同的理念，Sharding-jdbc目前是基于Jdbc驱动，无需额外的proxy，因此也无需关注proxy本身的高可用。Mycat 是基于 Proxy，它复写了 MySQL 协议，将 Mycat Server 伪装成一个 MySQL 数据库，而 Sharding-JDBC 是基于 JDBC 接口的扩展，是以jar包的形式提供轻量级服务的。","text":"国庆期间研究了数据库中间件例如Mycat和Sharding JDBC，Sharding-jdbc和Mycat使用不同的理念，Sharding-jdbc目前是基于Jdbc驱动，无需额外的proxy，因此也无需关注proxy本身的高可用。Mycat 是基于 Proxy，它复写了 MySQL 协议，将 Mycat Server 伪装成一个 MySQL 数据库，而 Sharding-JDBC 是基于 JDBC 接口的扩展，是以jar包的形式提供轻量级服务的。 先介绍一下整合过程中用到的主要软件的版本： 12Spring Boot: 1.5.8.RELEASESharding-JDBC: 2.0.3 注意： 实现读写分离我们自己要先配置好MySQL的主从复制，我自己已经配置好了一主两从。配置的步骤可以参考【实现MySQL主从复制】 pom.xml 依赖123456789101112131415161718192021222324252627282930313233&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;io.shardingjdbc&lt;/groupId&gt; &lt;artifactId&gt;sharding-jdbc-core-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;2.0.3&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;druid&lt;/artifactId&gt; &lt;version&gt;1.1.2&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.mybatis.spring.boot&lt;/groupId&gt; &lt;artifactId&gt;mybatis-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;1.3.2&lt;/version&gt; &lt;/dependency&gt;&lt;/dependencies&gt; application配置文件1234567891011121314151617181920212223242526272829303132333435363738server.port=8080sharding.jdbc.datasource.names=ds_master,ds_slave_1,ds_slave_2# 主数据源sharding.jdbc.datasource.ds_master.type=com.alibaba.druid.pool.DruidDataSourcesharding.jdbc.datasource.ds_master.driver-class-name=com.mysql.jdbc.Driversharding.jdbc.datasource.ds_master.url=jdbc:mysql://localhost:3306/db1?characterEncoding=utf-8sharding.jdbc.datasource.ds_master.username=rootsharding.jdbc.datasource.ds_master.password=123456sharding.jdbc.datasource.ds_master.maxPoolSize=20# 从数据源 ds_slave_1sharding.jdbc.datasource.ds_slave_1.type=com.alibaba.druid.pool.DruidDataSourcesharding.jdbc.datasource.ds_slave_1.driver-class-name=com.mysql.jdbc.Driversharding.jdbc.datasource.ds_slave_1.url=jdbc:mysql://localhost:3307/db1?characterEncoding=utf-8sharding.jdbc.datasource.ds_slave_1.username=rootsharding.jdbc.datasource.ds_slave_1.password=123456sharding.jdbc.datasource.ds_slave_1.maxPoolSize=20# 从数据源 ds_slave_2sharding.jdbc.datasource.ds_slave_2.type=com.alibaba.druid.pool.DruidDataSourcesharding.jdbc.datasource.ds_slave_2.driver-class-name=com.mysql.jdbc.Driversharding.jdbc.datasource.ds_slave_2.url=jdbc:mysql://localhost:3308/db1?characterEncoding=utf-8sharding.jdbc.datasource.ds_slave_2.username=rootsharding.jdbc.datasource.ds_slave_2.password=123456sharding.jdbc.datasource.ds_slave_2.maxPoolSize=20# 读写分离配置sharding.jdbc.config.masterslave.load-balance-algorithm-type=round_robinsharding.jdbc.config.masterslave.name=dataSourcesharding.jdbc.config.masterslave.master-data-source-name=ds_mastersharding.jdbc.config.masterslave.slave-data-source-names=ds_slave_1,ds_slave_2mybatis.config-location=classpath:mybatis-config.xmlmybatis.mapper-locations=classpath:mapper/*.xmlmybatis.typeAliasesPackage=net.ydstuio.shardingjdbc.repository sharding.jdbc.config.masterslave.load-balance-algorithm-type查询时的负载均衡算法，目前有2种算法，round_robin（轮询）和random（随机）。 算法接口是：io.shardingjdbc.core.api.algorithm.masterslave.MasterSlaveLoadBalanceAlgorithm。 实现类有RandomMasterSlaveLoadBalanceAlgorithm和RoundRobinMasterSlaveLoadBalanceAlgorithm。 sharding.jdbc.config.masterslave.master-data-source-name 主数据源名称。 sharding.jdbc.config.masterslave.slave-data-source-names 从数据源名称，多个用逗号隔开。 到此，Sharing-Jdbc的读写分离就已经就配置好了，看看是不是很简单。如果对数据的及时性要求很高，可以使用下面的代码，使得读取也落到主库上。 12// 强制路由主库HintManager.getInstance().setMasterRouteOnly();","categories":[],"tags":[{"name":"spring boot","slug":"spring-boot","permalink":"https://ydstudios.gitee.io/tags/spring-boot/"},{"name":"sharding-jdbc","slug":"sharding-jdbc","permalink":"https://ydstudios.gitee.io/tags/sharding-jdbc/"}]},{"title":"Spring Boot 1.5.8整合Dubbo","slug":"Spring-Boot-1-5-8整合Dubbo","date":"2018-10-06T23:06:57.000Z","updated":"2019-12-09T15:03:26.000Z","comments":true,"path":"post/53410e91.html","link":"","permalink":"https://ydstudios.gitee.io/post/53410e91.html","excerpt":"废话不多说，今天说说Spring Boot和Dubbo的整合,注册服务中心用的是Zookeeper，至于Dubbo、Zookeeper为何物我在此不再多言，不知道是什么自己去百度，本文适用于对微服务或者RPC了解的人准备的！","text":"废话不多说，今天说说Spring Boot和Dubbo的整合,注册服务中心用的是Zookeeper，至于Dubbo、Zookeeper为何物我在此不再多言，不知道是什么自己去百度，本文适用于对微服务或者RPC了解的人准备的！ 先介绍一下整合过程中用到的主要软件的版本： 1234Spring Boot: 1.5.8.RELEASEDubbo: dubbo-spring-boot-starter 0.1.1Zookeeper: 3.4.10dubbo-admin：2.5.4 其中dubbo-admin使用的是老版本的，新版本的大家可以到github上去搜索，这里给出这个版本的下载地址，我不保证任何时候都可以使用dubbo-admin-2.5.4Dubbo使用的是alibaba官方提供的starter ： 12345&lt;dependency&gt; &lt;groupId&gt;com.alibaba.boot&lt;/groupId&gt; &lt;artifactId&gt;dubbo-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;0.1.1&lt;/version&gt;&lt;/dependency&gt; 如果您的工程遇到了依赖问题, 请尝试添加如下 Maven 参考到工程的 pom.xml 文件中： 123456789101112&lt;repositories&gt; &lt;repository&gt; &lt;id&gt;sonatype-nexus-snapshots&lt;/id&gt; &lt;url&gt;https://oss.sonatype.org/content/repositories/snapshots&lt;/url&gt; &lt;releases&gt; &lt;enabled&gt;false&lt;/enabled&gt; &lt;/releases&gt; &lt;snapshots&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;/snapshots&gt; &lt;/repository&gt;&lt;/repositories&gt; 从现在开始, dubbo-spring-boot-project 将在每个发布中发行两个版本 : 0.2.x 是支持 Spring Boot 2.x 的主要版本（推荐，长期维护） 0.1.x 是支持 Spring Boot 1.x 的维护版本（兼容，短期维护）123版本 Java Spring Boot Dubbo0.2.0 1.8+ 2.0.x 2.6.2+0.1.1 1.7+ 1.5.x 2.6.2+ 以上牵涉到需要安装的软件，需要大家自行准备，本文的重点不在于此！友情提示,使用上面版本的dubbo-admin是个war包可以之前部署在Tomcat中，注意其中配置文件中的Zookeeper的地址，请修改成自己可用的Zookeeper地址！ 我之前写过一个基于Spring Boot和Spring Cloud的BT搜索项目，我主要是想用这个项目练手Spring Cloud，其中用了Zuul、Fegin、Ribbon、Spring Config、Hystrix、Eureka等组件。下面开始正式的编码，Spring Boot和Dubbo的整合。其中问题有很多，浪费了我国庆假期中的一天，其实最后发现问题解决很简单。 ####由服务提供方为服务消费方暴露接口 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253package net.ydstudio.dubbo.search.api.service;import net.ydstudio.dubbo.search.api.document.BtSearchDocument;import net.ydstudio.dubbo.search.api.model.EsSearchHotResult;import java.io.IOException;/** * Created by Sam on 18/7/7. */public interface BtSearchService &#123; /** * 创建索引 * @return * @throws IOException */ Boolean createIndex() throws IOException; /** * 通过word查询 文档 * @param word * @return * @throws IOException */ BtSearchDocument findOne(String word) throws IOException; /** * 根据id删除文档 * @param word * @throws IOException * @return */ Boolean delete(String word) throws IOException; /** * 保存 document * @param word * @throws IOException * @return true or false */ Boolean save(String word) throws IOException; /** * 删除索引 * @return * @throws IOException */ Boolean deleteIndex() throws IOException; /** * 热搜词 * @return */ EsSearchHotResult hotWordList();&#125; ####服务提供方 pom.xml中的依赖123456789101112131415161718192021222324 &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;net.ydstudio.dubbo&lt;/groupId&gt; &lt;artifactId&gt;search-api&lt;/artifactId&gt; &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.alibaba.boot&lt;/groupId&gt; &lt;artifactId&gt;dubbo-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;0.1.1&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt;&lt;/project&gt; 实现暴露的接口123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139package net.ydstudio.dubbo.search.service;import com.alibaba.dubbo.config.annotation.Service;import io.searchbox.client.JestClient;import io.searchbox.client.JestResult;import io.searchbox.core.Search;import net.ydstudio.dubbo.search.api.document.BtSearchDocument;import net.ydstudio.dubbo.search.api.model.EsSearchHotResult;import net.ydstudio.dubbo.search.api.service.BtSearchService;import net.ydstudio.dubbo.search.util.Md5Util;import org.elasticsearch.index.query.BoolQueryBuilder;import org.elasticsearch.index.query.QueryBuilder;import org.elasticsearch.index.query.QueryBuilders;import org.elasticsearch.search.builder.SearchSourceBuilder;import org.elasticsearch.search.sort.FieldSortBuilder;import org.elasticsearch.search.sort.SortBuilders;import org.elasticsearch.search.sort.SortOrder;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.beans.factory.annotation.Value;import org.springframework.core.io.Resource;import org.springframework.util.StringUtils;import java.io.IOException;import java.text.SimpleDateFormat;import java.util.Date;import java.util.List;/** * Created by Sam on 18/7/7. */@Servicepublic class BtSearchServiceImpl implements BtSearchService,EsSearchService&lt;BtSearchDocument&gt; &#123; /** * 索引 */ public static final String INDEX_NAME = \"search\"; /** * 索引类型 */ public static final String TYPE = \"searchWord\"; @Value(\"classpath:mapping/searchMapping.json\") private Resource mappingFile; @Autowired private JestClient jestClient; @Override public Boolean createIndex() throws IOException &#123; return createESIndex(jestClient, mappingFile, INDEX_NAME, TYPE); &#125; @Override public BtSearchDocument findOne(String word) throws IOException &#123; String id = Md5Util.md5(word); return findDocument(jestClient, INDEX_NAME, TYPE, id, BtSearchDocument.class); &#125; @Override public Boolean delete(String word) throws IOException &#123; String id = Md5Util.md5(word); return deleteDocument(jestClient, INDEX_NAME, TYPE, id); &#125; @Override public Boolean save(String word) throws IOException &#123; String id = Md5Util.md5(word); BtSearchDocument document = this.findOne(id); SimpleDateFormat format = new SimpleDateFormat(\"yyyy-MM-dd HH:mm:ss\"); Date date = new Date(); if (StringUtils.isEmpty(document))&#123; document = new BtSearchDocument(); document.setId(id); document.setCreated(format.format(date)); document.setNum(1); document.setWord(word); &#125;else &#123; document.setNum(document.getNum()+1); &#125; document.setUpdated(format.format(date)); return saveDocument(jestClient, INDEX_NAME, TYPE, id, document); &#125; @Override public Boolean deleteIndex() throws IOException &#123; return deleteESIndex(jestClient, INDEX_NAME); &#125; @Override public EsSearchHotResult hotWordList() &#123; // match_all BoolQueryBuilder boolQuery = QueryBuilders.boolQuery(); SearchSourceBuilder searchSourceBuilder = new SearchSourceBuilder(); QueryBuilder queryBuilder = QueryBuilders.matchAllQuery(); boolQuery.must(queryBuilder); FieldSortBuilder sortBuilder = SortBuilders.fieldSort(\"num\"); sortBuilder.order(SortOrder.DESC); searchSourceBuilder.query(boolQuery); searchSourceBuilder.sort(sortBuilder); // 构建Search对象 Search search = new Search.Builder(searchSourceBuilder.toString()) .addIndex(INDEX_NAME) .addType(TYPE) .build(); EsSearchHotResult esSearchHotResult = new EsSearchHotResult(); try &#123; JestResult result = jestClient.execute(search); List&lt;BtSearchDocument&gt; list = result.getSourceAsObjectList(BtSearchDocument.class); esSearchHotResult.setTook(result.getValue(\"took\").toString()); esSearchHotResult.setDocumentList(list); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; return esSearchHotResult; &#125;&#125; search服务提供方的application的配置信息： 123456789101112131415161718dubbo: application: id: search-provider name: search-provider protocosl: id: dubbo name: dubbo port: 20880 scan: basePackages: net.ydstudio.dubbo.search.service registry: id: my-registry address: zookeeper://xx.xx.xxx.xxx:xxx parameters: max-active: 8 max-wait: -1 max-idle: 8 min-idle: 0 注意 basePackages项配置的包路径，他对应的是设置扫描路径即被注解@service和@Reference描述的接口（或者说是暴露接口的实现类的包路径，这个很重要，我在此处耽搁了很久）,这个配置可以使用 @EnableDubbo 注解替代（加在启动类上） ####服务消费方#####pom.xml中的依赖 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;net.ydstudio.dubbo&lt;/groupId&gt; &lt;artifactId&gt;bt-model&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;net.ydstudio.dubbo&lt;/groupId&gt; &lt;artifactId&gt;search-api&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.alibaba.boot&lt;/groupId&gt; &lt;artifactId&gt;dubbo-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;0.1.1&lt;/version&gt; &lt;/dependency&gt; &lt;!--thymeleaf--&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-thymeleaf&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;net.sourceforge.nekohtml&lt;/groupId&gt; &lt;artifactId&gt;nekohtml&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-jdbc&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;scope&gt;runtime&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;druid&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.mybatis.spring.boot&lt;/groupId&gt; &lt;artifactId&gt;mybatis-spring-boot-starter&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.mybatis.generator&lt;/groupId&gt; &lt;artifactId&gt;mybatis-generator-core&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.mybatis.generator&lt;/groupId&gt; &lt;artifactId&gt;mybatis-generator&lt;/artifactId&gt; &lt;type&gt;pom&lt;/type&gt; &lt;/dependency&gt; &lt;/dependencies&gt; 配置文件123456789101112131415161718dubbo: application: id: bt-search name: bt-search protocosl: id: dubbo name: dubbo port: 20880 scan: basePackages: net.ydstudio.dubbo.bt registry: id: my-registry address: zookeeper://xx.xx.xxx.xxx:xxx parameters: max-active: 8 max-wait: -1 max-idle: 8 min-idle: 0 注意 basePackages 设置扫描路径,扫描被注解@service和@Reference的接口 RPC调用12@Referenceprivate BtSearchService btSearchService; 使用com.alibaba.dubbo.config.annotation.Reference中的 @Reference注解修饰接口BtSearchService ，然后像普通的bean使用即可。 整合总结由于软件更新的快，或者软件项目的生存周期太短，软件对应的说明文档会跟不上。Dubbo现在已经捐献给了Apache，这一方面可能会好一点。大家在开发的时候注意细心一点，多去看看GitHub，不然问题会很多！ 附注参考资料【官方Dubbo和Spring Boot整合的GitHub地址】","categories":[],"tags":[{"name":"dubbo","slug":"dubbo","permalink":"https://ydstudios.gitee.io/tags/dubbo/"},{"name":"spring boot","slug":"spring-boot","permalink":"https://ydstudios.gitee.io/tags/spring-boot/"}]},{"title":"Mycat 数据库分库分表中间件的分库配置","slug":"Mycat-数据库分库分表中间件的分库配置","date":"2018-10-03T23:04:44.000Z","updated":"2019-12-09T15:03:26.000Z","comments":true,"path":"post/dcd836f7.html","link":"","permalink":"https://ydstudios.gitee.io/post/dcd836f7.html","excerpt":"本文是Mycat 数据库分库分表中间件系列文章的第三篇，平时工作太忙，加上又忙着从PHP转Java，平日的空闲时间都去研究Java了。什么Spring MVC、Spring Boot、Spring Cloud、Dubbo，东西真尼玛的多！正好国庆一人没事干，再次拾起来Mycat的研究。","text":"本文是Mycat 数据库分库分表中间件系列文章的第三篇，平时工作太忙，加上又忙着从PHP转Java，平日的空闲时间都去研究Java了。什么Spring MVC、Spring Boot、Spring Cloud、Dubbo，东西真尼玛的多！正好国庆一人没事干，再次拾起来Mycat的研究。 首先，先按照 实现MySQL主从复制 上的步骤配置一个简单的MySQL主从环境，先贴一下相关的配置信息：MySQL中Master的配置： 123456789101112131415161718192021[client]port=3306[mysql]default-character-set=utf8[mysqld]port=3306basedir=\"C:/phpStudy/MySQL/\"datadir=\"C:/phpStudy/MySQL/data/\"character-set-server=utf8collation-server=utf8_general_ci default-storage-engine=MyISAM### addserver-id=1log-bin =mysql-binbinlog-format = mixedbinlog-do-db=db1 #此参数表示只记录指定数据库的二进制日志。binlog-do-db=db2binlog-do-db=db3#binlog-ignore-db=api,mysql,performance_schema #此参数表示忽略指定的数据库的二进制日志。#其他配置省略 MySQL中slave的配置： 12345678910111213141516171819[client]port=3307[mysql]default-character-set=utf8[mysqld]port=3307basedir=\"C:/phpStudy/MySQL3307/\"datadir=\"C:/phpStudy/MySQL3307/data/\"character-set-server=utf8collation-server=utf8_general_ci default-storage-engine=MyISAMserver-id=3307log-bin = mysql-binbinlog-format =mixedrelay-log=mysql-relayslave-skip-errors=all #跳过所有错误#其他配置省略 在master上创建用于同步数据的账户： 123grant replication client,replication slave on *.* to slave3307@'%' identified by 'slave3307';#刷新权限，立即生效flush privileges; 在Master执行命令查看log_pos和log_file 1234567show master status; +------------------+----------+--------------+------------------+ | File | Position | Binlog_Do_DB | Binlog_Ignore_DB | +------------------+----------+--------------+------------------+ | mysql-bin.000062 | 8233| | | +------------------+----------+--------------+------------------+ 注意 执行完此步骤后不要再操作主服务器MYSQL，防止主服务器状态值变化 在slave上执行下面的命令，讲master和slave关联起来： 12345hange master to master_host='127.0.0.1',master_port=3306,master_user='slave3307',master_password='slave3307',master_log_file='mysql-bin.000062',master_log_pos=8233;start slave #启动从服务器复制功能show slave status\\G 其中 Slave_IO_Running 和 Slave_SQL_Running 两列的值都为 “Yes”，表明 Slave 的 I/O 和 SQL 线程都在正常运行。正常的表现如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758mysql&gt; show slave status\\G*************************** 1. row *************************** Slave_IO_State: Waiting for master to send event Master_Host: 127.0.0.1 Master_User: slave3307 Master_Port: 3306 Connect_Retry: 60 Master_Log_File: mysql-bin.000007 Read_Master_Log_Pos: 24536 Relay_Log_File: mysql-relay.000005 Relay_Log_Pos: 24699 Relay_Master_Log_File: mysql-bin.000007 Slave_IO_Running: Yes Slave_SQL_Running: Yes Replicate_Do_DB: Replicate_Ignore_DB: Replicate_Do_Table: Replicate_Ignore_Table: Replicate_Wild_Do_Table: Replicate_Wild_Ignore_Table: Last_Errno: 0 Last_Error: Skip_Counter: 0 Exec_Master_Log_Pos: 24536 Relay_Log_Space: 25318 Until_Condition: None Until_Log_File: Until_Log_Pos: 0 Master_SSL_Allowed: No Master_SSL_CA_File: Master_SSL_CA_Path: Master_SSL_Cert: Master_SSL_Cipher: Master_SSL_Key: Seconds_Behind_Master: 0Master_SSL_Verify_Server_Cert: No Last_IO_Errno: 0 Last_IO_Error: Last_SQL_Errno: 0 Last_SQL_Error: Replicate_Ignore_Server_Ids: Master_Server_Id: 1 Master_UUID: fe1b230d-6ee9-11e8-b0c6-9c5c8e103115 Master_Info_File: C:\\phpStudy\\MySQL3307\\data\\master.info SQL_Delay: 0 SQL_Remaining_Delay: NULL Slave_SQL_Running_State: Slave has read all relay log; waiting for the slave I/O thread to update it Master_Retry_Count: 86400 Master_Bind: Last_IO_Error_Timestamp: Last_SQL_Error_Timestamp: Master_SSL_Crl: Master_SSL_Crlpath: Retrieved_Gtid_Set: Executed_Gtid_Set: Auto_Position: 01 row in set (0.00 sec) 友情提示：如果是直接复制MySQL的安装目录配置的主从，注意MySQL5.6开始的UUID的问题。下面开始Mycat的配置，Mycat中有三个重要的配置文件：server.xml、schema.xml、rule.xml，下面就分别贴一下对应的配置信息：server.xml中的配置信息： 1234567891011121314151617181920212223242526272829303132333435363738394041424344&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;!DOCTYPE mycat:server SYSTEM \"server.dtd\"&gt;&lt;mycat:server xmlns:mycat=\"http://io.mycat/\"&gt; &lt;system&gt; &lt;property name=\"useSqlStat\"&gt;0&lt;/property&gt; &lt;!-- 1为开启实时统计、0为关闭 --&gt; &lt;property name=\"useGlobleTableCheck\"&gt;0&lt;/property&gt; &lt;!-- 1为开启全加班一致性检测、0为关闭 --&gt; &lt;property name=\"sequnceHandlerType\"&gt;2&lt;/property&gt; &lt;!--默认为type 0: DirectByteBufferPool | type 1 ByteBufferArena--&gt; &lt;property name=\"processorBufferPoolType\"&gt;0&lt;/property&gt; &lt;!--分布式事务开关，0为不过滤分布式事务，1为过滤分布式事务（如果分布式事务内只涉及全局表，则不过滤），2为不过滤分布式事务,但是记录分布式事务日志--&gt; &lt;property name=\"handleDistributedTransactions\"&gt;0&lt;/property&gt; &lt;!--off heap for merge/order/group/limit 1开启 0关闭--&gt; &lt;property name=\"useOffHeapForMerge\"&gt;1&lt;/property&gt; &lt;!--单位为m--&gt; &lt;property name=\"memoryPageSize\"&gt;1m&lt;/property&gt; &lt;!--单位为k--&gt; &lt;property name=\"spillsFileBufferSize\"&gt;1k&lt;/property&gt; &lt;property name=\"useStreamOutput\"&gt;0&lt;/property&gt; &lt;!--单位为m--&gt; &lt;property name=\"systemReserveMemorySize\"&gt;384m&lt;/property&gt; &lt;!--是否采用zookeeper协调切换 --&gt; &lt;property name=\"useZKSwitch\"&gt;true&lt;/property&gt; &lt;/system&gt; &lt;user name=\"root\"&gt; &lt;property name=\"password\"&gt;mycat&lt;/property&gt; &lt;!--多个数据库逗号隔开 --&gt; &lt;property name=\"schemas\"&gt;testdb&lt;/property&gt; &lt;/user&gt; &lt;user name=\"mycatread\"&gt; &lt;property name=\"password\"&gt;mycatread&lt;/property&gt; &lt;property name=\"schemas\"&gt;testdb&lt;/property&gt; &lt;property name=\"readOnly\"&gt;true&lt;/property&gt; &lt;/user&gt;&lt;/mycat:server&gt; schema.xml中的配置信息： 12345678910111213141516171819202122&lt;?xml version=\"1.0\"?&gt;&lt;!DOCTYPE mycat:schema SYSTEM \"schema.dtd\"&gt;&lt;mycat:schema xmlns:mycat=\"http://io.mycat/\"&gt; &lt;schema name=\"testdb\" checkSQLschema=\"false\" sqlMaxLimit=\"100\"&gt; &lt;table name=\"t_user\" dataNode=\"dn1,dn2,dn3\" rule=\"crc32slot\"&gt; &lt;!-- 父表为t_user，子表为t_admin。t_admin表中的字段user_id引用t_user表中的id主键字段。 --&gt; &lt;childTable name=\"t_admin\" joinKey=\"user_id\" parentKey=\"id\" /&gt; &lt;/table&gt; &lt;/schema&gt; &lt;dataNode name=\"dn1\" dataHost=\"localhost1\" database=\"db1\" /&gt; &lt;dataNode name=\"dn2\" dataHost=\"localhost1\" database=\"db2\" /&gt; &lt;dataNode name=\"dn3\" dataHost=\"localhost1\" database=\"db3\" /&gt; &lt;dataHost name=\"localhost1\" maxCon=\"1000\" minCon=\"10\" balance=\"1\" writeType=\"0\" dbType=\"mysql\" dbDriver=\"native\" switchType=\"2\" slaveThreshold=\"100\"&gt; &lt;heartbeat&gt;show slave status&lt;/heartbeat&gt; &lt;writeHost host=\"hostM1\" url=\"127.0.0.1:3306\" user=\"root\" password=\"123456\"&gt; &lt;!--&lt;readHost host=\"hostS1\" url=\"127.0.0.1:3307\" user=\"root\" password=\"123456\"/&gt;--&gt; &lt;/writeHost&gt; &lt;/dataHost&gt;&lt;/mycat:schema&gt; rule.xml中的配置信息： 1234&lt;function name=\"crc32slot\" class=\"io.mycat.route.function.PartitionByCRC32PreSlot\"&gt; &lt;property name=\"count\"&gt;3&lt;/property&gt;&lt;!-- 要分片的数据库节点数量，必须指定，否则没法分片 --&gt;&lt;/function&gt; 注意下面的地方： 12345678&lt;!--schema.xml中--&gt;&lt;table name=\"t_user\" dataNode=\"dn1,dn2,dn3\" rule=\"crc32slot\"&gt;&lt;!--rule.xml中--&gt;&lt;function name=\"crc32slot\" class=\"io.mycat.route.function.PartitionByCRC32PreSlot\"&gt; &lt;property name=\"count\"&gt;3&lt;/property&gt;&lt;!-- 要分片的数据库节点数量，必须指定，否则没法分片 --&gt; &lt;/function&gt; 使用crc32lot进行分片，分片的数据库节点的数量默认给了2，如果你修改了这里，请一定要删除conf/ruledata/crc32slot_T_USER.properties这个文件，并重新启动Mycat，不然你的数据是不会分到db3这个数据库节点的。到这里Mycat分库配置就完成了，你只需要在Mycat的管理上操作，就能在master、slave上看到效果： 123456789101112131415161718192021222324252627mysql&gt;mysql -uroot -pmycat -P8066Warning: Using a password on the command line interface can be insecure.Welcome to the MySQL monitor. Commands end with ; or \\g.Your MySQL connection id is 3Server version: 5.6.29-mycat-1.6-RELEASE-20161028204710 MyCat Server (OpenClounCopyright (c) 2000, 2018, Oracle and/or its affiliates. All rights reserved.Oracle is a registered trademark of Oracle Corporation and/or itsaffiliates. Other names may be trademarks of their respectiveowners.Type 'help;' or '\\h' for help. Type '\\c' to clear the current input statement.mysql&gt; insert into t_user(id, name) values(1, 'aaa');Query OK, 1 row affected (0.02 sec)mysql&gt; insert into t_user(id, name) values(2, 'bbb');Query OK, 1 row affected (0.00 sec)mysql&gt; insert into t_user(id, name) values(3, 'ccc');Query OK, 1 row affected (0.00 sec)mysql&gt; insert into t_user(id, name) values(4, 'ddd');Query OK, 1 row affected (0.00 sec)mysql&gt; insert into t_user(id, name) values(5, 'eee');Query OK, 1 row affected (0.00 sec) 在master上看效果： 1234567891011121314151617181920212223242526272829303132333435363738394041C:\\Users\\nick&gt;mysql -uroot -p123456Warning: Using a password on the command line interface can be insecure.Welcome to the MySQL monitor. Commands end with ; or \\g.Your MySQL connection id is 10628Server version: 5.6.40-log MySQL Community Server (GPL)Copyright (c) 2000, 2018, Oracle and/or its affiliates. All rights reserved.Oracle is a registered trademark of Oracle Corporation and/or itsaffiliates. Other names may be trademarks of their respectiveowners.Type 'help;' or '\\h' for help. Type '\\c' to clear the current input statement.mysql&gt; select * from db1.t_user;+----+------+-------+| id | name | _slot |+----+------+-------+| 3 | ccc | 32411 || 5 | eee | 27566 |+----+------+-------+2 rows in set (0.00 sec)mysql&gt; select * from db2.t_user;+----+------+-------+| id | name | _slot |+----+------+-------+| 1 | aaa | 44983 || 2 | bbb | 65037 |+----+------+-------+2 rows in set (0.00 sec)mysql&gt; select * from db3.t_user;+----+------+-------+| id | name | _slot |+----+------+-------+| 4 | ddd | 68408 |+----+------+-------+1 row in set (0.00 sec) 在slave上查看数据也是如此，此处就贴出部分数据： 1234567891011121314151617181920212223C:\\Users\\nick&gt;mysql -uroot -p -P3307Enter password: ******Welcome to the MySQL monitor. Commands end with ; or \\g.Your MySQL connection id is 10412Server version: 5.6.40-log MySQL Community Server (GPL)Copyright (c) 2000, 2018, Oracle and/or its affiliates. All rights reserved.Oracle is a registered trademark of Oracle Corporation and/or itsaffiliates. Other names may be trademarks of their respectiveowners.Type 'help;' or '\\h' for help. Type '\\c' to clear the current input statement.mysql&gt; select * from db1.t_user;+----+------+-------+| id | name | _slot |+----+------+-------+| 3 | ccc | 32411 || 5 | eee | 27566 |+----+------+-------+2 rows in set (0.00 sec)","categories":[],"tags":[{"name":"linux","slug":"linux","permalink":"https://ydstudios.gitee.io/tags/linux/"},{"name":"mycat","slug":"mycat","permalink":"https://ydstudios.gitee.io/tags/mycat/"}]},{"title":"Feign调用返回复杂对象时报错java.lang.ClassCastException: java.util.LinkedHashMap cannot be cast to xxx","slug":"Feign调用返回复杂对象时报错java-lang-ClassCastException-java-util-LinkedHashMap-cannot-be-cast-to-xxx","date":"2018-09-30T23:39:50.000Z","updated":"2019-12-09T15:03:26.000Z","comments":true,"path":"post/4f6a533b.html","link":"","permalink":"https://ydstudios.gitee.io/post/4f6a533b.html","excerpt":"在公司的数据中心项目里使用了Feign，进行接口的调用，在返回一个复杂的对象时候，出现下面的一个错误： 1java.lang.ClassCastException: java.util.LinkedHashMap cannot be cast to xxx","text":"在公司的数据中心项目里使用了Feign，进行接口的调用，在返回一个复杂的对象时候，出现下面的一个错误： 1java.lang.ClassCastException: java.util.LinkedHashMap cannot be cast to xxx 大致意思就是 LinkedHashMap 不能够转换成 xxx 对象，我很奇怪！因为这个接口相关的地方都没有使用到 LinkedHashMap，他是从哪里冒出来的？我开始调试下面的代码： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546@Overridepublic List&lt;EsFilterKV&gt; getFilterKV(Integer companyId, Integer appId, Long usrId, String filterName) &#123; if (StringUtils.isEmpty(filterName))&#123; return null; &#125; List&lt;CompanyDistributionFilterGroup&gt; distributionFilterGroupList = companyDistributionService.findDistributionFilterGroupList(companyId,true); List&lt;ProductDocument&gt; productDocumentList = new ArrayList&lt;ProductDocument&gt;(100); try &#123; for (CompanyDistributionFilterGroup group: distributionFilterGroupList) &#123; List&lt;FilterItem&gt; filterItemList = filterGroupFilterService.findFilterItemList(group.getFilterGroupId()); MtSearchFilterRequest request = new MtSearchFilterRequest(); request.setFilterItemList(filterItemList); MtSearchResult mtSearchResult = productSearchService.search(request); productDocumentList.addAll(mtSearchResult.getData()); &#125; &#125; catch (MtException e) &#123; e.printStackTrace(); &#125; List&lt;Long&gt; longList = productDocumentList.stream().map(p-&gt;p.getId()).distinct().collect(Collectors.toList()); if (CollectionUtils.isEmpty(longList))&#123; return null; &#125; List&lt;Map&gt; idValueList = productPackageService.findPackageIdValueList(longList,filterName,true); if (CollectionUtils.isEmpty(idValueList))&#123; return null; &#125; List&lt;EsFilterKV&gt; esFilterKVs = new ArrayList&lt;EsFilterKV&gt;(idValueList.size()); for (Map map: idValueList)&#123; EsFilterKV esFilterKV = new EsFilterKV(); esFilterKV.setName(map.get(\"name\").toString()); esFilterKV.setId(new Long(map.get(\"id\").toString())); esFilterKV.setJsonName(\"&#123;'id':\"+esFilterKV.getId()+\",'name':'\"+esFilterKV.getName()+\"'&#125;\"); esFilterKVs.add(esFilterKV); &#125; return esFilterKVs;&#125; 我发现在 productDocumentList.addAll(mtSearchResult.getData()); 这一行，productDocumentList里面放的全是LinkedHashMap，不是想象中的对象列表。通过面向百度编程，我把productSearchService.search()这个方法返回的泛型给去掉了，指定了真实的类型，从而解决了上面的问题！ 通过百度得知有不少人遇到这种问题，有个感觉比较靠谱的解释如下： 因为rpc远程调用在底层还是使用的HTTPClient，所以在传递参数的时候，必定要有个顺序，当你传递map的时候map里面的值也要有顺序，不然服务层在接的时候就出问题了，所以它才会从map转为linkedhashMap！spring 有一个类叫ModelMap，继承了linkedhashMap public class ModelMap extends LinkedHashMap ,所以一个接口返回的结果就可以直接用ModelMap来接，注意ModelMap是没有泛型的，不管你返回的结果是什么类型的map，泛型是多复杂的map，都可以直接new一个Modelmap，用它来接返回的结果！！！","categories":[],"tags":[{"name":"java","slug":"java","permalink":"https://ydstudios.gitee.io/tags/java/"},{"name":"feign","slug":"feign","permalink":"https://ydstudios.gitee.io/tags/feign/"}]},{"title":"php-fpm的进程池相关参数说明","slug":"php-fpm的进程池相关参数说明","date":"2018-09-17T22:59:19.000Z","updated":"2019-12-09T15:03:26.000Z","comments":true,"path":"post/c1dfde53.html","link":"","permalink":"https://ydstudios.gitee.io/post/c1dfde53.html","excerpt":"最近公司的官网据业务说时不时的卡，而负责维护官网的人请假去德国跑马拉松了（ps:有钱真好），我又苦逼的帮他处理官网的事情。 php-fpm进程池开启进程有两种方式，一种是static，直接开启指定数量的php-fpm进程，不再增加或者减少；另一种则是dynamic，开始时开启一定数量的php-fpm进程，当请求量变大时，动态的增加php-fpm进程数到上限，当空闲时自动释放空闲的进程数到一个下限。这两种不同的执行方式，可以根据服务器的实际需求来进行调整。","text":"最近公司的官网据业务说时不时的卡，而负责维护官网的人请假去德国跑马拉松了（ps:有钱真好），我又苦逼的帮他处理官网的事情。 php-fpm进程池开启进程有两种方式，一种是static，直接开启指定数量的php-fpm进程，不再增加或者减少；另一种则是dynamic，开始时开启一定数量的php-fpm进程，当请求量变大时，动态的增加php-fpm进程数到上限，当空闲时自动释放空闲的进程数到一个下限。这两种不同的执行方式，可以根据服务器的实际需求来进行调整。 其中涉及到的一些参数，分别是pm、pm.max_children、pm.start_servers、pm.min_spare_servers和pm.max_spare_servers。pm表示使用那种方式，有两个值可以选择，就是static（静态）或者dynamic（动态）。 下面4个参数的意思分别为： 1234pm.max_children：静态方式下开启的php-fpm进程数量，在动态方式下他限定php-fpm的最大进程数（这里要注意pm.max_spare_servers的值只能小于等于pm.max_children）pm.start_servers：动态方式下的起始php-fpm进程数量。pm.min_spare_servers：动态方式空闲状态下的最小php-fpm进程数量。pm.max_spare_servers：动态方式空闲状态下的最大php-fpm进程数量。 php的配置文件里面给出了pm.start_servers的计算公式：min_spare_servers + (max_spare_servers - min_spare_servers) / 2 如果dm设置为static，那么其实只有pm.max_children这个参数生效。系统会开启参数设置数量的php-fpm进程。 如果dm设置为dynamic，4个参数都生效。系统会在php-fpm运行开始时启动pm.start_servers个php-fpm进程，然后根据系统的需求动态在pm.min_spare_servers和pm.max_spare_servers之间调整php-fpm进程数。","categories":[],"tags":[{"name":"php","slug":"php","permalink":"https://ydstudios.gitee.io/tags/php/"},{"name":"php-fpm","slug":"php-fpm","permalink":"https://ydstudios.gitee.io/tags/php-fpm/"}]},{"title":"Spring Cloud Config客户端报错Could not locate PropertySource: I/O error on GET request xxx","slug":"Spring-Cloud-Config客户端报错Could-not-locate-PropertySource-I-O-error-on-GET-request-xxx","date":"2018-09-15T23:46:48.000Z","updated":"2019-12-09T15:03:26.000Z","comments":true,"path":"post/29fad61b.html","link":"","permalink":"https://ydstudios.gitee.io/post/29fad61b.html","excerpt":"今天在研究Spring Cloud Config配置中心，出了一个很让人恼火的问题，我先贴一下我的相关配置信息：","text":"今天在研究Spring Cloud Config配置中心，出了一个很让人恼火的问题，我先贴一下我的相关配置信息： 客户端的bootstrap.yml配置： 123456789101112131415161718192021server: port: 9009spring: application: name: admin-server profiles: active: dev cloud: config: name: admin-server-config label: master profile: dev discovery: enabled: true service-id: config-servereureka: client: service-url: defaultZone: http://Eurexxxr:xxxx-12xxx56@accp.xxx.net:8786/eureka/ 出现的问题如下： 1Could not locate PropertySource: I/O error on GET request for \"http://config-server:9008/admin-server-config/dev/master\": Connection refused; nested exception is java.net.ConnectException: Connection refused 大致的意思是说”http://config-server:9008/admin-server-config/dev/master&quot;这个链接不能够访问，可我试着访问一下，却是可以访问的，这就让我百思不得其解！于是乎百度google走起，在stackoverflow上倒是找到几条相关的问题，但是问题的解答方案都不是很靠谱！于是乎放弃答案的寻找，我就去菜市场买菜，毕竟晚饭还是要吃的!酒足饭饱之后我又来研究这个问题。按理说提示这种链接不能访问，就是hosts文件映射的问题（ps：配置中心config-server我部署在我阿里云博客上），可是那个链接确实可以直接访问的…………………………………………我就试着看看我的hosts文件，第一次我还没有看出问题，我不死心的又看第二次，总算让我发现了问题的所在，下面是当时hosts文件的内容： 123456789101112131415161718192021# Host Database## localhost is used to configure the loopback interface# when the system is booting. Do not change this entry.##127.0.0.1 localhostxx.78.xx.12 config-server127.0.0.1 dev127.0.0.1 search-provider127.0.0.1 config-server127.0.0.1 work.ydstudios.com127.0.0.1 work.damei.com127.0.0.1 work.gx.com127.0.0.1 work.phpcms.com127.0.0.1 work.typecho.com127.0.0.1 work.seacms.com127.0.0.1 work.dy360.net127.0.0.1 work.btsearch.com127.0.0.1 work.search.com127.0.0.1 work.mm.com127.0.0.1 work.duowan.com 不知道大家看出来没有？有两个ip指向了config-server，而且我发现这种配置，不同时候起作用的配置还不一样！直接访问url地址的时候，第一个配置生效，程序里确实第二个配置生效，尴尬…………","categories":[],"tags":[{"name":"spring cloud","slug":"spring-cloud","permalink":"https://ydstudios.gitee.io/tags/spring-cloud/"},{"name":"spring cloud config","slug":"spring-cloud-config","permalink":"https://ydstudios.gitee.io/tags/spring-cloud-config/"}]},{"title":"Spring Cloud整合配置中心Eureka中的服务状态显示UNKOWN","slug":"Spring-Cloud整合配置中心Eureka中的服务状态显示UNKOWN","date":"2018-09-09T23:42:27.000Z","updated":"2019-12-09T15:03:26.000Z","comments":true,"path":"post/5e4a0e78.html","link":"","permalink":"https://ydstudios.gitee.io/post/5e4a0e78.html","excerpt":"今天在研究Spring Cloud的配置中心的时候，发现将应用的配置移到git上，启动应用后发现，在Euraka注册中心上这个应用的状态始终为UNKOWN，可是这个应用是可以提供服务的。下面是应用的bootstrap的配置：","text":"今天在研究Spring Cloud的配置中心的时候，发现将应用的配置移到git上，启动应用后发现，在Euraka注册中心上这个应用的状态始终为UNKOWN，可是这个应用是可以提供服务的。下面是应用的bootstrap的配置： 12345678910111213141516171819202122232425262728293031spring: application: name: search-provider cloud: config: name: search-config profile: dev label: master discovery: enabled: true service-id: config-servereureka: client: fetch-registry: true register-with-eureka: true healthcheck: enabled: true service-url: defaultZone: http://xxxx:xxxx@xxx.ydstudio.net:xxx/eureka/ instance: #instance-id默认值是主机名：应用名：应用端口 instance-id: $&#123;spring.application.name&#125;:$&#123;random.value&#125; #instance-id: $&#123;spring.cloud.client.ipAddress&#125;:$&#123;server.port&#125; instance-id: $&#123;spring.cloud.client.ipAddress&#125;:$&#123;spring.application.name&#125;:$&#123;server.port&#125; hostname: $&#123;spring.cloud.client.ipAddress&#125; #hostname: $&#123;spring.application.name&#125; # 默认30s，表示eureka client发送心跳给server端的频率 lease-renewal-interval-in-seconds: 15 # 默认90s，表示eureka server至上一次收到client的心跳之后，等待下一次心跳的超时时间，在这个时间内若没收到下一次心跳，则将移除该instance lease-expiration-duration-in-seconds: 25 # 将自己的ip显示到EuekaServer上 prefer-ip-address: true 日志打印应用的状态： 1StatusChangeEvent [timestamp=1536481926709, current=UNKNOWN, previous=UP] 这些配置肯定没有什么问题，因为这些配置在没有挪到bootstrap.yml中之前都是可以正常使用的。现在Eureka注册中心却不能检测到应用的状态，这样让我百思不得其解。后来我在stackoverflow上找到了答案stackoverflow地址,问题只有一个答案，其中重要内容如下： 1eureka.client.healthcheck.enabled=true should only be set in application.yml. Setting the value in bootstrap.yml will cause undesirable side effects like registering in eureka with an UNKNOWN status. 知道这一点后我将bootstrap.yml中的内容修改如下面一样： 123456789101112131415161718spring: application: name: search-provider cloud: config: name: search-config profile: dev label: master #uri: http://localhost:9008 discovery: enabled: true service-id: config-servereureka: client: service-url: defaultZone: http://xxxx:xxxx@xxx.ydstudio.net:xxx/eureka/ 在Eureka注册中心中应用的状态就显示正常了！","categories":[],"tags":[{"name":"spring cloud","slug":"spring-cloud","permalink":"https://ydstudios.gitee.io/tags/spring-cloud/"},{"name":"eureka","slug":"eureka","permalink":"https://ydstudios.gitee.io/tags/eureka/"}]},{"title":"Elastic Search搜索数据Terms聚合返回的不正确的问题","slug":"Elastic-Search搜索数据Terms聚合返回的不正确的问题","date":"2018-08-30T23:37:03.000Z","updated":"2020-10-24T03:13:01.000Z","comments":true,"path":"post/39992895.html","link":"","permalink":"https://ydstudios.gitee.io/post/39992895.html","excerpt":"项目中使用Elastic Search做搜索，在聚合产品的标签的时候，我发现标签的种类有很多超过了10个（这一点很重要），但是聚合标签出来的数据只有10个，缺少了很多标签，查询的语句如下：","text":"项目中使用Elastic Search做搜索，在聚合产品的标签的时候，我发现标签的种类有很多超过了10个（这一点很重要），但是聚合标签出来的数据只有10个，缺少了很多标签，查询的语句如下： 1234567891011121314151617181920212223242526272829&#123; \"from\" : 0, \"size\" : 15, \"query\" : &#123; \"bool\" : &#123; \"must\" : [ &#123; \"terms\" : &#123; \"categoryId\" : [ 1046, 1045, 1044 ] &#125; &#125;, &#123; \"terms\" : &#123; \"tagList.id\" : [ 1063, 1138, 1115, 1142 ] &#125; &#125; ] &#125; &#125;, \"aggregations\" : &#123; \"categoryId\" : &#123; \"terms\" : &#123; \"field\" : \"categoryId\" &#125; &#125;, \"tagList.tagContent\" : &#123; \"terms\" : &#123; \"field\" : \"tagList.tagContent\" &#125; &#125; &#125;&#125; 百思不得其解，看文档发现自己的查询和Terms的聚合使用都是没有任何问题的，但是出来的数据为什么会少呢？于是昨天上午花了将近一个小时的时间才找到问题的所在，下面是文档上对返回数据的一个解释：By default, the terms aggregation will return the buckets for the top ten terms ordered by the doc_count. One can change this default behaviour by setting the size parameter. SizeThe size parameter can be set to define how many term buckets should be returned out of the overall terms list. By default, the node coordinating the search process will request each shard to provide its own top size term buckets and once all shards respond, it will reduce the results to the final list that will then be returned to the client. This means that if the number of unique terms is greater than size, the returned list is slightly off and not accurate (it could be that the term counts are slightly off and it could even be that a term that should have been in the top size buckets was not returned). 原来Elastic Search对数据聚合默认返回10个，我聚合的数据术语桶超过了10个，可以自行设置size来返回不同数量的术语桶。于是乎，查询的语句修改成下面的样子就成功返回了所有的数据： 12345678910111213141516171819202122232425262728293031&#123; \"from\" : 0, \"size\" : 15, \"query\" : &#123; \"bool\" : &#123; \"must\" : [ &#123; \"terms\" : &#123; \"categoryId\" : [ 1046, 1045, 1044 ] &#125; &#125;, &#123; \"terms\" : &#123; \"tagList.id\" : [ 1063, 1138, 1115, 1142 ] &#125; &#125; ] &#125; &#125;, \"aggregations\" : &#123; \"categoryId\" : &#123; \"terms\" : &#123; \"field\" : \"categoryId\", \"size\" : 100 &#125; &#125;, \"tagList.tagContent\" : &#123; \"terms\" : &#123; \"field\" : \"tagList.tagContent\", \"size\" : 100 &#125; &#125; &#125;&#125; 其实我还有一个问题，在使用Range范围聚合的时候，发现聚合的数据不准确，就是说10-20本来就30条记录，但是聚合显示这个范围的数据只有10个，这个问题我暂时还没有找到原因，有人知道的话，我很高兴有人能告诉我！ 2018/10/12更新：对于上面的Range范围聚合不正确的原因已经找到，发现还是自己太过粗心，人家提供的开发文档没有仔细的看，终于我在2018/10/11的时候重新看了一下Elasticsearch Reference 5.5 » Aggregations » Bucket Aggregations » Range Aggregation,最开始的一段话就是问题的答案！ 1234567891011121314151617181920/*A multi-bucket value source based aggregation that enables the user to define a set of ranges - each representing a bucket. During the aggregation process, the values extracted from each document will be checked against each bucket range and \"bucket\" the relevant/matching document. Note that this aggregation includes the from value and excludes the to value for each range.Example:*/&#123; \"aggs\" : &#123; \"price_ranges\" : &#123; \"range\" : &#123; \"field\" : \"price\", \"ranges\" : [ &#123; \"to\" : 50 &#125;, &#123; \"from\" : 50, \"to\" : 100 &#125;, &#123; \"from\" : 100 &#125; ] &#125; &#125; &#125;&#125; 上面的统计的含义是： 统计价格小于50的总数 统计价格大于等于50，小于100的总数 统计价格大于等于100的总数 总之，就是统计包含下限，不包含上限。","categories":[],"tags":[{"name":"java","slug":"java","permalink":"https://ydstudios.gitee.io/tags/java/"},{"name":"elasticSearch","slug":"elasticSearch","permalink":"https://ydstudios.gitee.io/tags/elasticSearch/"}]},{"title":"IntelliJ Idea 常用快捷键列表","slug":"IntelliJ-Idea-常用快捷键列表","date":"2018-08-28T23:34:44.000Z","updated":"2019-12-09T15:03:26.000Z","comments":true,"path":"post/fea71f82.html","link":"","permalink":"https://ydstudios.gitee.io/post/fea71f82.html","excerpt":"加粗的是我常用的快捷键","text":"加粗的是我常用的快捷键 Ctrl+Shift + Enter，语句完成“！”，否定完成，输入表达式时按 “！”键Ctrl+E，最近的文件Ctrl+Shift+E，最近更改的文件Shift+Click，可以关闭文件Ctrl+[ OR ]，可以跑到大括号的开头与结尾Ctrl+F12，可以显示当前文件的结构Ctrl+F7，可以查询当前元素在当前文件中的引用，然后按 F3 可以选择Ctrl+N，可以快速打开类Ctrl+Shift+N，可以快速打开文件Alt+Q，可以看到当前方法的声明Ctrl+P，可以显示参数信息Ctrl+Shift+Insert，可以选择剪贴板内容并插入Alt+Insert，可以生成构造器/Getter/Setter等Ctrl+Alt+V，可以引入变量。例如：new String(); 自动导入变量定义Ctrl+Alt+T，可以把代码包在一个块内，例如：try/catchCtrl+Enter，导入包，自动修正Ctrl+Alt+L，格式化代码Ctrl+Alt+I，将选中的代码进行自动缩进编排，这个功能在编辑 JSP 文件时也可以工作Ctrl+Alt+O，优化导入的类和包Ctrl+R，替换文本Ctrl+F，查找文本Ctrl+Shift+Space，自动补全代码Ctrl+空格，代码提示（与系统输入法快捷键冲突）Ctrl+Shift+Alt+N，查找类中的方法或变量Alt+Shift+C，最近的更改Alt+Shift+Up/Down，上/下移一行Shift+F6，重构 - 重命名Ctrl+X，删除行Ctrl+D，复制行Ctrl+/或Ctrl+Shift+/，注释（//或者//）Ctrl+J，自动代码（例如：serr）Ctrl+Alt+J，用动态模板环绕Ctrl+H，显示类结构图（类的继承层次）**Ctrl+Q，显示注释文档Alt+F1，查找代码所在位置Alt+1，快速打开或隐藏工程面板Ctrl+Alt+left/right，返回至上次浏览的位置Alt+left/right，切换代码视图Alt+Up/Down，在方法间快速移动定位Ctrl+Shift+Up/Down，向上/下移动语句F2 或 Shift+F2，高亮错误或警告快速定位Tab，代码标签输入完成后，按 Tab，生成代码Ctrl+Shift+F7，高亮显示所有该文本，按 Esc 高亮消失Alt+F3，逐个往下查找相同文本，并高亮显示Ctrl+Up/Down，光标中转到第一行或最后一行下Ctrl+B/Ctrl+Click，快速打开光标处的类或方法（跳转到定义处）Ctrl+Alt+B，跳转到方法实现处Ctrl+Shift+Backspace，跳转到上次编辑的地方Ctrl+O，重写方法Ctrl+Alt+Space，类名自动完成Ctrl+Alt+Up/Down，快速跳转搜索结果Ctrl+Shift+J，整合两行Alt+F8，计算变量值Ctrl+Shift+V，可以将最近使用的剪贴板内容选择插入到文本Ctrl+Alt+Shift+V，简单粘贴Shift+Esc，不仅可以把焦点移到编辑器上，而且还可以隐藏当前（或最后活动的）工具窗口F12，把焦点从编辑器移到最近使用的工具窗口Shift+F1，要打开编辑器光标字符处使用的类或者方法 Java 文档的浏览器Ctrl+W，可以选择单词继而语句继而行继而函数Ctrl+Shift+W，取消选择光标所在词Alt+F7，查找整个工程中使用地某一个类、方法或者变量的位置Ctrl+I，实现方法Ctrl+Shift+U，大小写转化Ctrl+Y，删除当前行Shift+Enter，向下插入新行psvm/sout，main/System.out.println(); Ctrl+J，查看更多Ctrl+Shift+F，全局查找Ctrl+F，查找/Shift+F3，向上查找/F3，向下查找Ctrl+Shift+S，高级搜索Ctrl+U，转到父类Ctrl+Alt+S，打开设置对话框Alt+Shift+Inert，开启/关闭列选择模式Ctrl+Alt+Shift+S，打开当前项目/模块属性Ctrl+G，定位行Alt+Home，跳转到导航栏Ctrl+Enter，上插一行Ctrl+Backspace，按单词删除Ctrl+”+/-“，当前方法展开、折叠Ctrl+Shift+”+/-“，全部展开、折叠【调试部分、编译】Ctrl+F2，停止Alt+Shift+F9，选择 DebugAlt+Shift+F10，选择 RunCtrl+Shift+F9，编译Ctrl+Shift+F10，运行Ctrl+Shift+F8，查看断点F8，步过F7，步入Shift+F7，智能步入Shift+F8，步出Alt+Shift+F8，强制步过Alt+Shift+F7，强制步入Alt+F9，运行至光标处Ctrl+Alt+F9，强制运行至光标处F9，恢复程序Alt+F10，定位到断点Ctrl+F8，切换行断点Ctrl+F9，生成项目Alt+1，项目Alt+2，收藏Alt+6，TODOAlt+7，结构Ctrl+Shift+C，复制路径Ctrl+Alt+Shift+C，复制引用，必须选择类名Ctrl+Alt+Y，同步Ctrl+，快速切换方案（界面外观、代码风格、快捷键映射等菜单）Shift+F12，还原默认布局Ctrl+Shift+F12，隐藏/恢复所有窗口Ctrl+F4，关闭Ctrl+Shift+F4，关闭活动选项卡Ctrl+Tab，转到下一个拆分器Ctrl+Shift+Tab，转到上一个拆分器【重构】Ctrl+Alt+Shift+T，弹出重构菜单Shift+F6，重命名F6，移动F5，复制Alt+Delete，安全删除Ctrl+Alt+N，内联【查找】Ctrl+F，查找Ctrl+R，替换F3，查找下一个Shift+F3，查找上一个Ctrl+Shift+F，在路径中查找Ctrl+Shift+R，在路径中替换Ctrl+Shift+S，搜索结构Ctrl+Shift+M，替换结构Alt+F7，查找用法Ctrl+Alt+F7，显示用法Ctrl+F7，在文件中查找用法Ctrl+Shift+F7，在文件中高亮显示用法【VCS】Alt+，VCS 操作菜单Ctrl+K，提交更改Ctrl+T，更新项目Ctrl+Alt+Shift+D，显示变化","categories":[],"tags":[{"name":"java","slug":"java","permalink":"https://ydstudios.gitee.io/tags/java/"},{"name":"ide","slug":"ide","permalink":"https://ydstudios.gitee.io/tags/ide/"}]},{"title":"BigDecimal运算出现Non-terminating decimal expansion; no exact representable decimal result","slug":"BigDecimal运算出现Non-terminating-decimal-expansion-no-exact-representable-decimal-result","date":"2018-08-24T23:28:55.000Z","updated":"2019-12-09T15:03:26.000Z","comments":true,"path":"post/77a90911.html","link":"","permalink":"https://ydstudios.gitee.io/post/77a90911.html","excerpt":"我们都知道Java提供了两个高精度计算的类:BigInteger和BigDeciaml。虽然它们大体上属于“包装器类”的范畴，但两者都没有对应的基本类型。BigDeciaml支持任意精度的定点数。例如，可以用它进行精确的货币计算。（以上摘自Java编程思想第4版）","text":"我们都知道Java提供了两个高精度计算的类:BigInteger和BigDeciaml。虽然它们大体上属于“包装器类”的范畴，但两者都没有对应的基本类型。BigDeciaml支持任意精度的定点数。例如，可以用它进行精确的货币计算。（以上摘自Java编程思想第4版） 最近的项目使用BigDeciaml，代码如下: 123456789101112131415161718/*** @param originPrice 原始货币金额* @param betweenCurrencyRate 原始货币兑换中间货币的汇率* @param toCurrencyRate 中间货币兑换目标货币的汇率* @return java.math.BigDecimal*/public static BigDecimal rate(BigDecimal originPrice,BigDecimal betweenCurrencyRate,BigDecimal toCurrencyRate)&#123; return originPrice.multiply(betweenCurrencyRate).divide(toCurrencyRate);&#125;BigDecimal chaoIn = exchangeRate.getChaoIn();BigDecimal cNYChaoIn = exchangeRateCNY.getChaoIn();adultPrice = RateExchangeUtil.rate(adultPrice,chaoIn,cNYChaoIn);childPrice = RateExchangeUtil.rate(childRetail.multiply(new BigDecimal(childNum)),chaoIn,cNYChaoIn);// 计算酒店里面总价hotelTotalPrice = hotelTotalPrice.add(childPrice).add(adultPrice); 在使用BigDecimal做运算的时候，出现了下面的异常： 1Non-terminating decimal expansion; no exact representable decimal result. 后来通过搜索得知，通过BigDecimal的divide方法进行除法时当不整除，出现无限循环小数时，就会抛出上面的异常。我们应该使用divide的重载方法： 123456789101112131415161718192021BigDecimal.divide(BigDecimal divisor, int scale, RoundingMode roundingMode) ;scale为小数位数；roundingMode为小数模式；ROUND_CEILING如果 BigDecimal 是正的，则做 ROUND_UP 操作；如果为负，则做 ROUND_DOWN 操作。ROUND_DOWN从不在舍弃(即截断)的小数之前增加数字。ROUND_FLOOR如果 BigDecimal 为正，则作 ROUND_UP ；如果为负，则作 ROUND_DOWN 。ROUND_HALF_DOWN若舍弃部分&gt; .5，则作 ROUND_UP；否则，作 ROUND_DOWN 。ROUND_HALF_EVEN如果舍弃部分左边的数字为奇数，则作 ROUND_HALF_UP ；如果它为偶数，则作 ROUND_HALF_DOWN 。ROUND_HALF_UP若舍弃部分&gt;=.5，则作 ROUND_UP ；否则，作 ROUND_DOWN 。ROUND_UNNECESSARY该“伪舍入模式”实际是指明所要求的操作必须是精确的，，因此不需要舍入操作。ROUND_UP总是在非 0 舍弃小数(即截断)之前增加数字。 最后修正rate方法中的问题，代码如下： 1234567891011121314151617/*** 货币转换 美元转加元，人民币作为中间货币* 美元 * 美元兑换人民币的汇率 / 人民币兑换加元的汇率* 出现问题 Non-terminating decimal expansion; no exact representable decimal result.* 要指定 divide的后两个参数* JAVA中如果用BigDecimal做除法的时候一定要在divide方法中传递scale参数，* 定义精确到小数点后几位，否则在不整除的情况下，结果是无限循环小数时，就会抛出以上异常。** @param originPrice 原始货币金额* @param betweenCurrencyRate 原始货币兑换中间货币的汇率* @param toCurrencyRate 中间货币兑换目标货币的汇率* @return java.math.BigDecimal*/public static BigDecimal rate(BigDecimal originPrice,BigDecimal betweenCurrencyRate,BigDecimal toCurrencyRate)&#123; return originPrice.multiply(betweenCurrencyRate).divide(toCurrencyRate,4,BigDecimal.ROUND_HALF_UP);&#125;","categories":[],"tags":[{"name":"java","slug":"java","permalink":"https://ydstudios.gitee.io/tags/java/"},{"name":"BigDecimal","slug":"BigDecimal","permalink":"https://ydstudios.gitee.io/tags/BigDecimal/"}]},{"title":"Nginx实现反向代理 Node.js","slug":"Nginx实现反向代理-Node-js","date":"2018-08-07T23:25:53.000Z","updated":"2019-12-09T15:03:26.000Z","comments":true,"path":"post/45183e9d.html","link":"","permalink":"https://ydstudios.gitee.io/post/45183e9d.html","excerpt":"公司有项目前端是用node.js进行服务器渲染，然后再返回给浏览器，进而解决单页面的SEO问题。项目部署的时候，使用Nginx反向代理Node.js。具体的步骤如下：","text":"公司有项目前端是用node.js进行服务器渲染，然后再返回给浏览器，进而解决单页面的SEO问题。项目部署的时候，使用Nginx反向代理Node.js。具体的步骤如下： （Nginx、Node.js的安装和基本配置直接跳过） 首先我们要在nginx.cnf文件中的http节点打开下面的配置： 12345678910111213141516171819http &#123; log_format main '$remote_addr - $remote_user [$time_local] \"$request\" ' '$status $body_bytes_sent \"$http_referer\" ' '\"$http_user_agent\" \"$http_x_forwarded_for\"'; access_log /var/log/nginx/access.log main; sendfile on; tcp_nopush on; tcp_nodelay on; keepalive_timeout 65; types_hash_max_size 2048; include /etc/nginx/mime.types; default_type application/octet-stream; # 打开这一行的配置 include /etc/nginx/conf.d/*.conf;&#125; 然后每个域名的配置文件就放到这个目录/etc/nginx/conf.d/下，文件后缀以conf结束。 第一种方式，这种简单： 12345678910111213141516171819202122232425262728293031323334server &#123; listen 80 ; server_name localhost; root /xxx/xxx/hxxydexx/; #set $my_server_name $scheme://$server_name; #if ( $my_server_name != https://$server_name ) &#123; # rewrite ^ https://$server_name$request_uri? permanent; #&#125; error_log /var/log/nginx/hyde_error.log error; access_log /var/log/nginx/hyde_accss.log main; location / &#123; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header Host $http_host; proxy_set_header X-Nginx-Proxy true; proxy_http_version 1.1; proxy_set_header Connection &quot;&quot;; # 不需要考虑到负载的，就无需配置upstream节点。 proxy_pass http://127.0.0.1:3000; &#125; error_page 404 /404.html; location = /xxx/xxx/40x.html &#123; &#125; error_page 500 502 503 504 /50x.html; location = /xxx/xxx/50x.html &#123; &#125;&#125; 2.第二种方式，考虑到负载 12345678910111213141516171819202122232425262728293031323334353637upstream node &#123; server 127.0.0.1:3000; &#125;server &#123; listen 80 ; server_name localhost; root /xxx/xxx/hxxydexx/; #set $my_server_name $scheme://$server_name; #if ( $my_server_name != https://$server_name ) &#123; # rewrite ^ https://$server_name$request_uri? permanent; #&#125; error_log /var/log/nginx/hyde_error.log error; access_log /var/log/nginx/hyde_accss.log main; location / &#123; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header Host $http_host; proxy_set_header X-Nginx-Proxy true; proxy_http_version 1.1; proxy_set_header Connection \"\"; # 配置upstream节点 proxy_pass http://node; &#125; error_page 404 /404.html; location = /xxx/xxx/40x.html &#123; &#125; error_page 500 502 503 504 /50x.html; location = /xxx/xxx/50x.html &#123; &#125;&#125; 然后重启或者重新载入nginx的配置文件即可。命令如下： 12345678#检查nginx配置文件中语法是否正确nginx -t#重启nginxservice nginx restart#重载配置文件nginx -s reload 注意问题：上面可能会出现下面的问题： 1234567891011121314events.js:72 throw er; // Unhandled 'error' event ^Error: listen EADDRINUSE at errnoException (net.js:884:11) at Server._listen2 (net.js:1022:14) at listen (net.js:1044:10) at Server.listen (net.js:1110:5) at Object.&lt;anonymous&gt; (folderName/app.js:33:24) at Module._compile (module.js:456:26) at Object.Module._extensions..js (module.js:474:10) at Module.load (module.js:356:32) at Function.Module._load (module.js:312:12) at Function.Module.runMain (module.js:497:10) 这个其实是Node.js服务多开端口被占用导致的报错，出现这种问题，可以使用Node.js项目管理工具pm2，或者使用netstat -anop进行查看端口被那个进程占用，然后杀掉重启服务！ 附上Nginx的负载均衡策略： 轮询（默认）每个请求按时间顺序逐一分配到不同的后端服务器，如果后端服务器down掉，能自动剔除。 1234upstream backserver &#123; server 192.168.0.14; server 192.168.0.15; &#125; 指定权重指定轮询几率，weight和访问比率成正比，用于后端服务器性能不均的情况。 1234upstream backserver &#123; server 192.168.0.14 weight=10; server 192.168.0.15 weight=10; &#125; IP绑定 ip_hash每个请求按访问ip的hash结果分配，这样每个访客固定访问一个后端服务器，可以解决session的问题。 12345678910111213141516171819202122232425262728upstream backserver &#123; ip_hash; server 192.168.0.14:88; server 192.168.0.15:80; &#125; ``` - fair（第三方）按后端服务器的响应时间来分配请求，响应时间短的优先分配。```bashupstream backserver &#123; server 192.168.0.14:88; server 192.168.0.15:80; fair; &#125; ``` - url_hash（第三方）按访问url的hash结果来分配请求，使每个url定向到同一个后端服务器，后端服务器为缓存时比较有效。 ```bashupstream backserver &#123; server squid1:3128; server squid2:3128; hash $request_uri; hash_method crc32; &#125;","categories":[],"tags":[{"name":"nginx","slug":"nginx","permalink":"https://ydstudios.gitee.io/tags/nginx/"},{"name":"proxy","slug":"proxy","permalink":"https://ydstudios.gitee.io/tags/proxy/"}]},{"title":"Java学习系列文章第九篇：Java中泛型的学习","slug":"Java学习系列文章第九篇：Java中泛型的学习","date":"2018-08-04T23:03:24.000Z","updated":"2019-12-09T15:03:26.000Z","comments":true,"path":"post/d9ad2c86.html","link":"","permalink":"https://ydstudios.gitee.io/post/d9ad2c86.html","excerpt":"什么是泛型Java 泛型（generics）是 JDK5 中引入的一个新特性, 泛型提供了编译时类型安全检测机制，该机制允许程序员在编译时检测到非法的类型。泛型的本质是参数化类型，也就是说所操作的数据类型被指定为一个参数。(PHP就不需要泛型，哈哈)","text":"什么是泛型Java 泛型（generics）是 JDK5 中引入的一个新特性, 泛型提供了编译时类型安全检测机制，该机制允许程序员在编译时检测到非法的类型。泛型的本质是参数化类型，也就是说所操作的数据类型被指定为一个参数。(PHP就不需要泛型，哈哈) 下面我们看一道关于Java泛型的经典测试题： 1234List&lt;String&gt; list1 = new ArrayList&lt;String&gt;();List&lt;Integer&gt; list2 = new ArrayList&lt;Integer&gt;();System.out.println(list1.getClass() == list2.getClass()); 上面代码最终结果输出的是什么？不了解泛型的和很熟悉泛型的同学应该能够答出来，而对泛型有所了解，但是了解不深入的同学可能会答错。 正确答案是 true。 上面提到了泛型的本质是类型参数化，如何解释类型参数化呢？ 123456789101112public class Cache &#123; Object value; public Object getValue() &#123; return value; &#125; public void setValue(Object value) &#123; this.value = value; &#125;&#125; 这样的 Cache 是能够存取任何类型的值，但是我们在使用的时候会有点麻烦。我们要获取真正的类型，就要进行类型的强制转换了。 12345Cache cache = new Cache();cache.setValue(134);int value = (int) cache.getValue();cache.setValue(\"hello\");String value1 = (String) cache.getValue(); 这样的编程真的是太麻烦了，万一忘记了存放变量的类型，程序在运行的时候就会报类型转换错误。于是在JDK5 中引用的泛型给我们带来了另一番编程体验。 123456789101112public class Cache&lt;T&gt; &#123; T value; public Object getValue() &#123; return value; &#125; public void setValue(T value) &#123; this.value = value; &#125;&#125; 这就是泛型，它将 value 这个属性的类型也参数化了，这就是所谓的参数化类型。再看它的使用方法。 1234567Cache&lt;String&gt; cache1 = new Cache&lt;String&gt;();cache1.setValue(\"123\");String value2 = cache1.getValue();Cache&lt;Integer&gt; cache2 = new Cache&lt;Integer&gt;();cache2.setValue(123);int value3 = cache2.getValue(); 最显而易见的好处就是它不再需要对取出来的结果进行强制转换了。但，还有另外一点不同。泛型除了可以将类型参数化外，而参数一旦确定好，如果类似不匹配，编译器就不通过。综合上面信息，我们可以得到下面的结论: 与普通的 Object 代替一切类型这样简单粗暴而言，泛型使得数据的类别可以像参数一样由外部传递进来。它提供了一种扩展能力。它更符合面向抽象开发的软件编程宗旨。 当具体的类型确定后，泛型又提供了一种类型检测的机制，只有相匹配的数据才能正常的赋值，否则编译器就不通过。所以说，它是一种类型安全检测机制，一定程度上提高了软件的安全性防止出现低级的失误。 泛型提高了程序代码的可读性，不必要等到运行的时候才去强制转换，在定义或者实例化阶段，因为 Cache 这个类型显化的效果，程序员能够一目了然猜测出代码要操作的数据类型。 泛型的定义和使用泛型按照使用情况可以分为 3 种。 泛型类 泛型方法 泛型接口 泛型类那如何定义泛型类呢？ 123public class Test&lt;T&gt; &#123; T field;&#125; 尖括号 &lt;&gt; 中的 T 被称作是类型参数，用于指代任何类型。事实上，T 只是一种习惯性写法，如果你愿意。你可以这样写。 123public class Test&lt;TMD&gt; &#123; TMD field;&#125; 但出于规范的目的，Java 还是建议我们用单个大写字母来代表类型参数。常见的如： T 代表一般的任何类。 E 代表Element 的意思，或者是Exception 异常的意思 K 代表 Key 的意思 V 代表 Value 的意思，通常与 K 一起配合使用 S 代表 Subtype 的意思 如果一个类被 的形式定义，那么它就被称为泛型类。那么对于泛型类怎么样使用呢？ 12Test&lt;String&gt; test1 = new Test&lt;&gt;();Test&lt;Integer&gt; test2 = new Test&lt;&gt;(); 只要在对泛型类创建实例的时候，在尖括号中赋值相应的类型便是。T 就会被替换成对应的类型，如 String 或者是 Integer。你可以相像一下，当一个泛型类被创建时，内部自动扩展成下面的代码。 123public class Test&lt;String&gt; &#123; String field;&#125; 当然，泛型类不是只能接受一个类型参数，它还可以这样接受多个类型参数。 123456789101112public class MultiType &lt;E,T&gt;&#123; E value1; T value2; public E getValue1()&#123; return value1; &#125; public T getValue2()&#123; return value2; &#125;&#125; 泛型方法123456public class Test1 &#123; public &lt;T&gt; void testMethod(T t)&#123; &#125;&#125; 泛型方法与泛型类稍有不同的地方是，类型参数也就是尖括号那一部分是写在返回值前面的。 中的 T 被称为类型参数，而方法中的 T 被称为参数化类型，它不是运行时真正的参数。 当然，声明的类型参数，其实也是可以当作返回值的类型的。 123public &lt;T&gt; T testMethod1(T t)&#123; return null;&#125; 泛型类与泛型方法的共存现象: 123456789public class Test&lt;T&gt;&#123; public void testMethod(T t)&#123; System.out.println(t.getClass().getName()); &#125; public &lt;T&gt; T testMethod1(T t)&#123; return t; &#125;&#125; 上面代码中，Test1 是泛型类，testMethod 是泛型类中的普通方法，而 testMethod1 是一个泛型方法。而泛型类中的类型参数与泛型方法中的类型参数是没有相应的联系的，泛型方法始终以自己定义的类型参数为准。 所以，针对上面的代码，我们可以这样编写测试代码。 123Test1&lt;String&gt; t = new Test1();t.testMethod(\"generic\");Integer i = t.testMethod1(new Integer(1)); 泛型类的实际类型参数是 String，而传递给泛型方法的类型参数是 Integer，两者不想干。 但是，为了避免混淆，如果在一个泛型类中存在泛型方法，那么两者的类型参数最好不要同名。比如，Test 代码可以更改为这样 123456789public class Test1&lt;T&gt;&#123; public void testMethod(T t)&#123; System.out.println(t.getClass().getName()); &#125; public &lt;E&gt; E testMethod1(E e)&#123; return e; &#125;&#125; 泛型接口泛型接口定义： 12public interface Iterable&lt;T&gt; &#123;&#125; 通配符 ？除了用 表示泛型外，还有 这种形式。？ 被称为通配符。 可能有同学会想，已经有了 的形式了，为什么还要引进 这样的概念呢？ 12345678class Base&#123;&#125;class Sub extends Base&#123;&#125;Sub sub = new Sub();Base base = sub; 上面代码显示，Base 是 Sub 的父类，它们之间是继承关系，所以 Sub 的实例可以给一个 Base 引用赋值，那么 12List&lt;Sub&gt; lsub = new ArrayList&lt;Sub&gt;();List&lt;Base&gt; lbase = lsub; 最后一行代码成立吗？编译会通过吗？ 答案是通过不了编译的。 编译器不会让它通过的。Sub 是 Base 的子类，不代表 List 和 List 有继承关系。 但是，在现实编码中，确实有这样的需求，希望泛型能够处理某一范围内的数据类型，比如某个类和它的子类，对此 Java 引入了通配符这个概念。 所以，通配符的出现是为了指定泛型中的类型范围。 通配符有 3 种形式: &lt;?&gt; 被称作无限定的通配符 &lt;? extends T&gt; 被称作有上限的通配符 &lt;? super T&gt; 被称作有下限的通配符 无限定通配符12public void testWildCards(Collection&lt;?&gt; collection)&#123;&#125; 上面的代码中，方法内的参数是被无限定通配符修饰的 Collection 对象，它隐略地表达了一个意图或者可以说是限定，那就是 testWidlCards() 这个方法内部无需关注 Collection 中的真实类型，因为它是未知的。所以，你只能调用 Collection 中与类型无关的方法。 1234567891011121314public class TestWildCards&#123; public void testWildCards(Collection&lt;?&gt; collection)&#123; #报错 collection.add(123); #报错 collection.add(\"hello\"); #报错 collection.add(new Object()); collection.iterator().next(); collection.size(); &#125;&#125; 我们可以看到，当 &lt;?&gt; 存在时，Collection 对象丧失了 add() 方法的功能，编译器不通过。我们再看下面的代码： 12List&lt;?&gt; wildlist = new ArrayList&lt;String&gt;();wildlist.add(123);// 编译不通过 有人说，&lt;?&gt; 提供了只读的功能，也就是它删减了增加具体类型元素的能力，只保留与具体类型无关的功能。它不管装载在这个容器内的元素是什么类型，它只关心元素的数量、容器是否为空？我想这种需求还是很常见的吧。 有同学可能会想，&lt;?&gt; 既然作用这么渺小，那么为什么还要引用它呢？ 个人认为，提高了代码的可读性，程序员看到这段代码时，就能够迅速对此建立极简洁的印象，能够快速推断源码作者的意图。 &lt;? extends T&gt;&lt;?&gt; 代表着类型未知，但是我们的确需要对于类型的描述再精确一点，我们希望在一个范围内确定类别，比如类型 A 及 类型 A 的子类都可以。 123public void testSub(Collection&lt;? extends Base&gt; para)&#123;&#125; 上面代码中，para 这个 Collection 接受 Base 及 Base 的子类的类型。但是，它仍然丧失了写操作的能力。也就是说: 12para.add(new Sub());para.add(new Base()); 仍然编译不通过。没有关系，我们不知道具体类型，但是我们至少清楚了类型的范围。 &lt;? super T&gt;这个和 &lt;? extends T&gt; 相对应，代表 T 及 T 的超类。 123public void testSuper(Collection&lt;? super Sub&gt; para)&#123;&#125;` &lt;? super T&gt; 神奇的地方在于，它拥有一定程度的写操作的能力。 1234public void testSuper(Collection&lt;? super Sub&gt; para)&#123; para.add(new Sub());//编译通过 para.add(new Base());//编译不通过&#125; 通配符与类型参数的区别一般而言，通配符能干的事情都可以用类型参数替换。比如: 12public void testWildCards(Collection&lt;?&gt; collection)&#123;&#125; 可以被 12public &lt;T&gt; void test(Collection&lt;T&gt; collection)&#123;&#125; 取代。值得注意的是，如果用泛型方法来取代通配符，那么上面代码中 collection 是能够进行写操作的。只不过要进行强制转换。 1234public &lt;T&gt; void test(Collection&lt;T&gt; collection)&#123; collection.add((T)new Integer(12)); collection.add((T)\"123\");&#125; 需要特别注意的是，类型参数适用于参数之间的类别依赖关系，举例说明: 123456789public class Test2 &lt;T,E extends T&gt;&#123; T value1; E value2;&#125;public &lt;D,S extends D&gt; void test(D d,S s)&#123;&#125; E 类型是 T 类型的子类，显然这种情况类型参数更适合。有一种情况是，通配符和类型参数一起使用。 123public &lt;T&gt; void test(T t,Collection&lt;? extends T&gt; collection)&#123;&#125; 如果一个方法的返回类型依赖于参数的类型，那么通配符也无能为力。 123public T test1(T t)&#123; return value1;&#125; 类型擦除泛型是 Java 1.5 版本才引进的概念，在这之前是没有泛型的概念的，但显然，泛型代码能够很好地和之前版本的代码很好地兼容。 这是因为，泛型信息只存在于代码编译阶段，在进入 JVM 之前，与泛型相关的信息会被擦除掉，专业术语叫做类型擦除。 通俗地讲，泛型类和普通类在 java 虚拟机内是没有什么特别的地方。回顾文章开始时的那段代码: 1234List&lt;String&gt; list1 = new ArrayList&lt;String&gt;();List&lt;Integer&gt; list2 = new ArrayList&lt;Integer&gt;();System.out.println(list1.getClass() == list2.getClass()); 打印的结果为 true 是因为 List 和 List 在 jvm 中的 Class 都是 List.class。 泛型信息被擦除了。 可能同学会问，那么类型 String 和 Integer 怎么办？ 答案是泛型转译。 12345678public class Erasure &lt;T&gt;&#123; T object; public Erasure(T object) &#123; this.object = object; &#125;&#125; Erasure 是一个泛型类，我们查看它在运行时的状态信息可以通过反射。 123456Erasure&lt;String&gt; erasure = new Erasure&lt;String&gt;(\"hello\");Class eclz = erasure.getClass();System.out.println(\"erasure class is:\"+eclz.getName());#打印的结果是erasure class is:com.frank.test.Erasure Class 的类型仍然是 Erasure 并不是 Erasure 这种形式，那我们再看看泛型类中 T 的类型在 jvm 中是什么具体类型。 1234567Field[] fs = eclz.getDeclaredFields();for ( Field f:fs) &#123; System.out.println(\"Field name \"+f.getName()+\" type:\"+f.getType().getName());&#125;#打印结果是Field name object type:java.lang.Object 那我们可不可以说，泛型类被类型擦除后，相应的类型就被替换成 Object 类型呢？ 这种说法，不完全正确。 我们更改一下代码。 123456789101112public class Erasure &lt;T extends String&gt;&#123; T object; public Erasure(T object) &#123; this.object = object; &#125;&#125;#现在再看测试结果：Field name object type:java.lang.String 我们现在可以下结论了，在泛型类被类型擦除的时候，之前泛型类中的类型参数部分如果没有指定上限，如 则会被转译成普通的 Object 类型，如果指定了上限如 则类型参数就被替换成类型上限。 所以，在反射中: 123456789101112public class Erasure &lt;T&gt;&#123; T object; public Erasure(T object) &#123; this.object = object; &#125; public void add(T object)&#123; &#125;&#125; add() 这个方法对应的 Method 的签名应该是 Object.class。 123456789101112Erasure&lt;String&gt; erasure = new Erasure&lt;String&gt;(\"hello\");Class eclz = erasure.getClass();System.out.println(\"erasure class is:\"+eclz.getName());Method[] methods = eclz.getDeclaredMethods();for ( Method m:methods )&#123; System.out.println(\" method:\"+m.toString());&#125;#打印结果是method:public void com.frank.test.Erasure.add(java.lang.Object) 也就是说，如果你要在反射中找到 add 对应的 Method，你应该调用 getDeclaredMethod(“add”,Object.class) 否则程序会报错，提示没有这么一个方法，原因就是类型擦除的时候，T 被替换成 Object 类型了。 泛型中值得注意的地方泛型类或者泛型方法中，不接受 8 种基本数据类型。12345678// 这种是错误的使用方式List&lt;int&gt; li = new ArrayList&lt;&gt;();List&lt;boolean&gt; li = new ArrayList&lt;&gt;();// 需要使用它们对应的包装类List&lt;Integer&gt; li = new ArrayList&lt;&gt;();List&lt;Boolean&gt; li1 = new ArrayList&lt;&gt;();` 对泛型方法的困惑123public &lt;T&gt; T test(T t)&#123; return null;&#125; 有的同学可能对于连续的两个 T 感到困惑，其实 是为了说明类型参数，是声明,而后面的不带尖括号的 T 是方法的返回值类型。你可以相像一下，如果 test() 这样被调用: 123test(\"123\");// 那么实际上相当于public String test(String t); Java 不能创建具体类型的泛型数组这句话可能难以理解，用代码来说明： 12List&lt;Integer&gt;[] li2 = new ArrayList&lt;Integer&gt;[];List&lt;Boolean&gt; li3 = new ArrayList&lt;Boolean&gt;[]; 这两行代码是无法在编译器中编译通过的。原因还是类型擦除带来的影响。 List 和 List 在 jvm 中等同于List ，所有的类型信息都被擦除，程序也无法分辨一个数组中的元素类型具体是 List类型还是 List 类型。但是， 123List&lt;?&gt;[] li3 = new ArrayList&lt;?&gt;[10];li3[1] = new ArrayList&lt;String&gt;();List&lt;?&gt; v = li3[1]; 借助于无限定通配符却可以，前面讲过 ？ 代表未知类型，所以它涉及的操作都基本上与类型无关，因此 jvm 不需要针对它对类型作判断，因此它能编译通过，但是，只提供了数组中的元素因为通配符原因，它只能读，不能写。比如，上面的 v 这个局部变量，它只能进行 get() 操作，不能进行 add() 操作，这个在前面通配符的内容小节中已经讲过。 泛型，并不神奇我们可以看到，泛型其实并没有什么神奇的地方，泛型代码能做的非泛型代码也能做。 而类型擦除，是泛型能够与之前的 java 版本代码兼容共存的原因。 可量也正因为类型擦除导致了一些隐患与局限。 但，我还是要建议大家使用泛型，如官方文档所说的，如果可以使用泛型的地方，尽量使用泛型。 毕竟它抽离了数据类型与代码逻辑，本意是提高程序代码的简洁性和可读性，并提供可能的编译时类型转换安全检测功能。 类型擦除不是泛型的全部，但是它却能很好地检测我们对于泛型这个概念的理解程度。来源：blog.csdn.net/briblue/article/details/76736356","categories":[],"tags":[{"name":"java","slug":"java","permalink":"https://ydstudios.gitee.io/tags/java/"}]},{"title":"Centos 6.x 安装git","slug":"Centos-6-x-安装git","date":"2018-08-03T23:01:47.000Z","updated":"2019-12-09T15:03:26.000Z","comments":true,"path":"post/7226e685.html","link":"","permalink":"https://ydstudios.gitee.io/post/7226e685.html","excerpt":"在Centos上安装软件时，我都是先更新一下系统的yum源，保持yum源是比较新的。","text":"在Centos上安装软件时，我都是先更新一下系统的yum源，保持yum源是比较新的。 1yum update 下面进入今日的主题，安装git： 1.下载源码包查看系统yum源中的git的版本，发现是比较老的版本，于是采用源码包编译安装。 12cd /usr/local/srcwget https://mirrors.edge.kernel.org/pub/software/scm/git/git-2.9.0.tar.gz 2.编译安装 123456cd /usr/local/srctar -zxvf git-2.9.0.tar.gz cd /usr/local/src/git-2.9.0#安装到 /usr/local/git./configure prefix=/usr/local/git make &amp;&amp; make instal 报错 12345[root@iZbp1fuxiq5o2qp7z60ydpZ git-2.9.0]# make &amp;&amp; make install CC credential-store.oIn file included from credential-store.c:1:0:cache.h:40:18: fatal error: zlib.h: No such file or directory #include &lt;zlib.h&gt; 解决方法： 12#安装依赖yum -y install zlib zlib-devel 再次执行make，仍然报错 12345/usr/bin/perl Makefile.PL PREFIX='/usr/local/git' INSTALL_BASE='' --localedir='/usr/local/git/share/locale'Can't locate ExtUtils/MakeMaker.pm in @INC (@INC contains: /usr/local/lib64/perl5 /usr/local/share/perl5 /usr/lib64/perl5/vendor_perl /usr/share/perl5/vendor_perl /usr/lib64/perl5 /usr/share/perl5 .) at Makefile.PL line 3.BEGIN failed--compilation aborted at Makefile.PL line 3.make[1]: *** [perl.mak] Error 2make: *** [perl/perl.mak] Error 2 解决办法： 12#安装下面的依赖yum install perl-ExtUtils-CBuilder perl-ExtUtils-MakeMaker cpan 再次执行make &amp; make install，编译安装成功 3.创建git用户，并将/usr/local/git所属组和所属主修改成git 123useradd gitchown -R git:git /usr/local/git 4.将git加到系统环境变量里面 12export GIT_HOME=/usr/local/gitexport PATH=$GIT_HOME/bin:$PATH 5.初始化仓库 123456git init --bare mt-front.git#下面是几种克隆方式git clone git@xx.xx.xx.xx:/home/git/repository/mt-front.gitgit clone ssh://git@xx.xx.xx.xx:/home/git/repository/mt-front.gitgit clone http://git@xx.xx.xx.xx:/home/git/repository/mt-front.git 本地clone的时候会报错 提示git-upload-pack命令不存在123bash: git-upload-pack: command not found#解决ln -s /usr/local/git/bin/git-upload-pack /usr/bin/git-upload-pack 提示git-receive-pack命令不存在12bash: git-receive-pack: command not foundln -s /usr/local/git/bin/git-receive-pack /usr/bin/git-receive-pack 7.配置git记住密码，在git的全局配置文件gitconfig文件中添加下面的配置： 12345[credential] helper = store[user] name = nick email = nick@da.com","categories":[],"tags":[{"name":"linux","slug":"linux","permalink":"https://ydstudios.gitee.io/tags/linux/"},{"name":"git","slug":"git","permalink":"https://ydstudios.gitee.io/tags/git/"}]},{"title":"Maven跳过单元测试-Dmaven.test.skip和skipTests的区别","slug":"Maven跳过单元测试-Dmaven-test-skip和skipTests的区别","date":"2018-08-01T22:46:25.000Z","updated":"2019-12-09T15:03:26.000Z","comments":true,"path":"post/64d34904.html","link":"","permalink":"https://ydstudios.gitee.io/post/64d34904.html","excerpt":"Maven中有两种方式跳过单元测试，其实这两种方式是有一点不同的，其具体不同如下： -DskipTests，不执行测试用例，但编译测试用例类生成相应的class文件至target/test-classes下。-Dmaven.test.skip=true，不执行测试用例，也不编译测试用例类。","text":"Maven中有两种方式跳过单元测试，其实这两种方式是有一点不同的，其具体不同如下： -DskipTests，不执行测试用例，但编译测试用例类生成相应的class文件至target/test-classes下。-Dmaven.test.skip=true，不执行测试用例，也不编译测试用例类。 使用Dmaven.test.skip，不但跳过单元测试的运行，也跳过测试代码的编译。1mvn package -Dmaven.test.skip=true 也可以在pom.xml文件中修改12345678910111213141516&lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugin&lt;/groupId&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt; &lt;version&gt;2.1&lt;/version&gt; &lt;configuration&gt; &lt;skip&gt;true&lt;/skip&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-surefire-plugin&lt;/artifactId&gt; &lt;version&gt;2.5&lt;/version&gt; &lt;configuration&gt; &lt;skip&gt;true&lt;/skip&gt; &lt;/configuration&gt; &lt;/plugin&gt; 使用 mvn package -DskipTests 跳过单元测试，但是会继续编译；如果没时间修改单元测试的bug，或者单元测试编译错误。使用上面的，不要用这个。12345678&lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-surefire-plugin&lt;/artifactId&gt; &lt;version&gt;2.5&lt;/version&gt; &lt;configuration&gt; &lt;skipTests&gt;true&lt;/skipTests&gt; &lt;/configuration&gt; &lt;/plugin&gt;","categories":[],"tags":[{"name":"java","slug":"java","permalink":"https://ydstudios.gitee.io/tags/java/"},{"name":"maven","slug":"maven","permalink":"https://ydstudios.gitee.io/tags/maven/"}]},{"title":"Java学习系列文章第八篇：Java8中方法引用","slug":"Java学习系列文章第八篇：Java8中方法引用","date":"2018-07-30T22:15:58.000Z","updated":"2019-12-09T15:03:26.000Z","comments":true,"path":"post/bcbb06a2.html","link":"","permalink":"https://ydstudios.gitee.io/post/bcbb06a2.html","excerpt":"方法引用提供了非常有用的语法，可以直接引用已有Java类或对象（实例）的方法或构造器。与lambda联合使用，方法引用可以使语言的构造更紧凑简洁，减少冗余代码。","text":"方法引用提供了非常有用的语法，可以直接引用已有Java类或对象（实例）的方法或构造器。与lambda联合使用，方法引用可以使语言的构造更紧凑简洁，减少冗余代码。 下面，我们以定义了4个方法的Car这个类作为例子，区分Java中支持的4种不同的方法引用。 1234567891011121314151617public static class Car &#123; public static Car create( final Supplier&lt; Car &gt; supplier ) &#123; return supplier.get(); &#125; public static void collide( final Car car ) &#123; System.out.println( \"Collided \" + car.toString() ); &#125; public void follow( final Car another ) &#123; System.out.println( \"Following the \" + another.toString() ); &#125; public void repair() &#123; System.out.println( \"Repaired \" + this.toString() ); &#125;&#125; 第一种方法引用是构造器引用，它的语法是Class::new，或者更一般的Class&lt; T &gt;::new。请注意构造器没有参数。 12final Car car = Car.create( Car::new );final List&lt; Car &gt; cars = Arrays.asList( car ); 第二种方法引用是静态方法引用，它的语法是Class::static_method。请注意这个方法接受一个Car类型的参数。 1cars.forEach( Car::collide ); 第三种方法引用是特定类的任意对象的方法引用，它的语法是Class::method。请注意，这个方法没有参数。 1cars.forEach( Car::repair ); 最后，第四种方法引用是特定对象的方法引用，它的语法是instance::method。请注意，这个方法接受一个Car类型的参数 12final Car police = Car.create( Car::new );cars.forEach( police::follow ); 运行上面的Java程序在控制台上会有下面的输出（Car的实例可能不一样）： 123Collided com.javacodegeeks.java8.method.references.MethodReferences$Car@7a81197dRepaired com.javacodegeeks.java8.method.references.MethodReferences$Car@7a81197dFollowing the com.javacodegeeks.java8.method.references.MethodReferences$Car@7a81197d 关于方法引用的更多详情请参考官方文档。","categories":[],"tags":[{"name":"java","slug":"java","permalink":"https://ydstudios.gitee.io/tags/java/"}]},{"title":"Java学习系列文章第七篇：Java8中接口的默认方法与静态方法","slug":"Java学习系列文章第七篇：Java8中接口的默认方法与静态方法","date":"2018-07-30T22:04:59.000Z","updated":"2019-12-09T15:03:26.000Z","comments":true,"path":"post/cca5e66b.html","link":"","permalink":"https://ydstudios.gitee.io/post/cca5e66b.html","excerpt":"Java8用默认方法与静态方法这两个新概念来扩展接口的声明。默认方法使接口有点像Traits（Scala中特征(trait)类似于Java中的Interface，但它可以包含实现代码，也就是目前Java8新增的功能。PHP中也有trait这样一说，他是在不破坏类原有的继承、实现关系，给类新加相同的方法），但与传统的接口又有些不一样，它允许在已有的接口中添加新方法，而同时又保持了与旧版本代码的兼容性。","text":"Java8用默认方法与静态方法这两个新概念来扩展接口的声明。默认方法使接口有点像Traits（Scala中特征(trait)类似于Java中的Interface，但它可以包含实现代码，也就是目前Java8新增的功能。PHP中也有trait这样一说，他是在不破坏类原有的继承、实现关系，给类新加相同的方法），但与传统的接口又有些不一样，它允许在已有的接口中添加新方法，而同时又保持了与旧版本代码的兼容性。 默认方法与抽象方法不同之处在于抽象方法必须要求实现，但是默认方法则没有这个要求。相反，每个接口都必须提供一个所谓的默认实现，这样所有的接口实现者将会默认继承它（如果有必要的话，可以覆盖这个默认实现）。让我们看看下面的例子： 1234567891011121314151617private interface Defaulable &#123; // Interfaces now allow default methods, the implementer may or // may not implement (override) them. default String notRequired() &#123; return \"Default implementation\"; &#125; &#125; private static class DefaultableImpl implements Defaulable &#123;&#125; private static class OverridableImpl implements Defaulable &#123; @Override public String notRequired() &#123; return \"Overridden implementation\"; &#125;&#125; Defaulable接口用关键字default声明了一个默认方法notRequired()，Defaulable接口的实现者之一DefaultableImpl实现了这个接口，并且让默认方法保持原样。Defaulable接口的另一个实现者OverridableImpl用自己的方法覆盖了默认方法。 Java8带来的另一个有趣的特性是接口可以声明（并且可以提供实现）静态方法。例如： 123456private interface DefaulableFactory &#123; // Interfaces now allow static methods static Defaulable create( Supplier&lt; Defaulable &gt; supplier ) &#123; return supplier.get(); &#125;&#125; 下面的一小段代码片段把上面的默认方法与静态方法黏合到一起。 1234567public static void main( String[] args ) &#123; Defaulable defaulable = DefaulableFactory.create( DefaultableImpl::new ); System.out.println( defaulable.notRequired() ); defaulable = DefaulableFactory.create( OverridableImpl::new ); System.out.println( defaulable.notRequired() );&#125; 这个程序的控制台输出如下： 12Default implementationOverridden implementation 在JVM中，默认方法的实现是非常高效的，并且通过字节码指令为方法调用提供了支持。默认方法允许继续使用现有的Java接口，而同时能够保障正常的编译过程。这方面好的例子是大量的方法被添加到java.util.Collection接口中去：stream()，parallelStream()，forEach()，removeIf()，…… 尽管默认方法非常强大，但是在使用默认方法时我们需要小心注意一个地方：在声明一个默认方法前，请仔细思考是不是真的有必要使用默认方法，因为默认方法会带给程序歧义，并且在复杂的继承体系中容易产生编译错误。更多详情请参考 官方文档","categories":[],"tags":[{"name":"java","slug":"java","permalink":"https://ydstudios.gitee.io/tags/java/"},{"name":"interface","slug":"interface","permalink":"https://ydstudios.gitee.io/tags/interface/"}]},{"title":"Maven中的dependencyManagement和dependencies","slug":"Maven中的dependencyManagement和dependencies","date":"2018-07-28T22:19:50.000Z","updated":"2019-12-09T15:03:26.000Z","comments":true,"path":"post/c65e0153.html","link":"","permalink":"https://ydstudios.gitee.io/post/c65e0153.html","excerpt":"最近公司的数据中心项目是使用Maven进行管理，在开发的过程中遇到了dependencyManagement和dependencies，不知道大家对这两个元素有什么想法？ Maven使用dependencyManagement元素来提供了一种管理依赖版本号的方式。通常会在一个组织或者项目的最顶层的父POM 中看到dependencyManagement 元素。使用pom.xml中的dependencyManagement元素能让所有在子项目中引用一个依赖而不用显式的列出版本号。Maven会沿着父子层次向上走，直到找到一个拥有dependencyManagement元素的项目，然后它就会使用在这个dependencyManagement 元素中指定的版本号。","text":"最近公司的数据中心项目是使用Maven进行管理，在开发的过程中遇到了dependencyManagement和dependencies，不知道大家对这两个元素有什么想法？ Maven使用dependencyManagement元素来提供了一种管理依赖版本号的方式。通常会在一个组织或者项目的最顶层的父POM 中看到dependencyManagement 元素。使用pom.xml中的dependencyManagement元素能让所有在子项目中引用一个依赖而不用显式的列出版本号。Maven会沿着父子层次向上走，直到找到一个拥有dependencyManagement元素的项目，然后它就会使用在这个dependencyManagement 元素中指定的版本号。 dependencyManagement例如，在父模块中的pom： 12345678910111213141516&lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;com.dm&lt;/groupId&gt; &lt;artifactId&gt;mt-model&lt;/artifactId&gt; &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.dm&lt;/groupId&gt; &lt;artifactId&gt;mt-tool&lt;/artifactId&gt; &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt; &lt;/dependency&gt; &lt;dependencies&gt; &lt;/dependencyManagement&gt; 然后我们在子模块里就可以这样引用mt-model和mt-tool: 12345678910&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;com.dm&lt;/groupId&gt; &lt;artifactId&gt;mt-model&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.dm&lt;/groupId&gt; &lt;artifactId&gt;mt-tool&lt;/artifactId&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 发现两者有什么区别了么？其实就是在子模块中引用pom没有添加版本号。 这样做的好处就是：如果有多个子项目都引用同一样依赖，则可以避免在每个使用的子项目里都声明一个版本号，这样当想升级或切换到另一个版本时，只需要在顶层父容器里更新，而不需要一个一个子项目的修改 ；另外如果某个子项目需要另外的一个版本，只需要声明version就可。dependencyManagement里只是声明依赖，并不实现引入，因此子项目需要显式的声明需要用的依赖。 dependencies相对于dependencyManagement，所有声明在dependencies里的依赖都会自动引入，并默认被所有的子项目继承。","categories":[],"tags":[{"name":"java","slug":"java","permalink":"https://ydstudios.gitee.io/tags/java/"},{"name":"maven","slug":"maven","permalink":"https://ydstudios.gitee.io/tags/maven/"}]},{"title":"Spring Boot 1.x 使用Value注解给静态变量赋值","slug":"Spring-Boot-1-x-使用Value注解给静态变量赋值","date":"2018-07-18T22:18:41.000Z","updated":"2019-12-09T15:03:26.000Z","comments":true,"path":"post/54acd1d8.html","link":"","permalink":"https://ydstudios.gitee.io/post/54acd1d8.html","excerpt":"昨天在使用@Value注解给静态变量赋值的时候，发现静态变量的值始终是null。后来搜索一下得知其中原因，Spring Boot 不允许/不支持把值注入到静态变量中。但是我们可以变通一下解决这个问题。因为Spring Boot支持set方法注入，我们可以利用非静态set方法注入静态变量。","text":"昨天在使用@Value注解给静态变量赋值的时候，发现静态变量的值始终是null。后来搜索一下得知其中原因，Spring Boot 不允许/不支持把值注入到静态变量中。但是我们可以变通一下解决这个问题。因为Spring Boot支持set方法注入，我们可以利用非静态set方法注入静态变量。 废话不多说，贴上我昨天写的代码： 123456789101112131415161718192021222324252627282930313233343536373839@Componentpublic class CoverImageUtil &#123; private static String endpoint; private static String bucketName; @Value(\"$&#123;oss.endpoint&#125;\") private void setEndpoint(String name)&#123; endpoint = name; &#125; @Value(\"$&#123;oss.bucketName&#125;\") private void setBucketName(String name)&#123; bucketName = name; &#125; public static String getImage(String path)&#123; if (StringUtils.isEmpty(path))&#123; return null; &#125; // XXX的图片地址 https://oss.XXX.com/uploads/8F/70/8F70879210F08AAA6F4A04A3D42F3704.jpg if (path.contains(\"oss.XXX.com\"))&#123; return path; &#125; String[] str = path.split(\",\"); // mt的图片地址 // key = customer/coverImg/1002,FAFA5EFEAF3CBE3B23B2748D13E629A1,418530,image/jpeg // url = https://m-t-tesing.oss-cn-hangzhou.aliyuncs.com/customer/coverImg/1002 StringBuilder url = new StringBuilder(\"https://\"); url.append(bucketName) .append(\".\") .append(endpoint) .append(\"/\") .append(str[0]); return url.toString(); &#125;&#125; 注意 代码中需要@Component注解 set方法要是非静态的","categories":[],"tags":[{"name":"java","slug":"java","permalink":"https://ydstudios.gitee.io/tags/java/"},{"name":"spring boot","slug":"spring-boot","permalink":"https://ydstudios.gitee.io/tags/spring-boot/"}]},{"title":"Mybatis方法传多个参数（三种解决方案）","slug":"Mybatis方法传多个参数（三种解决方案）","date":"2018-07-01T22:17:31.000Z","updated":"2019-12-09T15:03:26.000Z","comments":true,"path":"post/98398c3d.html","link":"","permalink":"https://ydstudios.gitee.io/post/98398c3d.html","excerpt":"Mybatis的Mapper接口的参数，一般是一个对象，但如果不是对象，并且有多个参数的时候我们应该怎样做呢？ 我们的第一个想法是把参数封装成一个java.util.Map类型，然后在方法的注释上面写上map的key是什么，但是，这样的做法明显不够直观，不能够清楚的看出方法的参数是什么，而且影响到了java的多态性（方法名相同，参数数量或类型不同）。","text":"Mybatis的Mapper接口的参数，一般是一个对象，但如果不是对象，并且有多个参数的时候我们应该怎样做呢？ 我们的第一个想法是把参数封装成一个java.util.Map类型，然后在方法的注释上面写上map的key是什么，但是，这样的做法明显不够直观，不能够清楚的看出方法的参数是什么，而且影响到了java的多态性（方法名相同，参数数量或类型不同）。 1.多个形参传递多参数Dao层的函数方法 1public User selectUser(String name,String area); 对应的Mapper.xml文件 123&lt;select id=\"selectUser\" resultMap=\"BaseResultMap\"&gt; select * from user_user where user_name = #&#123;0&#125; and user_area=#&#123;1&#125;&lt;/select&gt; 其中，#{0}代表接收的是dao层中的第一个参数，#{1}代表dao层中第二参数，更多参数以此类推即可。 2.采用Map传递多参数Dao层的函数方法 1public User selectUser(Map paramMap); 对应的Mapper.xml文件 123&lt;select id=\"selectUser\" resultMap=\"BaseResultMap\"&gt; select * from user_user where user_name = #&#123;userName,jdbcType=VARCHAR&#125; and user_area=#&#123;userArea,jdbcType=VARCHAR&#125;&lt;/select&gt; Service层调用 123456private User SelectUser()&#123; Map paramMap = new hashMap(); paramMap.put(“userName”,”对应具体的参数值”); paramMap.put(“userArea”,”对应具体的参数值”); User user = xxx.selectUser(paramMap); &#125; 这种方法不够直观，看到接口方法不能直接的知道需要传递的参数有哪些？ 3.使用@param注解Dao层的函数方法 1public User selectUser(@Param(“userName”)String name,@Param(“userArea”)String area); 对应的Mapper.xml文件 123&lt;select id=\"selectUser\" resultMap=\"BaseResultMap\"&gt; select * from user_user where user_name = #&#123;userName，jdbcType=VARCHAR&#125;and user_area=#&#123;userArea,jdbcType=VARCHAR&#125;&lt;/select&gt; 这种方法最好，能让开发者看到dao层方法就知道该传什么样的参数，在xml中也相比其他两种方法清楚。","categories":[],"tags":[{"name":"java","slug":"java","permalink":"https://ydstudios.gitee.io/tags/java/"},{"name":"mybatis","slug":"mybatis","permalink":"https://ydstudios.gitee.io/tags/mybatis/"}]},{"title":"Java学习系列文章第六篇：项目开发利器-Maven","slug":"Java学习系列文章第六篇：项目开发利器-Maven","date":"2018-06-29T22:03:43.000Z","updated":"2019-12-09T15:03:26.000Z","comments":true,"path":"post/1c04f2f4.html","link":"","permalink":"https://ydstudios.gitee.io/post/1c04f2f4.html","excerpt":"什么是MavenMaven是现在的Java程序员应该都会遇到或使用的一个工具。那Maven具体是什么？到底能干什么？ Maven是一个项目管理工具，用来管理项目的生命周期，如项目中各个项目之间的依赖管理，项目中使用到的jar包依赖管理，还有许多项目构建的插件等。使用Maven最大的好处就是不再需要我们手工维护项目中的各种jar包，及jar包需要依赖的其他jar包，也不再需要我们解决各种jar包之间的冲突，这一切maven帮我们都做好了。 目前Java开发中流行的集成开发工具Idea已经集成了Maven工具，打开Idea的设置选择Maven项，就可以看到自带的Maven的一些信息。我觉得Idea自带的Maven就挺好的，就没有去再安装一个。但是之前不知道Idea自带Maven工具，为此有次出现一个问题，就是因为两个Maven仓库的原因。在公司的电脑上我自己装了一个Maven，自己的电脑上没有装，直接使用Idea自带的Maven。","text":"什么是MavenMaven是现在的Java程序员应该都会遇到或使用的一个工具。那Maven具体是什么？到底能干什么？ Maven是一个项目管理工具，用来管理项目的生命周期，如项目中各个项目之间的依赖管理，项目中使用到的jar包依赖管理，还有许多项目构建的插件等。使用Maven最大的好处就是不再需要我们手工维护项目中的各种jar包，及jar包需要依赖的其他jar包，也不再需要我们解决各种jar包之间的冲突，这一切maven帮我们都做好了。 目前Java开发中流行的集成开发工具Idea已经集成了Maven工具，打开Idea的设置选择Maven项，就可以看到自带的Maven的一些信息。我觉得Idea自带的Maven就挺好的，就没有去再安装一个。但是之前不知道Idea自带Maven工具，为此有次出现一个问题，就是因为两个Maven仓库的原因。在公司的电脑上我自己装了一个Maven，自己的电脑上没有装，直接使用Idea自带的Maven。 Maven仓库使用 Maven 给我们带来的最直接的帮助，就是 jar 包得到了统一管理，那么这些 jar 包存放在哪里呢？它们就在您的 本地仓库 中，位于 C:\\Users\\用户名.m2 目录下（当然也可以修改这个默认地址）。 实际上可将本地仓库理解“缓存”，因为项目首先会从本地仓库中获取 jar 包，当无法获取指定 jar 包的时候，本地仓库会从 远程仓库（或 中央仓库） 中下载 jar 包，并放入本地仓库中以备将来使用。这个远程仓库是 Maven 官方提供的，可通过 http://search.maven.org/ 来访问。这样一来，本地仓库会随着项目的积累越来越大。通过下面这张图可以清晰地表达项目、本地仓库、远程仓库之间的关系。 Maven命令Maven执行命令有两种方式： Interactive Mode（交互模式） Batch Mode（批处理模式） 交互模式创建项目执行下面的命令 1mvn archetype:generate 接下来会出现下面的选项： 项目 Archetype Version（原型版本号）是什么？—— 可选择 1.0 版本 项目 groupId（组织名） 是什么？—— 可输入 net.ydstudio 项目 artifactId（构件名）是什么？—— 可输入 maven-demo 项目 version（版本号）是什么？—— 可输入 1.0 项目 package（包名）是什么？—— 可输入 net.ydstudio.demo 使用批处理模式创建项目1mvn archetype:generate -DinteractiveMode=false -DarchetypeArtifactId=maven-archetype-webapp -DgroupId=net.ydstudio -DartifactId=maven-demo -Dversion=1.0 创建项目这事我觉得还是用Idea来做，方便快捷。用命令创建的话，命令有时记得不全，不是很方便。 创建成功之后的项目中一般会有下面的几个目录，如果没有的话就自行创建。 src/main/java，主要代码存放的地方 src/test/java，测试代码存放的地方 src/main/resources，配置、资源文件存放的地方 src/main/webapp,Web应用相关代码存放的地方 还有一个重要的文件： pom.xml；pom.xml称为Project Object Model（项目对象模型），它用于描述整个Maven项目，所以也称为Maven描述文件。pom.xml 才是理解 Maven 的关键点，很有必要看看它到底长什么样。 pom文件打开生成的pom文件，内容如下： 1234567891011121314151617181920212223242526&lt;project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd\"&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;net.ydstudio&lt;/groupId&gt; &lt;artifactId&gt;maven-demo&lt;/artifactId&gt; &lt;version&gt;1.0&lt;/version&gt; &lt;packaging&gt;war&lt;/packaging&gt; &lt;name&gt;maven-demo Maven Webapp&lt;/name&gt; &lt;url&gt;http://maven.apache.org&lt;/url&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;3.8.1&lt;/version&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;finalName&gt;maven-demo&lt;/finalName&gt; &lt;/build&gt;&lt;/project&gt; 我们来解释一下： modelVersion:这个是pom的版本号，现在都是4.0.0的，必须得有，但不需要修改。 groupId、artifactId、version：分别表示 Maven 项目的组织名、构件名、版本号，它们三个合起来就是 Maven 坐标，根据这个坐标可以在 Maven 仓库中对应唯一的 Maven 构件。 packaging：表示该项目的打包方式，war 表示打包为 war 文件，默认为 jar，表示打包为 jar 文件。 name、url：表示该项目的名称与 URL 地址，意义不大，可以省略。 dependencies：定义该项目的依赖关系，其中每一个 dependency 对应一个 Maven 项目，可见 Maven 坐标再次出现，还多了一个 scope，表示作用域（下面会描述）。 build：表示与构建相关的配置，这里的 finalName 表示最终构建后的名称 maven-demo.war，这里的 finalName 还可以使用另一种方式来定义（下面会描述）。 我们可以在 pom.xml中定义一些列的项目依赖（构件包），每个构件包都会有一个 Scope（作用域），它表示该构件包在什么时候起作用，包括以下五种： compile：默认作用域，在编译、测试、运行时有效 test：对于测试时有效 runtime：对于测试、运行时有效 provided：对于编译、测试时有效，但在运行时无效 system：与 provided 类似，但依赖于系统资 Maven常见命令前面我们已经使用了几个 Maven 命令，例如：mvn archetype:generate，mvn tomcat7:run-war 等。其实，可使用两种不同的方式来执行 Maven 命令： mvn &lt;插件&gt;:&lt;目标&gt; [参数] mvn &lt;阶段&gt;现在我们接触到的都是第一种方式，而第二种方式才是我们日常中使用最频繁的，例如： mvn clean：清空输出目录（即 target 目录） mvn compile：编译源代码 mvn package：生成构件包（一般为 jar 包或 war 包） mvn install：将构件包安装到本地仓库 mvn deploy：将构件包部署到远程仓库 执行 Maven 命令需要注意的是：必须在Maven项目的根目录处执行，也就是当前目录下一定存在一个名为 pom.xml 的文件。","categories":[],"tags":[{"name":"java","slug":"java","permalink":"https://ydstudios.gitee.io/tags/java/"},{"name":"maven","slug":"maven","permalink":"https://ydstudios.gitee.io/tags/maven/"}]},{"title":"Java学习系列文章第五篇：说说Java中的异常","slug":"Java学习系列文章第五篇：说说Java中的异常","date":"2018-06-29T22:00:54.000Z","updated":"2019-12-09T15:03:26.000Z","comments":true,"path":"post/4b7e7a69.html","link":"","permalink":"https://ydstudios.gitee.io/post/4b7e7a69.html","excerpt":"在正常的程序设计中，程序异常处理是非常关键和重要的一部分。试想一个项目中没有一个好的异常处理，这个项目会怎么样？","text":"在正常的程序设计中，程序异常处理是非常关键和重要的一部分。试想一个项目中没有一个好的异常处理，这个项目会怎么样？ 什么是异常异常其实是程序上的错误，包括程序逻辑错误和系统错误。比如数组下标越界、内存溢出等，这些都是意外的情况，错误在我们的程序的编写过程中会经常发生，包括编译期间和运行期间的错误。在编译期间出现的错误编译器会帮助我们修正，可是在运行期间的错误编译器就无能为力了，并且运行期间的错误往往是难以预料的。 程序出现了错误，我们不能不去处理，这样的程序的健壮性太差了。为了提高程序的健壮性我们要合理的解决这些错误！于是Java中提供了异常的处理机制，通过异常来处理程序运行期间中出现的错误。通过这一特性，我们可以很好的提高程序的健壮性。 Java是一个全面的面向对象语言，不像PHP那样，既支持过程式编程，也支持面向对象编程。Java中异常的父类是java.lang.Throwable类。在Java中定义很多的的异常类，比如OutOfMenoryError、NullPointerException、IndexOutOfBoundsException等。 Exception 类的层次所有的异常类是从 java.lang.Exception 类继承的子类。 Exception 类是 Throwable类的子类。除了Exception类外，Throwable还有一个子类Error 。Java 程序通常不捕获错误。错误一般发生在严重故障时，它们在Java程序处理的范畴之外。 Error 用来指示运行时环境发生的错误。例如，JVM 内存溢出。一般地，程序不会从错误中恢复。异常类有两个主要的子类：IOException 类和 RuntimeException 类。 Exception，也就是我们经常见到的一些异常情况，例如NullPointerException、IndexOutOfBoundsException等，这些异常时是我们可以处理的异常。 Exception类的异常包括checked exception和unchecked exception（unchecked exception也称作运行时异常RuntimeException，Exception类的异常都是在运行期间发生的），对于运行时异常，Java编译器不要求必须进行异常处理捕获处理或者抛出，这个由程序员自行决定。 checked exception（检查异常），也称为非运行时异常（运行时异常以外的异常就是非运行时异常），Java编译器强制程序员必须捕获处理，比如常见的IOException和SQLException。对于非运行时异常如果不进行捕获或者抛出处理，Java编译器都不会通过。 在网上找了一个图，能够很清楚的描述在Java中，异常类的结构层次（有些时候语言就略显苍白，不如图片或者视频表现力丰富）。 在Java中，所有的异常都是继承至java.lang.Throwable类。Error类是error类型异常的父类，Exception类是exception类型异常的父类，RuntimeException类是所有运行时异常的父类，RuntimeException以外的并且继承Exception的类是非运行时异常。 典型的RuntimeException包括NullPointerException、IndexOutOfBoundsException、IllegalArgumentException等。 典型的非RuntimeException包括IOException、SQLException等。 Java如何处理异常在Java中如果需要处理异常，必须先对异常进行捕获（这一点是和PHP是相同的）。使用try和catch关键字进行处理。具体的规则如下： 123456789try&#123; // 程序代码&#125;catch(异常类型1 异常的变量名1)&#123; // 程序代码&#125;catch(异常类型2 异常的变量名2)&#123; // 程序代码&#125;finally&#123; // 程序代码&#125; 来段实际的代码： 12345678910111213import java.io.*;public class ExcepTest&#123; public static void main(String args[])&#123; try &#123; File file = new File(\"/Users/sam/a.txt\"); if(!file.exists()) file.createNewFile(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125;&#125; 被try块包围的代码说明这段代码可能会发生异常，一旦发生异常，异常便会被catch捕获到，然后需要在catch块中进行异常处理。这是一种处理异常的方式。在Java中还提供了另一种异常处理方式即抛出异常，顾名思义，也就是说一旦发生异常，我把这个异常抛出去，让调用者去进行处理，自己不进行具体的处理，此时需要用到throw和throws关键字。 我们看下面的代码： 123456789101112131415public class Main &#123; public static void main(String[] args) &#123; try &#123; createFile(); &#125; catch (Exception e) &#123; // TODO: handle exception &#125; &#125; public static void createFile() throws IOException&#123; File file = new File(\"/Users/sam/a.txt); if(!file.exists()) file.createNewFile(); &#125;&#125; 这段代码和上面一段代码的区别是，在实际的createFile方法中并没有捕获异常，而是用throws关键字声明抛出异常，即告知调用者此方法可能会抛出IOException，需要调用者进行捕获处理。那么在main方法中调用createFile方法的时候，采用try…catch块进行了异常捕获处理。 还可以使用throw关键字进行抛出异常。看下面的例子： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566package exception;/** * Created by Sam on 18/6/17. */public class Main &#123; public static void main(String[] args)&#123; People people = new People(\"Sam\",new Byte(\"25\")); people.sayAge(); &#125;&#125; class People &#123; private String name; private Byte age; public People(String name, Byte age) &#123; this.name = name; this.age = age; &#125; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; public Byte getAge() &#123; return age; &#125; public void setAge(Byte age) &#123; this.age = age; &#125; public Byte sayAge()&#123; if (age &gt; Byte.MIN_VALUE)&#123; throw new MyException(\"年龄太大了\",\"100\" ); &#125; return age; &#125;&#125;public class MyException extends RuntimeException &#123; private String code; public MyException(String message, String code) &#123; super(message); this.code = code; &#125; public String getCode() &#123; return code; &#125; public void setCode(String code) &#123; this.code = code; &#125;&#125; 上面就是利用throw关键字进行手动抛出异常。调用者可以捕获处理异常，也可以不用处理异常。下面我们就修改一下代码进行捕获处理： 1234567891011121314public class Main &#123; public static void main(String[] args)&#123; People people = new People(\"Sam\",new Byte(\"25\")); try &#123; people.sayAge(); &#125;catch (Exception e)&#123; System.out.println(e); &#125; &#125;&#125;程序输出：exception.MyException: 年龄太大了 也就说在Java中进行异常处理的话，对于可能会发生异常的代码，可以选择三种方法来进行异常处理： 1、对代码块用try..catch进行异常捕获处理； 2、在该代码的方法体外用throws进行抛出声明，告知此方法的调用者这段代码可能会出现这些异常，你需要谨慎处理。此时有两种情况： 如果声明抛出的异常是非运行时异常，此方法的调用者必须显示地用try..catch块进行捕获或者继续向上层抛出异常。 如果声明抛出的异常是运行时异常，此方法的调用者可以选择地进行异常捕获处理。 3、在代码块用throw手动抛出一个异常对象，此时也有两种情况，跟2）中的类似： 如果抛出的异常对象是非运行时异常，此方法的调用者必须显示地用try..catch块进行捕获或者继续向上层抛出异常。 如果抛出的异常对象是运行时异常，此方法的调用者可以选择地进行异常捕获处理。（如果最终将异常抛给main方法，则相当于交给jvm自动处理，此时jvm会简单地打印异常信息） 关于Java的异常机制就暂时说到这里。","categories":[],"tags":[{"name":"java","slug":"java","permalink":"https://ydstudios.gitee.io/tags/java/"},{"name":"exception","slug":"exception","permalink":"https://ydstudios.gitee.io/tags/exception/"}]},{"title":"Java学习系列文章第四篇：说说Java的三大特性","slug":"Java学习系列文章第四篇：说说Java的三大特性","date":"2018-06-29T21:58:31.000Z","updated":"2019-12-09T15:03:26.000Z","comments":true,"path":"post/2b809338.html","link":"","permalink":"https://ydstudios.gitee.io/post/2b809338.html","excerpt":"Java语言提供类、接口和继承等面向对象的特性，为了简单起见，只支持类之间的单继承，但支持接口之间的多继承，并支持类与接口之间的实现机制（关键字为implements）。Java语言全面支持动态绑定，而C++语言只对虚函数使用动态绑定。总之，Java语言是一个纯的面向对象程序设计语言。","text":"Java语言提供类、接口和继承等面向对象的特性，为了简单起见，只支持类之间的单继承，但支持接口之间的多继承，并支持类与接口之间的实现机制（关键字为implements）。Java语言全面支持动态绑定，而C++语言只对虚函数使用动态绑定。总之，Java语言是一个纯的面向对象程序设计语言。 说到面向对象编程，那就不得不提面向对象编程的三大特性：继承、封装和多态。那下面我们先来说一说封装。 封装隐藏了类的内部实现机制，可以在不想影响使用的情况下改变类的内部结构，同时也保护了数据。对外界也可以隐藏内部的细节，只暴露给外界访问的方法。 继承可以复用父类的代码。两个类之间若存在is-a的关系，就可以使用继承，从而达到代码的复用，同时也给多态的实现做了铺垫。 下面我们只要说说面向对象编程中的多态。所谓的多态就是指程序中定义的引用变量所指向的具体类型和通过该引用变量发出的方法调用在编程时并不确定，而是在程序的运行期间才确定，即一个引用变量到底会指向哪个类的实例对象，该引用变量发出的方法调用到底是哪个类中实现的方法，必须在程序运行期间才能确定，这就是所谓的动态绑定（dynamic binding）。 既然多态那么重要，那么多态到底有什么好处呢？ 可替换性 消除类型之间的耦合关系 可扩充性 接口性 灵活性 简化性 Java实现多态有三个必要的条件：继承、重写、向上转型 继承：在多态中必须存在有继承关系的子类和父类。 重写：子类对父类中某些方法进行重写定义，在调用这些方法时就会调用子类的方法。 向上转型：在多态中需要将父类的引用指向子类，只有这样该引用才能够具备调用父类和子类的方法。 多态的实现方式 1. 继承重写12345678910111213141516171819202122232425262728293031323334353637383940414243public abstract class Animal &#123; abstract void eat(); abstract void work();&#125;public class Cat extends Animal &#123; @Override public void eat() &#123; System.out.println(\"吃鱼\"); &#125; @Override public void work() &#123; System.out.println(\"抓老鼠\"); &#125;&#125;public class Dog extends Animal &#123; @Override public void eat() &#123; System.out.println(\"吃骨头\"); &#125; @Override public void work() &#123; System.out.println(\"看家\"); &#125;&#125;public class Test &#123; public static void main(String[] args)&#123; Animal a = new Cat(); // 向上转型 a.eat(); // 调用的是 Cat 的 eat Animal b = new Dog(); // 向上转型 b.eat(); // 调用的是 Dog 的 eat &#125;&#125; 所以基于继承实现的多态可以总结如下：对于引用子类的父类类型，在处理该引用时，它适用于继承该父类的所有子类，子类对象的不同，对方法的实现也就不同，执行相同动作产生的行为也就不同。如果父类是抽象类，那么子类必须要实现父类中所有的抽象方法，这样该父类所有的子类一定存在统一的对外接口，但其内部的具体实现可以各异。这样我们就可以使用顶层类提供的统一接口来处理该层次的方法。 2.基于接口实现多态继承是通过重写父类的同一方法的几个不同子类来体现的，那么就可就是通过实现接口并覆盖接口中同一方法的几不同的类体现的。 在接口的多态中，指向接口的引用必须是指定这实现了该接口的一个类的实例程序，在运行时，根据对象引用的实际类型来执行对应的方法。 继承都是单继承，只能为一组相关的类提供一致的服务接口。但是接口可以是多继承多实现，它能够利用一组相关或者不相关的接口进行组合与扩充，能够对外提供一致的服务接口。所以它相对于继承来说有更好的灵活性。","categories":[],"tags":[{"name":"java","slug":"java","permalink":"https://ydstudios.gitee.io/tags/java/"},{"name":"oop","slug":"oop","permalink":"https://ydstudios.gitee.io/tags/oop/"}]},{"title":"Java学习系列文章第三篇：说说equals和==","slug":"Java学习系列文章第三篇：说说equals和","date":"2018-06-29T21:49:25.000Z","updated":"2019-12-09T15:03:26.000Z","comments":true,"path":"post/28107323.html","link":"","permalink":"https://ydstudios.gitee.io/post/28107323.html","excerpt":"在上一篇文章中我们写过这样的代码 123456789101112// 直接赋值String str1 = \"hello world\";// 构造方法String str2 = new String(\"hello world\");String str3 = \"hello world\";String str4 = new String(\"hello world\");System.out.println(str1==str2);System.out.println(str1==str3);System.out.println(str2==str4); 我们现在都知道了它们的输出的结果，分别是false、true和false。","text":"在上一篇文章中我们写过这样的代码 123456789101112// 直接赋值String str1 = \"hello world\";// 构造方法String str2 = new String(\"hello world\");String str3 = \"hello world\";String str4 = new String(\"hello world\");System.out.println(str1==str2);System.out.println(str1==str3);System.out.println(str2==str4); 我们现在都知道了它们的输出的结果，分别是false、true和false。 ==是在比较什么在《think in java》这本书里有这样的一句话“关系操作符生成的是一个布尔的结果，它们计算的是操作数之间值的关系”。简单的说就是==号是用来比较值是否相等。 1234567891011121314151617181920public class Main &#123; public static void main(String[] args)&#123; int n = 30; int m = 30; System.out.println(n == m); String str = new String(\"hello world\"); String str1 = new String(\"hello world\"); String str2 = new String(\"hello world\"); System.out.println(str1 == str2); str1 = str; str2 = str; System.out.println(str1 == str2); &#125;&#125; 上面代码的输出结果可能大家都知道是true、false和true。也许会有人会有疑问，那我们来好好的解释一下： n == m结果为true，这个很容易理解，变量n和变量m存储的值都为3，肯定是相等的。而为什么str1和str2两次比较的结果不同？要理解这个其实只需要理解基本数据类型变量和非基本数据类型变量的区别。对于Java中的8中基本类型（4种整型byte、short、int、long，2种浮点型float、double，一个boolean型，还有一个字符型char）的变量，变量本身存储的就是值，关系操作符==进行比较时，比较的就是值本身。所以上面的 n == m的比较就是3 == 3，这个肯定结果是true啊！ 对于那些引用类型的变量如str1，变量存储的并不是值本身，而是其关联对象在内存的地址。所以变量str1中存储的是它指向的对象在内存中的存储地址，并不是“值”本身，也就是说并不是直接存储的字符串”hello world”。这里面的引用和C/C++中的指针很类似。因此在用==对str1和str2进行第一次比较时，得到的结果是false。因此它们分别指向的是不同的对象，也就是说它们实际存储的内存地址不同。而在第二次比较时，都让str1和str2指向了str指向的对象，那么得到的结果毫无疑问是true。 equals又是在比较什么equals方法是基类Object中的方法，因此对于所有继承Object的类都有该方法。先看一下下面的代码，猜猜输出的结果是什么 12345678910111213public class Main &#123; public static void main(String[] args)&#123;; Object object1 = new Object(); Object object2 = new Object(); Object object3 = object1; System.out.println(object1.equals(object2)); System.out.println(object1.equals(object3)); &#125;&#125; 上面代码的输出结果是false、true。我们来看看Object类中equals源码是什么样的： 123public boolean equals(Object obj) &#123; return (this == obj);&#125; 在Object类中的equals方法是用来比较两个对象的引用是否相等，即是两个变量的引用是否指向同一个对象。 那下面的代码肯定会有人有疑问了，为啥输出的结果是true 123456789101112public class Main &#123; public static void main(String[] args)&#123;; String str1 = new String(\"hello\"); String str2 = new String(\"hello\"); System.out.println(str1.equals(str2)); &#125;&#125; 要想知道是为什么，我们查看一下String类的equals的源码就知道是怎么回事了！ 123456789101112131415161718192021public boolean equals(Object anObject) &#123; if (this == anObject) &#123; return true; &#125; if (anObject instanceof String) &#123; String anotherString = (String)anObject; int n = value.length; if (n == anotherString.value.length) &#123; char v1[] = value; char v2[] = anotherString.value; int i = 0; while (n-- != 0) &#123; if (v1[i] != v2[i]) return false; i++; &#125; return true; &#125; &#125; return false;&#125; String类对equals方法进行了重写，用来比较指向的字符串对象所存储的字符串是否相等。 Java中其他的类例如Double、Date、Integer等，都对equals进行重写，来比较指向的对象存储的内容是否相等。 我们来总结一下： 对于==，如果作用于基本数据类型的变量，则直接比较两操作数的值是否相等；如果作用于引用类型的变量，则比较的引用变量所指向的对象的地址。 对于equals，如果类对equals没有进行重写，则比较的是引用类型的变量所指向的对象地址；诸如String类对equals方法进行了重写，比较的则是引用变量所指向的对象的内容。 注意equals方法不能作用于基本数据类型的变量","categories":[],"tags":[{"name":"java","slug":"java","permalink":"https://ydstudios.gitee.io/tags/java/"},{"name":"String","slug":"String","permalink":"https://ydstudios.gitee.io/tags/String/"}]},{"title":"Java学习系列文章第一篇：基本变量类型","slug":"Java学习系列文章第一篇：基本变量类型","date":"2018-06-29T21:48:24.000Z","updated":"2019-12-09T15:03:26.000Z","comments":true,"path":"post/c4de9814.html","link":"","permalink":"https://ydstudios.gitee.io/post/c4de9814.html","excerpt":"Java有着丰富的变量类型，有8种基本数据类型，以及他们对应的包装类型，还有数组集合等众多变量类型。相比之下PHP的数据类型就相对少多了，PHP只有布尔类型（Boolean）、整型（Integer）、浮点型（Float）、数组（Array）、字符串（String）、对象（Object）、资源类型（Resource）和NULL。本文我们先以基本数据类型为主，其他的类型为辅。","text":"Java有着丰富的变量类型，有8种基本数据类型，以及他们对应的包装类型，还有数组集合等众多变量类型。相比之下PHP的数据类型就相对少多了，PHP只有布尔类型（Boolean）、整型（Integer）、浮点型（Float）、数组（Array）、字符串（String）、对象（Object）、资源类型（Resource）和NULL。本文我们先以基本数据类型为主，其他的类型为辅。 12type identifier [ = value][, identifier [= value] ...] ;格式说明：type为Java数据类型。identifier是变量名。可以使用逗号隔开来声明多个同类型变量。 下面我就简单的举几个变量声明的例子： 12345byte a,b,c;short e = g = 10;int f = 1000;boolean h = false;char i = 'a'; Java的两大数据类型： 内置数据类型 引用数据类型内置数据类型Java语言提供了八种基本数据类型。六种数字类型（四个整数型，两个浮点型），一种字符类型，还有一种是布尔型。Java中的数字类型都是无符号（unsigned）的，不像C语言和C++那样还提供有符号的类型。在后续的MySQL学习中，你会发现MySQL中字段的类型里面会有无符号这个属性，这样就会出现MySQL的字段映射成Java变量时，会有取值范围不同的问题。byte byte 占用一个字节的内存； 最小值是-128（-2^7)， 最大值是127（2^7-1）； 默认值是0，对应的包装类型是Byte,包装类型的默认值是null； 例子：byte a = 10； short short 占用两个字节的内存； 最小值是-32768（-2^15）， 最大值是32767（2^15-1） 默认值是0，对应的包装类型是Short,默认值是null； 例子：short a = 1200； char char 占用两个字节的内存，采用 Unicode 编码； 最小值是 \\u0000（即为0）； 最大值是 \\uffff（即为65,535）； char 可以储存任何字符（部分生僻的中文字符可能不行）； 例子：char letter = ‘A’;。 int int 占用四个字节的内存； 最小值是-2,147,483,648（-2^31）， 最大值是2,147,483,6478（2^31-1） 默认值是0，对应的包装类型是 Integer，默认值是null； 一般的整型变量默认为 int 类型 例子：int a = -10020； long long 占用八个字节的内存； 最小值是 -9,223,372,036,854,775,808（-2^63），最大值是 9,223,372,036,854,775,807（2^63-1），这种类型主要使用在需要比较大整数的系统上； 默认值是 0L，对应的包装类型是 Long，默认值是null； 例子：long a = 100000L，Long b = -200000L。“L”理论上不分大小写，但是若写成”l”容易与数字”1”混淆，不容易分辩。所以最好大写。 float float 占用四个字节的内存； 默认值是 0.0f；浮点数不能用来表示精确的值，如货币，存储货币推荐使用 BigDecimal；对应的包装类型是Float，默认值是null； 例子：float f1 = 234.58f； double double 占用八个字节的内存； 默认值是 0.0d；double不能用来表示精确的值，如货币，存储货币推荐使用BigDecimal；对应的包装类型是Double，默认值是null； 例子：double d1 = 234.05d；double d2 = 467.90;注意：不带任何标志的浮点型数据，系统默认是double类型。 boolean boolean(JVM规范没有明确规定其所占的空间大小； 只有两个取值，true和false; 默认值false,对应的包装类型是Boolean，默认值是null； 例子： boolean b = false; void 对应包装类型Void，不常用 byte、int、long和short还可以使用16进制和8进制的方式标识，默认都是使用10进制。 123int decimal = 100;int octal = 0144;int hexa = 0x64; 自动类型转换整型和字符型数据可以混合运算，不同的类型的数据先转化成同一类型，然后进行计算。 低———————————&gt;高 byte-&gt;short,char—&gt; int —&gt;float —&gt; long—&gt; double 数据类型转换必须满足下面的规则： 不能对boolean类型进行类型转换 不能把对象类型转换成不相干类的对象 把类型范围大的转换成类型范围小的时候必须使用强制类型转换 强制转换的过程中会出现溢出或损失精度 引用类型 在Java中引用类型变量指向一个对象，这种变量在定义的时候，被声明成特定的类型，比如 Man、Student等。变量类型一但确定之后类型就不能改变。这一点和动态脚本语言PHP有着很大的不同，PHP中的变量定义后仍能保存其他类型的变量； 所用引用类型的默认值都是null； 写于：2018-06-06 14:56 修改：2018-06-06 22:56","categories":[],"tags":[{"name":"java","slug":"java","permalink":"https://ydstudios.gitee.io/tags/java/"},{"name":"String","slug":"String","permalink":"https://ydstudios.gitee.io/tags/String/"}]},{"title":"Java学习系列文章第二篇：字符串","slug":"Java学习系列文章第二篇：字符串","date":"2018-06-29T21:44:13.000Z","updated":"2019-12-09T15:03:26.000Z","comments":true,"path":"post/9d613e12.html","link":"","permalink":"https://ydstudios.gitee.io/post/9d613e12.html","excerpt":"在众多的编程语言里面，字符串都被广泛的使用。在Java中字符串属于对象，语言提供了String类来创建和操作字符串。","text":"在众多的编程语言里面，字符串都被广泛的使用。在Java中字符串属于对象，语言提供了String类来创建和操作字符串。 字符串String简单知识Java提供两种方式来定义字符串，例如： 123456定义字符使用单引号，定义字符串使用双引号；// 直接赋值String str1 = \"hello world\";// 构造方法String str2 = new String(\"hello world\"); 通过对String源码的查看： 12345678910111213public final class String implements java.io.Serializable, Comparable&lt;String&gt;, CharSequence &#123; /** The value is used for character storage. */ private final char value[]; /** Cache the hash code for the string */ private int hash; // Default to 0 /** use serialVersionUID from JDK 1.0.2 for interoperability */ private static final long serialVersionUID = -6849794470754667710L; ……&#125; 从上面的代码我们可以得出两点结论： Java中的String类被final修饰。在Java中被final修饰的类不允许被继承，并且成员方法默认被final修饰。在早期的JVM的版本，被final修饰的方法会被转为内嵌调用借此来提升执行效率，但是从Java1.5/6之后，这种方式就被取消了。在之后的版本里，final修饰类只是为了不让类被继承。 String类是通过char数组保存字符串的。 对字符串的每一次操作，例如连接子串都会重新创建一个新的String对象。我们可以从String中的concat方法源码中可以看出这一点，代码如下： 12345678910public String concat(String str) &#123; int otherLen = str.length(); if (otherLen == 0) &#123; return this; &#125; int len = value.length; char buf[] = Arrays.copyOf(value, len + otherLen); str.getChars(buf, len); return new String(buf, true);&#125; 当被连接的子串的长度为0时，直接返回自身，连接一个长度不为0的子串，通过char数组的系列操作，重新生成一个新的String对象。所以在此要注意对String类对象的任何改变都不影响到原对象，相关的任何change操作都会生成新的对象。 深入理解字符串String上面写了两种定义字符串的方式，不知道大家知道这两种方式的区别和联系么？ 123456789101112// 直接赋值String str1 = \"hello world\";// 构造方法String str2 = new String(\"hello world\");String str3 = \"hello world\";String str4 = new String(\"hello world\");System.out.println(str1==str2);System.out.println(str1==str3);System.out.println(str2==str4); 你能直接说出上面的执行结果么？如果不能请继续往下看，能的话也请继续往下看。具体的结果如下： 123falsetruefalse 在class文件中有一部分来存储编译期间生成的字面常量以及符号引用，这部分叫做class文件常量池，在运行期间对应着方法区的运行时常量池。在上述的代码中String str1 = “hello world”;和String str2 = new String(“hello world”);都在编译期生成了字面常量和符号引用，运行期间字面常量”hello world”都被存储在运行时常量池。JVM执行引擎会在运行时常量池中查找是否存在相同的字面常量，若有则直接将引用指向已经存在的字面常量；否则在运行时常量池中开辟一个新的空间来存储该字面量，并将引用指向该字面常量，通过这种方式来把String对象跟引用绑定。 通过new关键字生成对象这个过程是在堆heap中进行的，而在堆进行对象生成过程中，不会有检查对象是否已经存在这个行为。因此通过new来创建对象，创建出来的一定是新的对象，即在内存中有着新的内存地址，但字符串的内容是相同的。 下面是Java中不同变量在内存中存放的位置： 变量 内存位置 new出来的对象 heap 堆 局部变量、基本数据类型 stack 栈 静态变量、字符串、常量 data segment 数据区 代码 code segment 代码区 String、StringBuffer、StringBuilder的区别为什么已经存在了String了，还会出现StringBuffer、StringBuilder？如果一个字符串需要连接10000次其他的字符串，实现代码如下： 1234567891011public class Main &#123; public static void main(String[] args)&#123; String string = \"\"; for(int i=0;i&lt;10000;i++)&#123; string = string.concat(\"hello\"); &#125; &#125;&#125; 上述代码不断的new字符串对象，前面已经说了重要的一点对String类对象的任何改变都不影响到原对象，相关的任何change操作都会生成新的对象。，这种代码将会有多大的内存消耗。这个时候想必大家已经有了点答案。我将上述的代码稍微的修改一下： 1234567891011public class Main &#123; public static void main(String[] args)&#123; String string = \"\"; for(int i=0;i&lt;10000;i++)&#123; string += \"hello\"; &#125; &#125;&#125; 两部分代码看似只有一点差异，其实两者的内存消耗有着天大的差别。我们通过javap命令来反编译.class文件。具体内容如下： 12345678910111213141516171819202122232425262728293031D:\\work\\javaLearn\\out\\production\\javaLearn&gt;javap -c MainCompiled from \"Main.java\"public class Main &#123; public Main(); Code: 0: aload_0 1: invokespecial #1 // Method java/lang/Object.\"&lt;init&gt;\":()V 4: return public static void main(java.lang.String[]); Code: 0: ldc #2 // String 2: astore_1 3: iconst_0 4: istore_2 5: iload_2 6: sipush 10000 9: if_icmpge 38 12: new #3 // class java/lang/StringBuilder 15: dup 16: invokespecial #4 // Method java/lang/StringBuilder.\"&lt;init&gt;\":()V 19: aload_1 20: invokevirtual #5 // Method java/lang/StringBuilder.append:(Ljava/lang/String;)Ljava/lang/StringBuilder; 23: ldc #6 // String hello 25: invokevirtual #5 // Method java/lang/StringBuilder.append:(Ljava/lang/String;)Ljava/lang/StringBuilder; 28: invokevirtual #7 // Method java/lang/StringBuilder.toString:()Ljava/lang/String; 31: astore_1 32: iinc 2, 1 35: goto 5 38: return&#125; 从上面反编译出来的字节码中可以看出一点门道：string+=”hello”的操作事实上会自动被JVM优化成StringBuilder类的append操作。 那么有人会问既然有了StringBuilder类，为什么还需要StringBuffer类？查看源代码便一目了然，事实上，StringBuilder和StringBuffer类拥有的成员属性以及成员方法基本相同，区别是StringBuffer类的成员方法前面多了一个关键字：synchronized，不用多说，这个关键字是在多线程访问时起到安全保护作用的,也就是说StringBuffer是线程安全的。 我们来看下面的代码： 1234567891011public class Main &#123; public static void main(String[] args)&#123; String str1 = \"I \"+\"love \"+\"you\"; String str2 = \"I \"; String str3 = \"love \"; String str4 = \"you \"; String str5 = str2 + str3 + str4; &#125;&#125; 用javap命令来反编译.class文件： 1234567891011121314151617181920212223242526272829303132D:\\work\\javaLearn\\out\\production\\javaLearn&gt;javap -c MainCompiled from \"Main.java\"public class Main &#123; public Main(); Code: 0: aload_0 1: invokespecial #1 // Method java/lang/Object.\"&lt;init&gt;\":()V 4: return public static void main(java.lang.String[]); Code: 0: ldc #2 // String I love you 2: astore_1 3: ldc #3 // String I 5: astore_2 6: ldc #4 // String love 8: astore_3 9: ldc #5 // String you 11: astore 4 13: new #6 // class java/lang/StringBuilder 16: dup 17: invokespecial #7 // Method java/lang/StringBuilder.\"&lt;init&gt;\":()V 20: aload_2 21: invokevirtual #8 // Method java/lang/StringBuilder.append:(Ljava/lang/String;)Ljava/lang/StringBuilder; 24: aload_3 25: invokevirtual #8 // Method java/lang/StringBuilder.append:(Ljava/lang/String;)Ljava/lang/StringBuilder; 28: aload 4 30: invokevirtual #8 // Method java/lang/StringBuilder.append:(Ljava/lang/String;)Ljava/lang/StringBuilder; 33: invokevirtual #9 // Method java/lang/StringBuilder.toString:()Ljava/lang/String; 36: astore 5 38: return&#125; str1在编译之后就被直接赋值为”I love you”;str5却没有什么操作。综上所述我们可以得出一些结论： 对于直接通过加号相连字符串效率高，因为编译器直接确定了它的值。就像上面的”I “+”love “+”you”;的字符串相加，在编译期间就被优化成了”I love you“。 对于间接相加的，形如str2 + str3 + str4;编译期不会进行优化。 对于执行效率来说StringBuilder &gt; StringBuffer &gt; String，但这个也不是绝对的。比如String str = “hello”+ “world”的效率就比 StringBuilder st = new StringBuilder().append(“hello”).append(“world”)要高。但是，当字符串相加的操作或者字符改动的情况较少的时候，采用String肯定是比较好的；当字符串的操作较多的时候推荐使用StringBuilder，如果考虑到线程安全问题，无疑采用StringBuffer是最合适的。常见的字符串相关的面试题 下面的代码输出的结果是什么？123String a = \"hello2\"; String b = \"hello\" + 2; System.out.println((a == b)); 结果是true，它String b = “hello” + 2; 被编译器优化成了String b = “hello2”; 所以运行时字符串a和b指向同一个对象。 下面的代码输出的结果是什么？1234String a = \"hello2\"; String b = \"hello\"; String c = b + 2; System.out.println((a == c)); 输出结果为:false。由于有符号引用的存在，所以 String c = b + 2;不会在编译期间被优化，不会把b+2当做字面常量来处理的，通过StringBuilder生成了一个新的对象，因此这种方式生成的对象事实上是保存在堆上的。 下面的代码输出的结果是什么？1234String a = \"hello2\";final String b = \"hello\"; String c = b + 2; System.out.println((a == c)); 输出结果为：true。对于被final修饰的变量，会在class文件常量池中保存一个副本，也就是说不会通过连接而进行访问，对final变量的访问在编译期间都会直接被替代为真实的值。那么String c = b + 2;在编译期间就会被优化成：String c = “hello” + 2; 字符串的故事就暂时说到这里，后续有的话就继续更新。","categories":[],"tags":[{"name":"java","slug":"java","permalink":"https://ydstudios.gitee.io/tags/java/"},{"name":"String","slug":"String","permalink":"https://ydstudios.gitee.io/tags/String/"}]},{"title":"CentOS 6.x 下安装Zookeeper","slug":"CentOS-6-x-下安装Zookeeper","date":"2018-05-23T21:41:18.000Z","updated":"2019-12-09T15:03:26.000Z","comments":true,"path":"post/d229aadf.html","link":"","permalink":"https://ydstudios.gitee.io/post/d229aadf.html","excerpt":"目前在研究dubbo这个国内流行的RPC框架，实现去中心话的微服务需要用到Zookeeper,所以今天来说说如何安装Zookeeper。Zookeeper的官方下载地址","text":"目前在研究dubbo这个国内流行的RPC框架，实现去中心话的微服务需要用到Zookeeper,所以今天来说说如何安装Zookeeper。Zookeeper的官方下载地址 查看Centos版本： 12345cat /etc/issue[root@iZwz99xkrnh5xy0cqp8aofZ tomcat]# cat /etc/issueCentOS release 6.9 (Final)Kernel \\r on an \\m 1.下载安装包到/usr/local/src/目录下，并创建安装目录/usr/local/zookeeper。 1234cd /usr/local/src/#这里使用3.4.10，没有使用较高的版本wget https://mirrors.tuna.tsinghua.edu.cn/apache/zookeeper/zookeeper-3.4.10/zookeeper-3.4.10.tar.gzmkdir /usr/local/zookeeper 2.解压安装包,并将解压后的文件拷贝到/usr/local/zookeeper 12tar -zxvf zookeeper-3.4.10.tar.gzcp -R /usr/local/zookeeper/zookeeper-3.4.10/* /usr/local/zookeeper 3.添加系统变量到/etc/profile 123export ZOOKEEPER_HOME=/usr/local/zookeeperexport PATH=$ZOOKEEPER_HOME/bin:$PATHexport PATH 执行命令 source /etc/profile使配置文件立即生效 4.创建zookeeper配置文件,并作相应的配置 1234567891011121314151617181920cd /usr/local/zookeeper/conf/cp zoo_sample.cfg zoo.cfg#配置zookeeper#服务器与客户端之间交互的基本时间单元（ms）tickTime=2000#配置保存数据文件夹dataDir=/usr/local/zookeeper/data#配置保存日志文件夹，当此配置不存在时默认路径与dataDir一致dataLogDir=/var/log/zookeeper#客户端访问zookeeper的端口号clientPort=2181 5.zookeeper相关命令 1234启动服务：zkServer.sh start查看状态：zkServer.sh status关闭服务：zkServer.sh stop重启服务：zkServer.sh restart 至此，zookeeper安装完成","categories":[],"tags":[{"name":"linux","slug":"linux","permalink":"https://ydstudios.gitee.io/tags/linux/"},{"name":"zookeeper","slug":"zookeeper","permalink":"https://ydstudios.gitee.io/tags/zookeeper/"}]},{"title":"CentOS 6.x 下yum安装Tomcat8","slug":"CentOS-6-x-下yum安装Tomcat8","date":"2018-05-22T21:40:05.000Z","updated":"2019-12-09T15:03:26.000Z","comments":true,"path":"post/30e8c768.html","link":"","permalink":"https://ydstudios.gitee.io/post/30e8c768.html","excerpt":"废话不多说，今天准备在Centos上安装tomcat，学java不会tomcat，那不就是白学Java么？查看Centos版本：","text":"废话不多说，今天准备在Centos上安装tomcat，学java不会tomcat，那不就是白学Java么？查看Centos版本： 12345cat /etc/issue[root@iZwz99xkrnh5xy0cqp8aofZ tomcat]# cat /etc/issueCentOS release 6.9 (Final)Kernel \\r on an \\m 通过yum好处其实很多，环境变量不用配置，配置文件放在大家都熟悉的地方，通过rpm -ql xxx可以知道全部文件的地方等等。 1.安装Tomcat自带的yum源里面的Tomcat版本过低，所以我准备安装Tomcat8。 12345678910111213cd /usr/local/src/// 下载脚本# git clone https://github.com/boundlessgeo/rpm-tomcat8.git &amp;&amp; cd rpm-tomcat8// 安装rpm打包工具# yum -y install rpmdevtools// 打包rpm包# ./make_rpm.sh// 安装依赖# yum install -y redhat-lsb-core// 安装Tomcat8# rpm -ivh rpmbuild/RPMS/noarch/boundless-server-tomcat8-8.0.47-5.noarch.rpm备注：主要是这个脚本https://github.com/boundlessgeo/rpm-tomcat8，想要哪个版本可以直接上去这里修改文件即可。 2.设置开机启动 1chkconfig tomcat8 on 3.常用的命令 12345678// 服务状态service tomcat8 status// 服务启动service tomcat8 start// 服务停止service tomcat8 stop// 服务重启service tomcat8 restart 4.卸载Tomcat 1yum -e boundless-server-tomcat8.noarch","categories":[],"tags":[{"name":"linux","slug":"linux","permalink":"https://ydstudios.gitee.io/tags/linux/"},{"name":"tomcat","slug":"tomcat","permalink":"https://ydstudios.gitee.io/tags/tomcat/"}]},{"title":"Spring Boot 1.x集成spring-boot-devtools开发时实现热部署","slug":"Spring-Boot-1-x集成spring-boot-devtools开发时实现热部署","date":"2018-05-13T21:38:16.000Z","updated":"2019-12-09T15:03:26.000Z","comments":true,"path":"post/e13b17d3.html","link":"","permalink":"https://ydstudios.gitee.io/post/e13b17d3.html","excerpt":"热部署大家都知道在项目开发过程中，常常会改动页面数据或者修改数据结构，为了显示改动效果，往往需要重启应用查看改变效果，其实就是重新编译生成了新的Class文件，这个文件里记录着和代码等对应的各种信息，然后Class文件将被虚拟机的ClassLoader加载。而热部署正是利用了这个特点，它监听到如果有Class文件改动了，就会创建一个新的ClaassLoader进行加载该文件，经过一系列的过程，最终将结果呈现在我们眼前。","text":"热部署大家都知道在项目开发过程中，常常会改动页面数据或者修改数据结构，为了显示改动效果，往往需要重启应用查看改变效果，其实就是重新编译生成了新的Class文件，这个文件里记录着和代码等对应的各种信息，然后Class文件将被虚拟机的ClassLoader加载。而热部署正是利用了这个特点，它监听到如果有Class文件改动了，就会创建一个新的ClaassLoader进行加载该文件，经过一系列的过程，最终将结果呈现在我们眼前。 类加载机制Java中的类经过编译器可以把代码编译为存储字节码的Class文件，该Class文件存储了各种信息，最终要加载到虚拟机中运行使用。类加载机制（摘自《深入理解 Java 虚拟机》）虚拟机把描述类的数据从Class文件加载到内存中，并对数据进行校验、转换解析和初始化，最终形成可以被虚拟机直接使用的Java类型。 Spring Boot实现热部署Spring Boot实现热部署有如下方式： 使用 Spring Loaded 使用 spring-boot-devtools Spring Loaded这种方式是以Maven插件的形式去加载，所以启动时使用通过Maven命令mvn spring-boot:run启动，而通过Application.run方式启动的会无效，因为通过应用程序启动时，已经绕开了Maven插件机制。pom集成方式： 123456789101112131415&lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;springloaded&lt;/artifactId&gt; &lt;version&gt;1.2.5.RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;/plugin&gt; &lt;/plugins&gt;&lt;/build&gt; spring-boot-devtools这种方式无论怎么启动应用，都可以达到修改文件后重启应用。pom集成： 123456&lt;!-- 热部署模块 --&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-devtools&lt;/artifactId&gt; &lt;optional&gt;true&lt;/optional&gt; &lt;!-- 这个需要为 true 热部署才有效 --&gt;&lt;/dependency&gt; 集成注意 如果发现没有热部署效果，则需要检查IDE配置中有没有打开自动编译。 如果使用Thymeleaf模板引擎，需要把模板默认缓存设置为false12#禁止thymeleaf缓存（建议：开发环境设置为false，生成环境设置为true）spring.thymeleaf.cache=false 针对devtools的可以指定目录或者排除目录来进行热部署1234#添加那个目录的文件需要restartspring.devtools.restart.additional-paths=src/main/java#排除那个目录的文件不需要restartspring.devtools.restart.exclude=static/**,public/** 设置idea让他实现文件修改自动重启项目 找到idea的Preferences -&gt; Build, Execution, Deployment -&gt; Compiler，勾选Build project automatically 回到idea正常界面，Mac使用快捷键shift+option+command+/，window上的快捷键是Shift+Ctrl+Alt+/，打开Registry，勾选compiler.automake.allow.when.app.runningcompiler.automake.allow.when.app.running 通过以上的设置就可以在不重启服务的情况下加载html，但如果修改java文件，服务在几秒后会自动重启，如果不希望服务重启需要在application.properties或application.yml中添加spring.devtools.reatart.enable=false","categories":[],"tags":[{"name":"java","slug":"java","permalink":"https://ydstudios.gitee.io/tags/java/"},{"name":"spring boot","slug":"spring-boot","permalink":"https://ydstudios.gitee.io/tags/spring-boot/"},{"name":"spring-boot-devtools","slug":"spring-boot-devtools","permalink":"https://ydstudios.gitee.io/tags/spring-boot-devtools/"}]},{"title":"Maven项目出现“请使用 -source 8 或更高版本以启用 lambda 表达式”","slug":"Maven项目出现“请使用-source-8-或更高版本以启用-lambda-表达式”","date":"2018-05-09T21:36:34.000Z","updated":"2019-12-09T15:03:26.000Z","comments":true,"path":"post/db29c21a.html","link":"","permalink":"https://ydstudios.gitee.io/post/db29c21a.html","excerpt":"今天在编译项目的时候出现“请使用 -source 8 或更高版本以启用 lambda 表达式”的问题。这个问题是怎么出现的呢？其实是自己Maven的默认JDK的版本过低的原因，貌似JDK的版本默认为JDK1.5。知道问题的所在，我们就可以着手解决了。","text":"今天在编译项目的时候出现“请使用 -source 8 或更高版本以启用 lambda 表达式”的问题。这个问题是怎么出现的呢？其实是自己Maven的默认JDK的版本过低的原因，貌似JDK的版本默认为JDK1.5。知道问题的所在，我们就可以着手解决了。 在Maven中指定JDK的版本1234567891011121314151617&lt;project xmlns=\"...\"&gt; ... &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt; &lt;version&gt;3.3&lt;/version&gt; &lt;configuration&gt; &lt;source&gt;1.8&lt;/source&gt; &lt;target&gt;1.8&lt;/target&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; ...&lt;/project&gt; 在Maven的settings.xml里面指定全局JDK编译版本右键点击项目找到Maven，找到关于settings.xml选项，打开settings.xml文件，写入下面的内容：1234567891011121314&lt;profiles&gt; &lt;profile&gt; &lt;id&gt;jdk-1.8&lt;/id&gt; &lt;activation&gt; &lt;activeByDefault&gt;true&lt;/activeByDefault&gt; &lt;jdk&gt;1.8&lt;/jdk&gt; &lt;/activation&gt; &lt;properties&gt; &lt;maven.compiler.source&gt;1.8&lt;/maven.compiler.source&gt; &lt;maven.compiler.target&gt;1.8&lt;/maven.compiler.target&gt; &lt;maven.compiler.compilerVersion&gt;1.8&lt;/maven.compiler.compilerVersion&gt; &lt;/properties&gt; &lt;/profile&gt;&lt;/profiles&gt; 然后重新编译就可以了","categories":[],"tags":[{"name":"java","slug":"java","permalink":"https://ydstudios.gitee.io/tags/java/"},{"name":"maven","slug":"maven","permalink":"https://ydstudios.gitee.io/tags/maven/"}]},{"title":"Spring Boot 1.x 整合mybatis,并自动生成mapper和实体","slug":"Spring-Boot-1-x-整合mybatis-并自动生成mapper和实体","date":"2018-05-07T21:35:19.000Z","updated":"2019-12-09T15:03:26.000Z","comments":true,"path":"post/e5912e16.html","link":"","permalink":"https://ydstudios.gitee.io/post/e5912e16.html","excerpt":"最近一直都在学习Java，发现目前Java招聘中，mybatis出现的频率挺高的，可能是目前Java开发中使用比较多的数据库ORM框架。于是我准备研究下Spring Boot 1.x 和mybatis的整合。","text":"最近一直都在学习Java，发现目前Java招聘中，mybatis出现的频率挺高的，可能是目前Java开发中使用比较多的数据库ORM框架。于是我准备研究下Spring Boot 1.x 和mybatis的整合。 1.在pom.xml文件中添加下面的配置 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-jdbc&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;scope&gt;runtime&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;druid&lt;/artifactId&gt; &lt;version&gt;1.0.29&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-jpa&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!-- 这个是官方的mybatis依赖，这个你不加没法用噻 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.mybatis.spring.boot&lt;/groupId&gt; &lt;artifactId&gt;mybatis-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;1.3.2&lt;/version&gt; &lt;/dependency&gt; &lt;!-- 这个是自动生成mapper等的依赖，必须得加--&gt; &lt;dependency&gt; &lt;groupId&gt;org.mybatis.generator&lt;/groupId&gt; &lt;artifactId&gt;mybatis-generator&lt;/artifactId&gt; &lt;version&gt;1.3.5&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;/dependency&gt; &lt;!--这个是自动生成mapper等的依赖，必须得加 https://mvnrepository.com/artifact/org.mybatis.generator/mybatis-generator-core --&gt; &lt;dependency&gt; &lt;groupId&gt;org.mybatis.generator&lt;/groupId&gt; &lt;artifactId&gt;mybatis-generator-core&lt;/artifactId&gt; &lt;version&gt;1.3.5&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;groupId&gt;org.mybatis.generator&lt;/groupId&gt; &lt;artifactId&gt;mybatis-generator-maven-plugin&lt;/artifactId&gt; &lt;version&gt;1.3.2&lt;/version&gt; &lt;configuration&gt; &lt;!--配置文件存放的地址--&gt; &lt;!--&lt;configurationFile&gt;src/main/resources/mybatis-generator/generatorConfig.xml&lt;/configurationFile&gt;--&gt; &lt;verbose&gt;true&lt;/verbose&gt; &lt;overwrite&gt;true&lt;/overwrite&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; 2.在application.properties配置文件中进行数据库和mybatis扫描的配置 1234567891011121314151617spring.datasource.driver-class-name=com.mysql.jdbc.Driverspring.datasource.url=jdbc:mysql://localhost:3306/yddy?useUnicode=true&amp;characterEncoding=UTF-8&amp;serverTimezone=UTCspring.datasource.username=rootspring.datasource.password=123456mybatis.config-locations=classpath:mybatis-config.xml# mapper文件mybatis.mapper-locations=classpath:mapper/*.xml #这里直接写*，对后面自定义sql，编写xxxExt.xml有好处，自动扫描到这些文件，不用再改配置了mybatis.type-aliases-package=com.dameiweb.learn.modellogging.level.com.dameiweb.learn.dao=debug#mybatis.config = mybatis 配置文件名称#mybatis.mapperLocations = mapper xml 文件地址#mybatis.typeAliasesPackage = 实体类包路径#mybatis.typeHandlersPackage = type handlers 处理器包路径#mybatis.check-config-location = 检查 mybatis 配置是否存在，一般命名为 mybatis-config.xml#mybatis.executorType = 执行模式。默认是 SIMPLE 3.mybatis的配置和自动生成mapper的配置在resource目录下新建一个generatorConfig.xml和mybatis-config.xml。内容如下：generatorConfig.xml内容 123456789101112131415161718192021222324252627282930313233343536373839&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;!DOCTYPE generatorConfiguration PUBLIC \"-//mybatis.org//DTD MyBatis Generator Configuration 1.0//EN\" \"http://mybatis.org/dtd/mybatis-generator-config_1_0.dtd\"&gt;&lt;generatorConfiguration&gt; &lt;classPathEntry location=\"C:\\Users\\nick\\.m2\\repository/mysql/mysql-connector-java/5.1.46/mysql-connector-java-5.1.46.jar\"/&gt; &lt;context id=\"DB2Tables\" targetRuntime=\"MyBatis3\"&gt; &lt;commentGenerator&gt; &lt;property name=\"suppressDate\" value=\"true\"/&gt; &lt;property name=\"suppressAllComments\" value=\"true\"/&gt; &lt;/commentGenerator&gt; &lt;!--数据库链接地址账号密码--&gt; &lt;jdbcConnection driverClass=\"com.mysql.jdbc.Driver\" connectionURL=\"jdbc:mysql://localhost:3306/yddy?useUnicode=true&amp;amp;characterEncoding=UTF-8&amp;amp;serverTimezone=UTC\" userId=\"root\" password=\"123456\"&gt; &lt;/jdbcConnection&gt; &lt;javaTypeResolver&gt; &lt;property name=\"forceBigDecimals\" value=\"false\"/&gt; &lt;/javaTypeResolver&gt; &lt;!--生成Model类存放位置--&gt; &lt;javaModelGenerator targetPackage=\"com.dameiweb.learn.model\" targetProject=\"src/main/java\"&gt; &lt;property name=\"enableSubPackages\" value=\"true\"/&gt; &lt;!-- 从数据库返回的值被清理前后的空格 --&gt; &lt;property name=\"trimStrings\" value=\"false\"/&gt; &lt;/javaModelGenerator&gt; &lt;!--生成映射文件存放位置--&gt; &lt;sqlMapGenerator targetPackage=\"mapper\" targetProject=\"src/main/resources\"&gt; &lt;property name=\"enableSubPackages\" value=\"true\"/&gt; &lt;/sqlMapGenerator&gt; &lt;!--生成Dao类存放位置--&gt; &lt;!-- 客户端代码，生成易于使用的针对Model对象和XML配置文件 的代码 type=\"ANNOTATEDMAPPER\",生成Java Model 和基于注解的Mapper对象 type=\"MIXEDMAPPER\",生成基于注解的Java Model 和相应的Mapper对象 type=\"XMLMAPPER\",生成SQLMap XML文件和独立的Mapper接口 --&gt; &lt;javaClientGenerator type=\"XMLMAPPER\" targetPackage=\"com.dameiweb.learn.dao\" targetProject=\"src/main/java\"&gt; &lt;property name=\"enableSubPackages\" value=\"true\"/&gt; &lt;/javaClientGenerator&gt; &lt;/context&gt;&lt;/generatorConfiguration&gt; mybatis-config.xml的内容： 123456789101112131415161718192021222324252627282930313233343536373839&lt;?xml version=\"1.0\" encoding=\"UTF-8\" ?&gt;&lt;!DOCTYPE configuration PUBLIC \"-//mybatis.org//DTD Config 3.0//EN\" \"http://mybatis.org/dtd/mybatis-3-config.dtd\"&gt;&lt;configuration&gt; &lt;settings&gt; &lt;setting name=\"callSettersOnNulls\" value=\"true\"/&gt; &lt;setting name=\"cacheEnabled\" value=\"true\"/&gt; &lt;setting name=\"lazyLoadingEnabled\" value=\"true\"/&gt; &lt;setting name=\"aggressiveLazyLoading\" value=\"true\"/&gt; &lt;setting name=\"multipleResultSetsEnabled\" value=\"true\"/&gt; &lt;setting name=\"useColumnLabel\" value=\"true\"/&gt; &lt;setting name=\"useGeneratedKeys\" value=\"false\"/&gt; &lt;setting name=\"autoMappingBehavior\" value=\"PARTIAL\"/&gt; &lt;setting name=\"defaultExecutorType\" value=\"SIMPLE\"/&gt; &lt;setting name=\"mapUnderscoreToCamelCase\" value=\"true\"/&gt; &lt;setting name=\"localCacheScope\" value=\"SESSION\"/&gt; &lt;setting name=\"jdbcTypeForNull\" value=\"NULL\"/&gt; &lt;/settings&gt; &lt;typeAliases&gt; &lt;typeAlias alias=\"Integer\" type=\"java.lang.Integer\" /&gt; &lt;typeAlias alias=\"Long\" type=\"java.lang.Long\" /&gt; &lt;typeAlias alias=\"HashMap\" type=\"java.util.HashMap\" /&gt; &lt;typeAlias alias=\"LinkedHashMap\" type=\"java.util.LinkedHashMap\" /&gt; &lt;typeAlias alias=\"ArrayList\" type=\"java.util.ArrayList\" /&gt; &lt;typeAlias alias=\"LinkedList\" type=\"java.util.LinkedList\" /&gt; &lt;/typeAliases&gt;&lt;/configuration&gt; 这样我们就配置好了Spring Boot和mybatis与mybatis自动生成mapper和实体的功能。这样我们要生成某个表的实体和mapper的时候，只需要在generatorConfig.xml中javaClientGenerator标签之后添加下面的内容即可。此处我们已yd_movies表为例(主要数据库表名字不要用复数，这个表名肯定是我之前脑子抽了，搞个复数的名字) 12345678910111213 &lt;!--生成对应表及类名--&gt; &lt;table tableName=\"yd_movies\" domainObjectName=\"Movie\" enableCountByExample=\"false\" enableUpdateByExample=\"false\" enableDeleteByExample=\"false\" enableSelectByExample=\"false\" selectByExampleQueryId=\"false\"&gt; &lt;columnOverride column=\"cover_photos\" javaType=\"java.lang.String\" jdbcType=\"VARCHAR\" /&gt; &lt;columnOverride column=\"cover_photos_loc\" javaType=\"java.lang.String\" jdbcType=\"VARCHAR\" /&gt; &lt;columnOverride column=\"pubdates\" javaType=\"java.lang.String\" jdbcType=\"VARCHAR\" /&gt; &lt;columnOverride column=\"photographs_origin\" javaType=\"java.lang.String\" jdbcType=\"VARCHAR\" /&gt; &lt;columnOverride column=\"photographs_origin_loc\" javaType=\"java.lang.String\" jdbcType=\"VARCHAR\" /&gt; &lt;columnOverride column=\"languages\" javaType=\"java.lang.String\" jdbcType=\"VARCHAR\" /&gt; &lt;columnOverride column=\"awards\" javaType=\"java.lang.String\" jdbcType=\"VARCHAR\" /&gt; &lt;columnOverride column=\"summary\" javaType=\"java.lang.String\" jdbcType=\"VARCHAR\" /&gt; &lt;/table&gt;&lt;!--columnOverride使用这个标签是为了防止数据库字段类型为text时候，mybatis会生成xxxxWithBLOBs文件。--&gt; 然后点击idea右侧Maven projects，就可以看到mybatis-generate下的mybatis-generate：generate，双击mybatis-generate：generate即可。 generator 注意点，也可以说是BUG就是在生成的时候，在第二次生成的时候，dao 和entity 都会直接覆盖，而Mapper.xml 会直接追加，导致运行报错，而且很难找。 4.mybatis自定义sql当我们需要自定义sql的时候，我们不能把这些也放到mapper文件中。不然，当数据库表的字段有更新需要重新生成mapper的时候，这样我们自定义的sql不就被覆盖没了么！所以我们需要在MovieMapper.xml的同级目录里，新建一个MovieMapperExt.xml文件，把自定义的sql放到这里，例子如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344MovieMapperExt.xml文件内容&lt;?xml version=\"1.0\" encoding=\"UTF-8\" ?&gt;&lt;!DOCTYPE mapper PUBLIC \"-//mybatis.org//DTD Mapper 3.0//EN\" \"http://mybatis.org/dtd/mybatis-3-mapper.dtd\" &gt;&lt;mapper namespace=\"com.dameiweb.learn.dao.MovieMapper\" &gt; &lt;select id=\"selectByCondition\" resultMap=\"BaseResultMap\" parameterType=\"com.dameiweb.learn.dto.MovieRequest\"&gt; SELECT yd_movies.* FROM yd_movies LEFT JOIN yd_r_movie_countries ON yd_movies.id = yd_r_movie_countries.movie_id LEFT JOIN yd_countries cou ON yd_r_movie_countries.country_id = cou.id LEFT JOIN yd_r_movie_category ON yd_movies.id = yd_r_movie_category.movie_id LEFT JOIN yd_category cat ON yd_r_movie_category.category_id = cat.id LEFT JOIN yd_r_movie_actor ON yd_movies.id = yd_r_movie_actor.movie_id LEFT JOIN yd_actors a ON yd_r_movie_actor.actor_id = a.id WHERE yd_movies.status = 1 &lt;if test=\"movieRequest.title != null\"&gt; AND yd_movies.title LIKE \"%\"#&#123;movieRequest.title&#125;\"%\" &lt;/if&gt; &lt;if test=\"movieRequest.country != null\"&gt; AND cou.id = #&#123;movieRequest.country&#125; &lt;/if&gt; &lt;if test=\"movieRequest.category != null\"&gt; AND cat.id = #&#123;movieRequest.category&#125; &lt;/if&gt; &lt;if test=\"movieRequest.year != null\"&gt; AND year = #&#123;movieRequest.year&#125; &lt;/if&gt; &lt;if test=\"movieRequest.rate != null\"&gt; AND rate &gt;= #&#123;movieRequest.rate&#125; &lt;/if&gt; &lt;if test=\"movieRequest.rate != null\"&gt; AND rate &amp;lt; (#&#123;movieRequest.rate&#125;+1) &lt;/if&gt; &lt;if test=\"movieRequest.subtype != null\"&gt; AND yd_movies.subtype = #&#123;movieRequest.subtype&#125; &lt;/if&gt; GROUP BY yd_movies.id ORDER BY updated_at DESC, year &lt;/select&gt;&lt;/mapper&gt;","categories":[],"tags":[{"name":"java","slug":"java","permalink":"https://ydstudios.gitee.io/tags/java/"},{"name":"spring boot","slug":"spring-boot","permalink":"https://ydstudios.gitee.io/tags/spring-boot/"}]},{"title":"Spring Boot框架中读取配置的几种方式","slug":"Spring-Boot框架中读取配置的几种方式","date":"2018-05-04T21:33:18.000Z","updated":"2019-12-09T15:03:26.000Z","comments":true,"path":"post/28da4df8.html","link":"","permalink":"https://ydstudios.gitee.io/post/28da4df8.html","excerpt":"现在开发的主力语言已经从PHP转向Java，目前参与或负责公司的几个项目都是使用Spring Boot框架。用了Java之后，对比PHP就一个感觉：繁琐，Java比PHP繁琐的很！由于自己平时也在努力学习Java中，今天趁着有空闲来总结一下Spring Boot中读取配置的几种方式，加深一下自己的印象。","text":"现在开发的主力语言已经从PHP转向Java，目前参与或负责公司的几个项目都是使用Spring Boot框架。用了Java之后，对比PHP就一个感觉：繁琐，Java比PHP繁琐的很！由于自己平时也在努力学习Java中，今天趁着有空闲来总结一下Spring Boot中读取配置的几种方式，加深一下自己的印象。 读取application文件在application.yml或者properties文件中添加： 123info.address=RPCinfo.company=Springinfo.degree=high 1.@Value注解读取方式 123456789101112131415161718192021222324252627282930313233343536import org.springframework.beans.factory.annotation.Value;import org.springframework.stereotype.Component;@Componentpublic class InfoConfig &#123; @Value(\"$&#123;info.address&#125;\") private String address; @Value(\"$&#123;info.company&#125;\") private String company; @Value(\"$&#123;info.degree&#125;\") private String degree; public String getAddress() &#123; return address; &#125; public void setAddress(String address) &#123; this.address = address; &#125; public String getCompany() &#123; return company; &#125; public void setCompany(String company) &#123; this.company = company; &#125; public String getDegree() &#123; return degree; &#125; public void setDegree(String degree) &#123; this.degree = degree; &#125;&#125; 2.@ConfigurationProperties注解读取方式 12345678910111213141516171819202122232425262728293031@Component@ConfigurationProperties(prefix = \"info\")public class InfoConfig &#123; private String address; private String company; private String degree; public String getAddress() &#123; return address; &#125; public void setAddress(String address) &#123; this.address = address; &#125; public String getCompany() &#123; return company; &#125; public void setCompany(String company) &#123; this.company = company; &#125; public String getDegree() &#123; return degree; &#125; public void setDegree(String degree) &#123; this.degree = degree; &#125;&#125; 读取指定文件资源目录下建立config/db-config.properties: db.username=rootdb.password=123456 @PropertySource+@Value注解读取方式 1234567891011121314151617181920212223242526@Component@PropertySource(value = &#123; \"config/db-config.properties\"&#125;)public class DBConfig1 &#123; @Value(\"$&#123;db.username&#125;\") private String username; @Value(\"$&#123;db.password&#125;\") private String password; public String getUsername() &#123; return username; &#125; public void setUsername(String username) &#123; this.username = username; &#125; public String getPassword() &#123; return password; &#125; public void setPassword(String password) &#123; this.password = password; &#125;&#125; 注意：@PropertySource不支持yml文件读取。 @PropertySource+@ConfigurationProperties注解读取方式 1234567891011121314151617181920212223@Component@ConfigurationProperties(prefix = \"db\")@PropertySource(value = &#123;\"config/db-config.properties\"&#125;)public class DBConfig2 &#123; private String username; private String password; public String getUsername() &#123; return username; &#125; public void setUsername(String username) &#123; this.username = username; &#125; public String getPassword() &#123; return password; &#125; public void setPassword(String password) &#123; this.password = password; &#125;&#125; Environment读取方式 1234@Autowiredprivate Environment environment;environment.getProperty(\"info.degree\"); 从以上示例来看，Spring Boot可以通过@PropertySource,@Value,@Environment,@ConfigurationProperties的相互配合来读取配置文件的内容，读取单个的配置Environment无疑是最霸道的方法。","categories":[],"tags":[{"name":"java","slug":"java","permalink":"https://ydstudios.gitee.io/tags/java/"},{"name":"spring boot","slug":"spring-boot","permalink":"https://ydstudios.gitee.io/tags/spring-boot/"}]},{"title":"解决Java异常java.security.InvalidKeyException: Illegal key size","slug":"解决Java异常java-security-InvalidKeyException-Illegal-key-size","date":"2018-04-26T23:41:54.000Z","updated":"2019-12-09T15:03:26.000Z","comments":true,"path":"post/4770bec.html","link":"","permalink":"https://ydstudios.gitee.io/post/4770bec.html","excerpt":"最近公司准备对接马蜂窝的接口，今天在开发的过程中出现了下面的问题： 1java.security.InvalidKeyException: Illegal key size","text":"最近公司准备对接马蜂窝的接口，今天在开发的过程中出现了下面的问题： 1java.security.InvalidKeyException: Illegal key size 通过仔细阅读马蜂窝的demo时发现，他们在demo里写了下面一段话： 123456说明：异常java.security.InvalidKeyException:illegal Key Size的解决方案在官方网站下载JCE无限制权限策略文件（JDK7的下载地址： http://www.oracle.com/technetwork/java/javase/downloads/jce-7-download-432124.html下载后解压，可以看到local_policy.jar和US_export_policy.jar以及readme.txt如果安装了JRE，将两个jar文件放到%JRE_HOME%\\lib\\security目录下覆盖原来的文件如果安装了JDK，将两个jar文件放到%JDK_HOME%\\jre\\lib\\security目录下覆盖原来文件 后来百度得知这里面还有其他原因：如果密钥大于128, 会抛出java.security.InvalidKeyException: Illegal key size 异常. 因为密钥长度是受限制的, java运行时环境读到的是受限的policy文件. 文件位于${java_home}/jre/lib/security, 这种限制是因为美国对软件出口的控制. 由于现在我用的是Java8，所以我把解决办法整理如下： 123在官方网站下载JCE无限制权限策略文件JDK7的下载地址: http://www.oracle.com/technetwork/java/javase/downloads/jce-7-download-432124.htmlJDK8的下载地址: http://www.oracle.com/technetwork/java/javase/downloads/jce8-download-2133166.html 下载后解压，可以看到local_policy.jar和US_export_policy.jar以及readme.txt如果安装了JRE，将两个jar文件放到%JRE_HOME%\\lib\\security目录下覆盖原来的文件如果安装了JDK，还要将两个jar文件也放到%JDK_HOME%\\jre\\lib\\security目录下覆盖原来文件","categories":[],"tags":[{"name":"java","slug":"java","permalink":"https://ydstudios.gitee.io/tags/java/"}]},{"title":"什么是缓存雪崩和缓存穿透","slug":"什么是缓存雪崩和缓存穿透","date":"2018-03-13T23:39:32.000Z","updated":"2019-12-09T15:03:26.000Z","comments":true,"path":"post/d9968bcf.html","link":"","permalink":"https://ydstudios.gitee.io/post/d9968bcf.html","excerpt":"1. 缓存雪崩 如果缓存集中在一段时间内失效，发生大量的缓存穿透，所有的查询都落在数据库上，造成了缓存雪崩。这个没有完美解决办法，但可以分析用户行为，尽量让失效时间点均匀分布。大多数系统设计者考虑用加锁或者队列的方式保证缓存的单线程（进程）写，从而避免失效时大量的并发请求落到底层存储系统上。","text":"1. 缓存雪崩 如果缓存集中在一段时间内失效，发生大量的缓存穿透，所有的查询都落在数据库上，造成了缓存雪崩。这个没有完美解决办法，但可以分析用户行为，尽量让失效时间点均匀分布。大多数系统设计者考虑用加锁或者队列的方式保证缓存的单线程（进程）写，从而避免失效时大量的并发请求落到底层存储系统上。 解决办法 在缓存失效后，通过加锁或者队列来控制读数据库写缓存的线程数量。比如对某个key只允许一个线程查询数据和写缓存，其他线程等待。 可以通过缓存reload机制，预先去更新缓存，再即将发生大并发访问前手动触发加载缓存。 不同的key，设置不同的过期时间，让缓存失效的时间点尽量均匀。 做二级缓存，或者双缓存策略。A1为原始缓存，A2为拷贝缓存，A1失效时，可以访问A2，A1缓存失效时间设置为短期，A2设置为长期。 2. 缓存穿透 缓存穿透是指查询一个一定不存在的数据，由于缓存是不命中时需要从数据库查询，查不到数据则不写入缓存，这将导致这个不存在的数据每次请求都要到数据库去查询，造成缓存穿透。 解决办法 对所有可能查询的参数以hash形式存储，在控制层先进行校验，不符合则丢弃。还有最常见的则是采用布隆过滤器，将所有可能存在的数据哈希到一个足够大的bitmap中，一个一定不存在的数据会被这个bitmap拦截掉，从而避免了对底层存储系统的查询压力。 也可以采用一个更为简单粗暴的方法，如果一个查询返回的数据为空（不管是数据不存在，还是系统故障），我们仍然把这个空结果进行缓存，但它的过期时间会很短，最长不超过五分钟。","categories":[],"tags":[{"name":"redis","slug":"redis","permalink":"https://ydstudios.gitee.io/tags/redis/"}]},{"title":"MySQL中真正意义上的UTF8编码","slug":"MySQL中真正意义上的UTF8编码","date":"2018-02-22T23:37:26.000Z","updated":"2019-12-09T15:03:26.000Z","comments":true,"path":"post/451d6526.html","link":"","permalink":"https://ydstudios.gitee.io/post/451d6526.html","excerpt":"总所周知，目前的软件体系中运用最广泛的是Unicode字符集，而其中运用最多的编码规则就是UTF-8。用UTF-8有什么好处呢？简单的来说有一下几种好处： 乱码不会扩散, GB2312在丢失一字节等情况下会造成后续所有文字变成乱码。 不会产生错误的搜索结果, GB2312在搜索的时候相邻两个中文会拼出一个新的字符，导致出现错误的搜索结果。 更大的字符集。 很多语言直接支持 UTF-8，部分语言存储字符串到内存时直接使用UTF-8编码。 与 GB2312/GB18030相比, UTF-8是一个通用解决方案。 Unicode 一直有人维护，而 GB18030 下一次更新不知道会是什么时候了。另对于中文, UTF-8和GB2312在gzip压缩后都差不多，所以用来做网页对带宽影响很小。","text":"总所周知，目前的软件体系中运用最广泛的是Unicode字符集，而其中运用最多的编码规则就是UTF-8。用UTF-8有什么好处呢？简单的来说有一下几种好处： 乱码不会扩散, GB2312在丢失一字节等情况下会造成后续所有文字变成乱码。 不会产生错误的搜索结果, GB2312在搜索的时候相邻两个中文会拼出一个新的字符，导致出现错误的搜索结果。 更大的字符集。 很多语言直接支持 UTF-8，部分语言存储字符串到内存时直接使用UTF-8编码。 与 GB2312/GB18030相比, UTF-8是一个通用解决方案。 Unicode 一直有人维护，而 GB18030 下一次更新不知道会是什么时候了。另对于中文, UTF-8和GB2312在gzip压缩后都差不多，所以用来做网页对带宽影响很小。 MySQL中也有UTF-8编码，他还有另外一个称呼叫做utf8mb3，该字符集每个字符最多使用三个字节。三个字节的UTF-8最大能编码的 Unicode字符是0xffff，也就是Unicode中的基本多文种平面（BMP）。也就是说，任何不在基本多文本平面的 Unicode字符，都无法使用MySQL的UTF-8编码存储。包括Emoji表情（Emoji是一种特殊的Unicode 编码，常见于ios和 android手机上），和很多不常用的汉字，以及任何新增的 Unicode字符等等。这样的话就会出现一些特殊字符无法保存到MySQL中。接下来我们就来解决这个编码的问题！ 随着Unicode的字符集的规范确定，以及他的广泛应用，MySQL官方自己也发现了这个问题，于是乎官方在MySQL5.5.3的时候推出了utf8mb4编码，（他才是真正意义上的UTF-8编码）mb4就是most bytes 4的意思，专门用来兼容四字节的unicode。utf8mb4是utf8的超集，除了将编码改为utf8mb4外不需要做其他转换。 对于新增的utf8mb4编码官方有个小建议如下： 123Tip: To save space with utf8mb4, use VARCHAR instead of CHAR. Otherwise, MySQL must reserve four bytes for each character in a CHAR CHARACTER SET utf8mb4 column because that is the maximum possible length. For example, MySQL must reserve 40 bytes for a CHAR(10) CHARACTER SET utf8mb4 column.大意如下：为了节省空间 utf8mb4，请使用 VARCHAR而不是CHAR。否则，MySQL必须为CHAR CHARACTER SET utf8mb4列中的每个字符保留四个字节，因为这是最大可能的长度。例如，MySQL必须为一CHAR(10) CHARACTER SET utf8mb4 列保留40个字节。 那如何设置MySQL的编码呢？ 查看数据库服务器的版本，低于5.5.3则不支持utf8mb4。 1234567select VERSION();+-----------+| VERSION() |+-----------+| 5.5.53 |+-----------+ 查看数据库的编码 1234567891011121314mysql&gt; show variables like &apos;character%&apos;;+--------------------------+-----------------------------------+| Variable_name | Value |+--------------------------+-----------------------------------+| character_set_client | utf8 || character_set_connection | utf8 || character_set_database | utf8 || character_set_filesystem | binary || character_set_results | utf8 || character_set_server | utf8 || character_set_system | utf8 || character_sets_dir | C:\\phpStudy\\MySQL\\share\\charsets\\ |+--------------------------+-----------------------------------+ 将上述utf8全部修改成utf8mb4，编辑mysql的配置文件/etc/my.cnf（具体还可以参照文章Centos6.9上搭建lnmp环境） 123456789101112131415161718192021222324252627282930313233343536vi /etc/my.cnf[mysqld]character-set-server=utf8mb4 collation-server=utf8_general_ci sql_mode='NO_ENGINE_SUBSTITUTION'[mysql]default-character-set = utf8mb4[mysql.server]default-character-set = utf8mb4[mysqld_safe]default-character-set = utf8mb4[client]default-character-set = utf8mb4#重启mysqlservice mysqld restart#再次查看mysql&gt; show variables like 'character%';+--------------------------+-----------------------------------+| Variable_name | Value |+--------------------------+-----------------------------------+| character_set_client | utf8mb4 || character_set_connection | utf8mb4 || character_set_database | utf8mb4 || character_set_filesystem | binary || character_set_results | utf8mb4 || character_set_server | utf8mb4 || character_set_system | utf8mb4 || character_sets_dir | C:\\phpStudy\\MySQL\\share\\charsets\\ |+--------------------------+-----------------------------------+ 这样我们的数据库就编码就全都设置成了utf8mb4,一些特殊的中文字符和Emoji表情符就可以成功的保存在数据库中了","categories":[],"tags":[{"name":"mysql","slug":"mysql","permalink":"https://ydstudios.gitee.io/tags/mysql/"}]},{"title":"PHP如何通过开启Opcache来提高性能","slug":"PHP如何通过开启Opcache来提高性能","date":"2018-02-09T23:35:09.000Z","updated":"2019-12-09T15:03:26.000Z","comments":true,"path":"post/924aacb8.html","link":"","permalink":"https://ydstudios.gitee.io/post/924aacb8.html","excerpt":"在开启Opcache之前，我们先了解一下什么是Opcache: 12OPcache 通过将 PHP 脚本预编译的字节码存储到共享内存中来提升 PHP 的性能， 存储预编译字节码的好处就是 省去了每次加载和解析 PHP 脚本的开销。PHP 5.5.0 及后续版本中已经绑定了 OPcache 扩展。 对于 PHP 5.2，5.3 和 5.4 版本可以使用 » PECL 扩展中的 OPcache 库。","text":"在开启Opcache之前，我们先了解一下什么是Opcache: 12OPcache 通过将 PHP 脚本预编译的字节码存储到共享内存中来提升 PHP 的性能， 存储预编译字节码的好处就是 省去了每次加载和解析 PHP 脚本的开销。PHP 5.5.0 及后续版本中已经绑定了 OPcache 扩展。 对于 PHP 5.2，5.3 和 5.4 版本可以使用 » PECL 扩展中的 OPcache 库。 下面来说说如何安装Opcache,不同的PHP版本安装的方法稍有不同： 12345OPcache 只能编译为共享扩展。 如果你使用 --disable-all 参数 禁用了默认扩展的构建， 那么必须使用 --enable-opcache 选项来开启 OPcache。编译之后，就可以使用 zend_extension 指令来将 OPcache 扩展加载到 PHP 中。在非 Windows 平台使用 zend_extension=/full/path/to/opcache.so，Windows平台使用 zend_extension=C:\\path\\to\\php_opcache.dllPHP 5.2,5.3和 5.4版本中没有和PHP捆绑，需要自行安装，这里不做说明。 官方出了一个推荐配置如下： 123456789使用下列推荐设置来获得较好的 性能：opcache.memory_consumption=128opcache.interned_strings_buffer=8opcache.max_accelerated_files=4000opcache.revalidate_freq=60opcache.fast_shutdown=1opcache.enable_cli=1你也可以禁用 opcache.save_comments 并且启用 opcache.enable_file_override。 需要提醒的是，在生产环境中使用上述配置之前，必须经过严格测试。 因为上述配置存在一个已知问题，它会引发一些框架和应用的异常， 尤其是在存在文档使用了备注注解的时候。 配置扩展的时候注意以下问题 123在php.ini中增加opcache时需要使用zend_extension，而不是extension，不然会得到以下WARNINGPHP Warning: PHP Startup: Invalid library (appears to be a Zend Extension, try loading using zend_extension=opcache.so from php.ini) in Unknown on line 0 由于我自己的服务器安装的PHP7，已经安装了Opcache，所以我只需在php.ini文件中配置以下内容 123456789101112131415161718192021zend_extension=/usr/local/php/lib/php/extensions/no-debug-non-zts-20151012/opcache.so[opcache];开关打开opcache.enable=1;设置共享内存大小, 单位为：Mbopcache.memory_consumption=128opcache.interned_strings_buffer=8opcache.max_accelerated_files=4000opcache.revalidate_freq=60opcache.fast_shutdown=1opcache.enable_cli=1本次涉及到的有两个参数revalidate_freq，默认2检查脚本时间戳是否有更新的周期，以秒为单位。 设置为 0 会导致针对每个请求， OPcache 都会检查脚本更新validate_timestamps，默认1如果启用，那么 OPcache 会每隔 opcache.revalidate_freq 设定的秒数 检查脚本是否更新。 如果禁用此选项，你必须使用 opcache_reset() 或者 opcache_invalidate() 函数来手动重置 OPcache，也可以 通过重启 Web 服务器来使文件系统更改生效。 最后重启Nginx和PHP即可 下面是OPcache 可用的配置指令完整列表： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126; Determines if Zend OPCache is enabled;opcache.enable=0; Determines if Zend OPCache is enabled for the CLI version of PHP;opcache.enable_cli=0; The OPcache shared memory storage size.;opcache.memory_consumption=64; The amount of memory for interned strings in Mbytes.;opcache.interned_strings_buffer=4; The maximum number of keys (scripts) in the OPcache hash table.; Only numbers between 200 and 1000000 are allowed.;opcache.max_accelerated_files=2000; The maximum percentage of \"wasted\" memory until a restart is scheduled.;opcache.max_wasted_percentage=5; When this directive is enabled, the OPcache appends the current working; directory to the script key, thus eliminating possible collisions between; files with the same name (basename). Disabling the directive improves; performance, but may break existing applications.;opcache.use_cwd=1; When disabled, you must reset the OPcache manually or restart the; webserver for changes to the filesystem to take effect.;opcache.validate_timestamps=1; How often (in seconds) to check file timestamps for changes to the shared; memory storage allocation. (\"1\" means validate once per second, but only; once per request. \"0\" means always validate);opcache.revalidate_freq=2; Enables or disables file search in include_path optimization;opcache.revalidate_path=0; If disabled, all PHPDoc comments are dropped from the code to reduce the; size of the optimized code.;opcache.save_comments=1; If enabled, a fast shutdown sequence is used for the accelerated code; Depending on the used Memory Manager this may cause some incompatibilities.;opcache.fast_shutdown=0; Allow file existence override (file_exists, etc.) performance feature.;opcache.enable_file_override=0; A bitmask, where each bit enables or disables the appropriate OPcache; passes;opcache.optimization_level=0xffffffff;opcache.inherited_hack=1;opcache.dups_fix=0; The location of the OPcache blacklist file (wildcards allowed).; Each OPcache blacklist file is a text file that holds the names of files; that should not be accelerated. The file format is to add each filename; to a new line. The filename may be a full path or just a file prefix; (i.e., /var/www/x blacklists all the files and directories in /var/www; that start with 'x'). Line starting with a ; are ignored (comments).;opcache.blacklist_filename=; Allows exclusion of large files from being cached. By default all files; are cached.;opcache.max_file_size=0; Check the cache checksum each N requests.; The default value of \"0\" means that the checks are disabled.;opcache.consistency_checks=0; How long to wait (in seconds) for a scheduled restart to begin if the cache; is not being accessed.;opcache.force_restart_timeout=180; OPcache error_log file name. Empty string assumes \"stderr\".;opcache.error_log=; All OPcache errors go to the Web server log.; By default, only fatal errors (level 0) or errors (level 1) are logged.; You can also enable warnings (level 2), info messages (level 3) or; debug messages (level 4).;opcache.log_verbosity_level=1; Preferred Shared Memory back-end. Leave empty and let the system decide.;opcache.preferred_memory_model=; Protect the shared memory from unexpected writing during script execution.; Useful for internal debugging only.;opcache.protect_memory=0; Allows calling OPcache API functions only from PHP scripts which path is; started from specified string. The default \"\" means no restriction;opcache.restrict_api=; Mapping base of shared memory segments (for Windows only). All the PHP; processes have to map shared memory into the same address space. This; directive allows to manually fix the \"Unable to reattach to base address\"; errors.;opcache.mmap_base=; Enables and sets the second level cache directory.; It should improve performance when SHM memory is full, at server restart or; SHM reset. The default \"\" disables file based caching.;opcache.file_cache=; Enables or disables opcode caching in shared memory.;opcache.file_cache_only=0; Enables or disables checksum validation when script loaded from file cache.;opcache.file_cache_consistency_checks=1; Implies opcache.file_cache_only=1 for a certain process that failed to; reattach to the shared memory (for Windows only). Explicitly enabled file; cache is required.;opcache.file_cache_fallback=1; Enables or disables copying of PHP code (text segment) into HUGE PAGES.; This should improve performance, but requires appropriate OS configuration.;opcache.huge_code_pages=0; Validate cached file permissions.; opcache.validate_permission=0; Prevent name collisions in chroot'ed environment.; opcache.validate_root=0","categories":[],"tags":[{"name":"linux","slug":"linux","permalink":"https://ydstudios.gitee.io/tags/linux/"},{"name":"php","slug":"php","permalink":"https://ydstudios.gitee.io/tags/php/"}]},{"title":"Spring Boot的三种启动方式","slug":"Spring-Boot的三种启动方式","date":"2018-01-19T23:33:49.000Z","updated":"2019-12-09T15:03:26.000Z","comments":true,"path":"post/9de93e14.html","link":"","permalink":"https://ydstudios.gitee.io/post/9de93e14.html","excerpt":"Spring Boot有以下三种启动方式，最后一种我们可以很好的设置不同环境使用不同的配置文件。 1.IDE 运行Application这个类的main方法 123456789101112package com.dameiweb.girl;import org.springframework.boot.SpringApplication;import org.springframework.boot.autoconfigure.SpringBootApplication;@SpringBootApplicationpublic class GirlApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(GirlApplication.class, args); &#125;&#125;","text":"Spring Boot有以下三种启动方式，最后一种我们可以很好的设置不同环境使用不同的配置文件。 1.IDE 运行Application这个类的main方法 123456789101112package com.dameiweb.girl;import org.springframework.boot.SpringApplication;import org.springframework.boot.autoconfigure.SpringBootApplication;@SpringBootApplicationpublic class GirlApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(GirlApplication.class, args); &#125;&#125; 2.在springboot的应用的根目录下运行mvn spring-boot:run 12345678910111213141516171819202122232425262728293031323334$ mvn spring-boot:run[INFO] Scanning for projects...Downloading: http://maven.aliyun.com/nexus/content/groups/public/org/apache/maven/plugins/maven-help-plugin/2.2/maven-help-plugin-2.2.pomDownloaded: http://maven.aliyun.com/nexus/content/groups/public/org/apache/maven/plugins/maven-help-plugin/2.2/maven-help-plugin-2.2.pom (0 B at 0 B/s)Downloading: http://maven.aliyun.com/nexus/content/groups/public/org/apache/maven/plugins/maven-help-plugin/2.2/maven-help-plugin-2.2.jarDownloaded: http://maven.aliyun.com/nexus/content/groups/public/org/apache/maven/plugins/maven-help-plugin/2.2/maven-help-plugin-2.2.jar (0 B at 0 B/s)Downloading: http://maven.aliyun.com/nexus/content/groups/public/org/codehaus/codehaus-parent/3/codehaus-parent-3.pomDownloaded: http://maven.aliyun.com/nexus/content/groups/public/org/codehaus/codehaus-parent/3/codehaus-parent-3.pom (0 B at 0 B/s)[INFO][INFO] ------------------------------------------------------------------------[INFO] Building girl 0.0.1-SNAPSHOT[INFO] ------------------------------------------------------------------------[INFO][INFO] &gt;&gt;&gt; spring-boot-maven-plugin:1.5.9.RELEASE:run (default-cli) &gt; test-compile @ girl &gt;&gt;&gt;[INFO][INFO] --- maven-resources-plugin:2.6:resources (default-resources) @ girl ---[INFO] Using 'UTF-8' encoding to copy filtered resources.[INFO] Copying 3 resources[INFO] Copying 0 resource[INFO][INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ girl ---[INFO] Nothing to compile - all classes are up to date[INFO][INFO] --- maven-resources-plugin:2.6:testResources (default-testResources) @ girl ---[INFO] Using 'UTF-8' encoding to copy filtered resources.[INFO] skip non existing resourceDirectory D:\\work\\girl\\src\\test\\resources[INFO][INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ girl ---[INFO] Nothing to compile - all classes are up to date[INFO][INFO] &lt;&lt;&lt; spring-boot-maven-plugin:1.5.9.RELEASE:run (default-cli) &lt; test-compile @ girl &lt;&lt;&lt;[INFO][INFO][INFO] --- spring-boot-maven-plugin:1.5.9.RELEASE:run (default-cli) @ girl --- 3.在springboot的应用的根目录下 mvn install 生成jar后运行 123456789101112先执行 mvn install再执行 java -jar target/girl-0.0.1-SNAPSHOT.jar --spring.profile.active=prodjava -jar target/girl-0.0.1-SNAPSHOT.jar --spring.profile.active=prod . ____ _ __ _ _ /\\\\ / ___'_ __ _ _(_)_ __ __ _ \\ \\ \\ \\( ( )\\___ | '_ | '_| | '_ \\/ _` | \\ \\ \\ \\ \\\\/ ___)| |_)| | | | | || (_| | ) ) ) ) ' |____| .__|_| |_|_| |_\\__, | / / / / =========|_|==============|___/=/_/_/_/ :: Spring Boot :: (v1.5.9.RELEASE)","categories":[],"tags":[{"name":"java","slug":"java","permalink":"https://ydstudios.gitee.io/tags/java/"},{"name":"spring boot","slug":"spring-boot","permalink":"https://ydstudios.gitee.io/tags/spring-boot/"}]},{"title":"Java里有哪些语法糖","slug":"Java里有哪些语法糖","date":"2018-01-09T23:36:33.000Z","updated":"2019-12-09T15:03:26.000Z","comments":true,"path":"post/1f95052d.html","link":"","permalink":"https://ydstudios.gitee.io/post/1f95052d.html","excerpt":"语法糖（Syntactic Sugar），也称糖衣语法，指在计算机语言中添加的某种语法，这种语法对语言本身功能来说没有什么影响，只是为了方便程序员的开发，提高开发效率。说白了，语法糖就是对现有语法的一个封装。","text":"语法糖（Syntactic Sugar），也称糖衣语法，指在计算机语言中添加的某种语法，这种语法对语言本身功能来说没有什么影响，只是为了方便程序员的开发，提高开发效率。说白了，语法糖就是对现有语法的一个封装。 Java作为一种与平台无关的高级语言，当然也含有语法糖，这些语法糖并不被虚拟机所支持，在编译成字节码阶段就自动转换成简单常用语法。语法糖的大概有三个特性： 提高程序开发效率 程序性能不受影响 规避coder可能因手误而出现的错误 那Java有哪些语法糖呢？Java大概有如下6个语法糖： 泛型与类型擦除 自动装箱与拆箱 变长参数 增强for循环 内部类 枚举类","categories":[],"tags":[{"name":"java","slug":"java","permalink":"https://ydstudios.gitee.io/tags/java/"}]},{"title":"Java出现错误Implicit super constructor  **  is undefined for default constructor. Must define an explicit constructor","slug":"Java出现错误Implicit-super-constructor-is-undefined-for-default-constructor-Must-define-an-explicit-constructor","date":"2018-01-05T23:32:03.000Z","updated":"2019-12-09T15:03:26.000Z","comments":true,"path":"post/7764f1f8.html","link":"","permalink":"https://ydstudios.gitee.io/post/7764f1f8.html","excerpt":"公司开展新的项目，要求使用Java，给一个月的学习时间。(南京一些比较大的公司也开始转向Java，语言环境发了改变，学习Java也是大势所趋。2018年1月26日修改)，只好开始学习Java！说好有系统的培训，结果我现在在自学！在学习java的时候，遇到了下面的问题： 1Implicit super constructor People() is undefined for default constructor. Must define an explicit constructor","text":"公司开展新的项目，要求使用Java，给一个月的学习时间。(南京一些比较大的公司也开始转向Java，语言环境发了改变，学习Java也是大势所趋。2018年1月26日修改)，只好开始学习Java！说好有系统的培训，结果我现在在自学！在学习java的时候，遇到了下面的问题： 1Implicit super constructor People() is undefined for default constructor. Must define an explicit constructor 大致的意思是父类没有定义默认的无参数的构造方法，父类必须定义一个无参的构造方法。经过一番研究得知，Java中有一下的规则： 因为你的父类已经定义了一个有参的构造函数并且父类中没有默认的无参构造方法，此时编译器不会为你调用默认的构造函数， 当子类继承时，必须在自己的构造函数显式调用父类的构造函数，自己才能确保子类在初始化前父类会被实例化， 如果你父类中有无参的构造函数，子类就不会强制要求调用，即你写的那个就可以通过， 编译器会默认帮你调用父类的构造函数。 修改后的代码如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253//父类package dmw;public abstract class People &#123; public String name; protected int age; protected String gender; public People()&#123;&#125; public People(int age,String name,String gender)&#123; this.age = age; this.name =name; this.gender=gender; &#125; public int getAge()&#123; return this.age; &#125; public String getGender()&#123; return this.gender; &#125; public abstract boolean setAge(int age); public abstract boolean setGender(String gender);&#125;//子类package dmw;public class Student extends People &#123; public Student(int age, String name, String gender) &#123; super(age, name, gender); // TODO Auto-generated constructor stub &#125; @Override public boolean setAge(int age) &#123; this.age = age; return false; &#125; @Override public boolean setGender(String gender) &#123; this.gender = gender; return false; &#125; &#125; eclipse的maven配置","categories":[],"tags":[{"name":"java","slug":"java","permalink":"https://ydstudios.gitee.io/tags/java/"}]},{"title":"使用java和javac命令来编译运行Java","slug":"使用java和javac命令来编译运行Java","date":"2018-01-01T23:31:03.000Z","updated":"2019-12-09T15:03:26.000Z","comments":true,"path":"post/5d2bd789.html","link":"","permalink":"https://ydstudios.gitee.io/post/5d2bd789.html","excerpt":"废话不多说，直奔主题!","text":"废话不多说，直奔主题! 1.先创建一个Test.java的文件，里面保存的内容如下： 1234567class Test&#123; public static void main(String[] args)&#123; System.out.println(\"hello world!\"); &#125;&#125;` 2.使用javac编译Test.java文件，编译通过会生成一个Test.class的文件 1234C:\\Users\\nick\\Desktop&gt;javac Test.javaC:\\Users\\nick\\Desktop&gt; 3.使用java Test命令执行Test.class文件 1234C:\\Users\\nick\\Desktop&gt;java Testhello world!C:\\Users\\nick\\Desktop&gt;","categories":[],"tags":[{"name":"java","slug":"java","permalink":"https://ydstudios.gitee.io/tags/java/"}]},{"title":"Centos6.9安装ElasticSearch2.x","slug":"Centos6-9安装ElasticSearch2-x","date":"2017-12-25T23:29:35.000Z","updated":"2019-12-09T15:03:26.000Z","comments":true,"path":"post/971cda27.html","link":"","permalink":"https://ydstudios.gitee.io/post/971cda27.html","excerpt":"我这里安装的是ElasticSearch2.x系列。ElasticSearch官方文档1.检查系统是否已经安装了jdk","text":"我这里安装的是ElasticSearch2.x系列。ElasticSearch官方文档1.检查系统是否已经安装了jdk 1yum list installed |grep java 若有自带安装的JDK，如何卸载CentOS系统自带Java环境? 12卸载JDK相关文件输入：yum -y remove java-1.7.0-openjdk*卸载tzdata-java输入：yum -y remove tzdata-java.noarch 注：“*”表示卸载掉java 1.7.0的所有openjdk相关文件2.查看yum库中的Java安装包 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849yum -y list java*[root@iZwz99xkrnh5xy0cqp8aofZ ~]# yum -y list java*Loaded plugins: securityInstalled Packagesjava-1.8.0-openjdk.x86_64 1:1.8.0.151-1.b12.el6_9 @updatesjava-1.8.0-openjdk-debug.x86_64 1:1.8.0.151-1.b12.el6_9 @updatesjava-1.8.0-openjdk-demo.x86_64 1:1.8.0.151-1.b12.el6_9 @updatesjava-1.8.0-openjdk-demo-debug.x86_64 1:1.8.0.151-1.b12.el6_9 @updatesjava-1.8.0-openjdk-devel.x86_64 1:1.8.0.151-1.b12.el6_9 @updatesjava-1.8.0-openjdk-devel-debug.x86_64 1:1.8.0.151-1.b12.el6_9 @updatesjava-1.8.0-openjdk-headless.x86_64 1:1.8.0.151-1.b12.el6_9 @updatesjava-1.8.0-openjdk-headless-debug.x86_64 1:1.8.0.151-1.b12.el6_9 @updatesjava-1.8.0-openjdk-javadoc.noarch 1:1.8.0.151-1.b12.el6_9 @updatesjava-1.8.0-openjdk-javadoc-debug.noarch 1:1.8.0.151-1.b12.el6_9 @updatesjava-1.8.0-openjdk-src.x86_64 1:1.8.0.151-1.b12.el6_9 @updatesjava-1.8.0-openjdk-src-debug.x86_64 1:1.8.0.151-1.b12.el6_9 @updatesAvailable Packagesjava-1.5.0-gcj.x86_64 1.5.0.0-29.1.el6 base java-1.5.0-gcj-devel.x86_64 1.5.0.0-29.1.el6 base java-1.5.0-gcj-javadoc.x86_64 1.5.0.0-29.1.el6 base java-1.5.0-gcj-src.x86_64 1.5.0.0-29.1.el6 base java-1.6.0-openjdk.x86_64 1:1.6.0.41-1.13.13.1.el6_8 base java-1.6.0-openjdk-demo.x86_64 1:1.6.0.41-1.13.13.1.el6_8 base java-1.6.0-openjdk-devel.x86_64 1:1.6.0.41-1.13.13.1.el6_8 base java-1.6.0-openjdk-javadoc.x86_64 1:1.6.0.41-1.13.13.1.el6_8 base java-1.6.0-openjdk-src.x86_64 1:1.6.0.41-1.13.13.1.el6_8 base java-1.7.0-openjdk.x86_64 1:1.7.0.161-2.6.12.0.el6_9 updates java-1.7.0-openjdk-demo.x86_64 1:1.7.0.161-2.6.12.0.el6_9 updates java-1.7.0-openjdk-devel.x86_64 1:1.7.0.161-2.6.12.0.el6_9 updates java-1.7.0-openjdk-javadoc.noarch 1:1.7.0.161-2.6.12.0.el6_9 updates java-1.7.0-openjdk-src.x86_64 1:1.7.0.161-2.6.12.0.el6_9 updates java-clearsilver.i686 0.10.5-16.el6 epel java-dirq.noarch 1.4-1.el6 epel java-dirq-javadoc.noarch 1.4-1.el6 epel java-service-wrapper.x86_64 3.2.5-23.el6 epel java-service-wrapper-javadoc.noarch 3.2.5-23.el6 epel java-sleep.noarch 2.1-6.el6 epel java-sleep-javadoc.noarch 2.1-6.el6 epel java_cup.x86_64 1:0.10k-5.el6 base java_cup-javadoc.x86_64 1:0.10k-5.el6 base java_cup-manual.x86_64 1:0.10k-5.el6 base javacc.x86_64 4.1-0.5.el6 base javacc-demo.x86_64 4.1-0.5.el6 base javacc-manual.x86_64 4.1-0.5.el6 base javassist.noarch 3.9.0-6.el6 base javassist-javadoc.noarch 3.9.0-6.el6 base javastroke.x86_64 0.5.1-33.el6 epel javatar.noarch 2.5-5.el6 epel javatar-javadoc.noarch 以yum库中的java-1.8为例，将java-1.8.0的所有相关Java程序都安装上。 1yum -y install java-1.8.0-openjdk* 3.查看刚安装的Java版本信息 12345java -version 可查看Java版本[root@iZwz99xkrnh5xy0cqp8aofZ ~]# java -versionopenjdk version \"1.8.0_151\"OpenJDK Runtime Environment (build 1.8.0_151-b12)OpenJDK 64-Bit Server VM (build 25.151-b12, mixed mode) 4.通过rpm安装ElasticSearch 123456789先下载并安装公共签名密钥rpm --import https://artifacts.elastic.co/GPG-KEY-elasticsearch再安装rpm的repository[elasticsearch-2.x]name=Elasticsearch repository for 2.x packagesbaseurl=https://packages.elastic.co/elasticsearch/2.x/centosgpgcheck=1gpgkey=https://packages.elastic.co/GPG-KEY-elasticsearchenabled=1 安装ElasticSearch 12345678910111213sudo yum install elasticsearch sudo dnf install elasticsearch sudo zypper install elasticsearch使用yum在CentOS和旧的基于Red Hat分发使用dnf的Fedora和其他新的Red Hat分发使用zypper基于分布的OpenSUSE如果你嫌麻烦可以之前使用下面的方法安装：wget https://download.elastic.co/elasticsearch/release/org/elasticsearch/distribution/rpm/elasticsearch/2.4.2/elasticsearch-2.4.2.rpmsha1sum elasticsearch-5.6.5.rpmsudo rpm --install elasticsearch-5.6.5.rpm 5.使用chkconfig命令将Elasticsearch配置为在系统启动时自动启动 123456sudo chkconfig --add elasticsearch下面的命令启动或者关闭elasticsearch服务sudo -i service elasticsearch startsudo -i service elasticsearch stop 6.启动elasticsearch服务报错 123456789[root@iZwz99xkrnh5xy0cqp8aofZ src]# service elasticsearch startStarting elasticsearch: OpenJDK 64-Bit Server VM warning: If the number of processors is expected to increase from one, then you should configure the number of parallel GC threads appropriately using -XX:ParallelGCThreads=NOpenJDK 64-Bit Server VM warning: INFO: os::commit_memory(0x0000000085330000, 2060255232, 0) failed; error='Cannot allocate memory' (errno=12)## There is insufficient memory for the Java Runtime Environment to continue.# Native memory allocation (mmap) failed to map 2060255232 bytes for committing reserved memory.# An error report file with more information is saved as:# /tmp/hs_err_pid23700.log [FAILED] 这是因为机器的内存不足，可用内存少于1G,解决方法如下： 1234567vim /etc/elasticsearch/jvm.options ##启用如下两项 -Xms4g -Xmx4g##关闭如下两项##-Xms2g ##-Xmx2g 7.配置外网可用访问 123修改配置文件vim/etc/elasticsearch/elasticsearch.yml修改成：network.host: 0.0.0.0 至此，elasticsearch的基本配置完成！","categories":[],"tags":[{"name":"linux","slug":"linux","permalink":"https://ydstudios.gitee.io/tags/linux/"},{"name":"elasticSearch","slug":"elasticSearch","permalink":"https://ydstudios.gitee.io/tags/elasticSearch/"}]},{"title":"Centos6安装Mongodb","slug":"Centos6安装Mongodb","date":"2017-12-06T23:27:40.000Z","updated":"2019-12-09T15:03:26.000Z","comments":true,"path":"post/ab77d2e6.html","link":"","permalink":"https://ydstudios.gitee.io/post/ab77d2e6.html","excerpt":"1.运行下面命令查看服务器中是否存在Mongodb的源","text":"1.运行下面命令查看服务器中是否存在Mongodb的源 1yum info mongo-10gen 结果提示没有相关匹配的信息 ，说明系统中的yum源不包含MongoDB的相关资源，所以要在使用yum命令安装MongoDB前需要增加yum源。2.配置包管理系统（yum）创建一个/etc/yum.repos.d/mongodb-org-3.6.repo文件，以便您可以直接使用安装MongoDB yum。写入一下内容： 123456[mongodb-org-3.6]name=MongoDB Repositorybaseurl=https://repo.mongodb.org/yum/redhat/$releasever/mongodb-org/3.6/x86_64/gpgcheck=1enabled=1gpgkey=https://www.mongodb.org/static/pgp/server-3.6.asc 3.安装MongoDB包要安装最新的稳定版本的MongoDB，请发出以下命令： 1yum install -y mongodb-org 要安装特定版本的MongoDB，请分别指定每个组件包，并将版本号附加到包名称，如下例所示： 1yum install -y mongodb-org-3.6.0 mongodb-org-server-3.6.0 mongodb-org-shell-3.6.0 mongodb-org-mongos-3.6.0 mongodb-org-tools-3.6.0 4.在阿里云上安全组添加一条规则,并在Centos中的iptables添加一条记录 12345678#阿里云安全组规则允许 自定义 TCP 27017/27017 地址段访问 0.0.0.0/0#iptables-A INPUT -p tcp -m tcp --dport 27017 -j ACCEPT #重启iptables/etc/init.d/iptables restart 5.开机自启动,执行下面命令： 1chkconfig mongod on 6.Mongodb启动、关闭和重启命令 123456#启动service mongod start#关闭service mongod stop#restartservice mongod restart END Mongodb安装结束","categories":[],"tags":[{"name":"linux","slug":"linux","permalink":"https://ydstudios.gitee.io/tags/linux/"},{"name":"mongodb","slug":"mongodb","permalink":"https://ydstudios.gitee.io/tags/mongodb/"}]},{"title":"Nginx出现The plain HTTP request was sent to HTTPS port","slug":"Nginx出现The-plain-HTTP-request-was-sent-to-HTTPS-port","date":"2017-12-05T23:26:28.000Z","updated":"2019-12-09T15:03:26.000Z","comments":true,"path":"post/38efc407.html","link":"","permalink":"https://ydstudios.gitee.io/post/38efc407.html","excerpt":"Nginx突然出现了下面的问题： 1The plain HTTP request was sent to HTTPS port 在博客配置ssl证书的时候，博客的https地址和http地址是都能访问，不过，今天突然发现博客访问出现上面的问题，经过百度找到了问题的解决办法，在此记录一下。","text":"Nginx突然出现了下面的问题： 1The plain HTTP request was sent to HTTPS port 在博客配置ssl证书的时候，博客的https地址和http地址是都能访问，不过，今天突然发现博客访问出现上面的问题，经过百度找到了问题的解决办法，在此记录一下。 解决办法： 删掉ssl on; 并在 listen 443; 443后加上ssl即可。Nginx最新配置文件内容如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061server &#123; listen 80; listen 443 ssl; server_name www.ydstudio.net ydstudio.net; set $my_server_name $scheme://$server_name; root html; if ($host = 'ydstudio.net') &#123; rewrite ^ https://www.ydstudio.net$request_uri? permanent; &#125; #防止ip访问，如http://xxx.xxx.xxx.xxx或者https://xxx.xxx.xxx.xxx if ( $host ~* \"\\d+\\.\\d+\\.\\d+\\.\\d+\" ) &#123; rewrite ^ https://$server_name; &#125; if ( $my_server_name != https://$server_name ) &#123; return 301 https://$server_name$request_uri; #rewrite ^ https://$server_name$request_uri? permanent; &#125; location / &#123; try_files $uri $uri/ /index.php$is_args$args; index index.php index.html index.htm; &#125; #error_page 404 /404.html; # redirect server error pages to the static page /50x.html error_page 500 502 503 504 /50x.html; location = /50x.html &#123; root /usr/share/nginx/html; &#125; # pass the PHP scripts to FastCGI server listening on 127.0.0.1:9000 location ~ \\.php$ &#123; #root html; fastcgi_pass 127.0.0.1:9000; fastcgi_index index.php; fastcgi_param SCRIPT_FILENAME html/typecho/$fastcgi_script_name; include fastcgi_params; &#125; location ~ .*\\.(gif|jpg|jpeg|png|bmp|swf)$ &#123; #expires指令设置浏览器缓存过期时间 #可以在http、server、location三个作用域中设置 #缓存图片或视频30天 expires 30d; &#125; location ~ .*\\.(js|css)?$ &#123; #缓存js/css 1小时 expires 1h; &#125; #ssl on; ssl_certificate cert/214353452860792.pem; ssl_certificate_key cert/214353452860792.key; ssl_session_timeout 5m; ssl_ciphers ECDHE-RSA-AES128-GCM-SHA256:ECDHE:ECDH:AES:HIGH:!NULL:!aNULL:!MD5:!ADH:!RC4; ssl_protocols TLSv1 TLSv1.1 TLSv1.2; ssl_prefer_server_ciphers on;&#125;","categories":[],"tags":[{"name":"linux","slug":"linux","permalink":"https://ydstudios.gitee.io/tags/linux/"},{"name":"nginx","slug":"nginx","permalink":"https://ydstudios.gitee.io/tags/nginx/"}]},{"title":"PHP出现Deprecated Automatically populating  is deprecated and will be removed in a future version","slug":"PHP出现Deprecated-Automatically-populating-is-deprecated-and-will-be-removed-in-a-future-version","date":"2017-12-05T23:24:30.000Z","updated":"2019-12-09T15:03:26.000Z","comments":true,"path":"post/96d1bc5c.html","link":"","permalink":"https://ydstudios.gitee.io/post/96d1bc5c.html","excerpt":"本地新搭建的PHP环境，出现下面的问题： 12Deprecated Automatically populating $HTTP_RAW_POST_DATA is deprecated and will be removed in a future version. To avoid this warning set 'always_populate_raw_post_data' to '-1' in php.ini and use the php://input stream instead. in Unknown on line 0Warning: Cannot modify header information - headers already sent in Unknown on line 0","text":"本地新搭建的PHP环境，出现下面的问题： 12Deprecated Automatically populating $HTTP_RAW_POST_DATA is deprecated and will be removed in a future version. To avoid this warning set 'always_populate_raw_post_data' to '-1' in php.ini and use the php://input stream instead. in Unknown on line 0Warning: Cannot modify header information - headers already sent in Unknown on line 0 这个问题和PHP版本有关系，PHP 5.6已经废弃了$HTTP_RAW_POST_DATA。解决方法：修改php.ini 1always_populate_raw_post_data = -1 重启PHP即可","categories":[],"tags":[{"name":"php","slug":"php","permalink":"https://ydstudios.gitee.io/tags/php/"}]},{"title":"PHP出现以下错误Cannot find module (IP-MIB)","slug":"PHP出现以下错误Cannot-find-module-IP-MIB","date":"2017-12-05T23:22:45.000Z","updated":"2019-12-09T15:03:26.000Z","comments":true,"path":"post/82ed83ea.html","link":"","permalink":"https://ydstudios.gitee.io/post/82ed83ea.html","excerpt":"查看PHP版本的时候出现下面的问题： 1234567891011121314151617181920212223242526php -vMIB search path: c:/usr/share/snmp/mibsCannot find module (IP-MIB): At line 0 in (none)Cannot find module (IF-MIB): At line 0 in (none)Cannot find module (TCP-MIB): At line 0 in (none)Cannot find module (UDP-MIB): At line 0 in (none)Cannot find module (HOST-RESOURCES-MIB): At line 0 in (none)Cannot find module (SNMPv2-MIB): At line 0 in (none)Cannot find module (SNMPv2-SMI): At line 0 in (none)Cannot find module (NOTIFICATION-LOG-MIB): At line 0 in (none)Cannot find module (UCD-SNMP-MIB): At line 0 in (none)Cannot find module (UCD-DEMO-MIB): At line 0 in (none)Cannot find module (SNMP-TARGET-MIB): At line 0 in (none)Cannot find module (NET-SNMP-AGENT-MIB): At line 0 in (none)Cannot find module (DISMAN-EVENT-MIB): At line 0 in (none)Cannot find module (SNMP-VIEW-BASED-ACM-MIB): At line 0 in (none)Cannot find module (SNMP-COMMUNITY-MIB): At line 0 in (none)Cannot find module (SNMP-FRAMEWORK-MIB): At line 0 in (none)Cannot find module (SNMP-MPD-MIB): At line 0 in (none)Cannot find module (SNMP-USER-BASED-SM-MIB): At line 0 in (none)Cannot find module (SNMP-NOTIFICATION-MIB): At line 0 in (none)Cannot find module (SNMPv2-TM): At line 0 in (none)PHP 7.0.12 (cli) (built: Oct 13 2016 11:04:07) ( NTS )Copyright (c) 1997-2016 The PHP GroupZend Engine v3.0.0, Copyright (c) 1998-2016 Zend Technologies","text":"查看PHP版本的时候出现下面的问题： 1234567891011121314151617181920212223242526php -vMIB search path: c:/usr/share/snmp/mibsCannot find module (IP-MIB): At line 0 in (none)Cannot find module (IF-MIB): At line 0 in (none)Cannot find module (TCP-MIB): At line 0 in (none)Cannot find module (UDP-MIB): At line 0 in (none)Cannot find module (HOST-RESOURCES-MIB): At line 0 in (none)Cannot find module (SNMPv2-MIB): At line 0 in (none)Cannot find module (SNMPv2-SMI): At line 0 in (none)Cannot find module (NOTIFICATION-LOG-MIB): At line 0 in (none)Cannot find module (UCD-SNMP-MIB): At line 0 in (none)Cannot find module (UCD-DEMO-MIB): At line 0 in (none)Cannot find module (SNMP-TARGET-MIB): At line 0 in (none)Cannot find module (NET-SNMP-AGENT-MIB): At line 0 in (none)Cannot find module (DISMAN-EVENT-MIB): At line 0 in (none)Cannot find module (SNMP-VIEW-BASED-ACM-MIB): At line 0 in (none)Cannot find module (SNMP-COMMUNITY-MIB): At line 0 in (none)Cannot find module (SNMP-FRAMEWORK-MIB): At line 0 in (none)Cannot find module (SNMP-MPD-MIB): At line 0 in (none)Cannot find module (SNMP-USER-BASED-SM-MIB): At line 0 in (none)Cannot find module (SNMP-NOTIFICATION-MIB): At line 0 in (none)Cannot find module (SNMPv2-TM): At line 0 in (none)PHP 7.0.12 (cli) (built: Oct 13 2016 11:04:07) ( NTS )Copyright (c) 1997-2016 The PHP GroupZend Engine v3.0.0, Copyright (c) 1998-2016 Zend Technologies 解决办法： 123#关掉snmp扩展#windows注释掉 extension=php_snmp.dll","categories":[],"tags":[{"name":"linux","slug":"linux","permalink":"https://ydstudios.gitee.io/tags/linux/"},{"name":"php","slug":"php","permalink":"https://ydstudios.gitee.io/tags/php/"}]},{"title":"MySQL出现1267的错误","slug":"MySQL出现1267的错误","date":"2017-12-04T23:21:47.000Z","updated":"2019-12-09T15:03:26.000Z","comments":true,"path":"post/75a1fb40.html","link":"","permalink":"https://ydstudios.gitee.io/post/75a1fb40.html","excerpt":"执行SQL的时候，MySQL出现了下面的1267的错误，提示与字符集有关系。 11267 - Illegal mix of collations (utf8_unicode_ci,IMPLICIT) and (utf8_general_ci,IMPLICIT) for operation '='","text":"执行SQL的时候，MySQL出现了下面的1267的错误，提示与字符集有关系。 11267 - Illegal mix of collations (utf8_unicode_ci,IMPLICIT) and (utf8_general_ci,IMPLICIT) for operation '=' 查找问题的时候，发现有一张表是从其他地方拷贝过来的，这个表和我们目前项目中的表，稍微有点不同。数据表中字符串类型的字段，不仅有字符集这个属性，还有一个排序规则的属性。拷贝过来的表的字符串类型的字段的排序规则是utf8_unicode_ci，而我们的项目中的是utf8_general_ci，修改之后再次执行上述的SQL，就没有出现上面的错误！下面是问题的简单总结： 1234ci(caseinsensitive大小写不敏感)utf8_unicode_ci和utf8_general_ci对中、英文来说没有实质的差别。utf8_general_ci校对速度快，但准确度稍差。utf8_unicode_ci准确度高，但校对速度稍慢。 如果你的应用有德语、法语或者俄语，请一定使用utf8_unicode_ci。一般用utf8_general_ci就够了，到现在也没发现问题。。。 详细总结： 1、对于一种语言仅当使用utf8_unicode_ci排序做的不好时，才执行与具体语言相关的utf8字符集校对规则。例如，对于德语和法语，utf8_unicode_ci工作的很好，因此不再需要为这两种语言创建特殊的utf8校对规则。2、utf8_general_ci也适用与德语和法语，除了‘?’等于‘s’，而不是‘ss’之外。如果你的应用能够接受这些，那么应该使用 utf8_general_ci，因为它速度快。否则，使用utf8_unicode_ci，因为它比较准确。用一句话概况上面这段话：utf8_unicode_ci比较准确，utf8_general_ci速度比较快。通常情况下 utf8_general_ci的准确性就够我们用的了，在我看过很多程序源码后，发现它们大多数也用的是utf8_general_ci，所以新建数据 库时一般选用utf8_general_ci就可以了。 如何在MySQL5.0中使用UTF8在 my.cnf中增加下列参数复制代码代码如下: 1234[mysqld]init_connect='SET NAMES utf8'default-character-set=utf8default-collation = utf8_general_ci 执行查询 mysql&gt; show variables; 相关如下: 123456789character_set_client | utf8 character_set_connection | utf8 character_set_database | utf8 character_set_results | utf8 character_set_server | utf8 character_set_system | utf8collation_connection | utf8_general_ci collation_database | utf8_general_ci collation_server | utf8_general_ci 个人见解，对于数据库的使用，utf8_general 已经足够的准确，并且相较与 utf8_unicode速度上有优势，固可放心采用之","categories":[],"tags":[{"name":"mysql","slug":"mysql","permalink":"https://ydstudios.gitee.io/tags/mysql/"}]},{"title":"修改ssh的默认22端口","slug":"修改ssh的默认22端口","date":"2017-12-04T23:20:41.000Z","updated":"2019-12-09T15:03:26.000Z","comments":true,"path":"post/68417c42.html","link":"","permalink":"https://ydstudios.gitee.io/post/68417c42.html","excerpt":"在国外买的vps防止别人扫描端口，于是我把ssh默认的22端口修改一下。1.修改配置文件：/etc/ssh/sshd_config ，找到","text":"在国外买的vps防止别人扫描端口，于是我把ssh默认的22端口修改一下。1.修改配置文件：/etc/ssh/sshd_config ，找到 1#port 22 2.先将Port 22 前面的 # 号去掉，并另起一行。如定义SSH端口号为33322，则输入 1Port 33322 自定义端口选择建议在万位的端口（如：10000-65535之间）也许您会问为什么要先把port 22前面的 # 去掉呢？因为在配置文件中，# 是Linux的注释字符。注释字符后的代码程序是不会执行的。SSH默认的（即非手动指定）端口为22，所以配置文件在默认的情况下以注释字符出现。当需要指定其它端口或多端口同时访问时，就要删掉注释符号，告知程序按照您的意愿来执行响应操作。以上操作，手动指定SSH端口为22和33322（双端口号），保留22是为了防止个别防火墙屏蔽了其它端口导致无法连接VPS（如没单独指定22，新指定的33322端口防火墙也没放行，那么可能无法通过SSH连接VPS或服务器）。为了防止不必要问题的产生，所以要给自己保留条“后路”。3、修改完毕后，重启SSH服务，并退出当前连接的SSH端口。（如图） 1service sshd restart CentOS下SSH默认端口22修改成其他端口4.重启完毕，尝试使用新端口登陆连接成功，需要重新添加SSH-RSA验证，点击是（或Yes）即可。5.若能正常访问，返回第一步，根据第二步的操作将原port 22整段注释或删掉，再按第三步重启SSH即可。以上步骤重启后使用默认22号端口无法进入SSH，达到目的。【请注意】：如果您启用了防火墙iptables，那么必须先添加新开的33322端口补充：iptables开放端口端示例为了方便举例说明，就直接拿来一段我的现有服务器上运行的防火墙iptables内容。 请注意：后面的注释说明文字： 1234567891011121314151617vim /etc/sysconfig/iptables# Firewall configuration written by system-config-firewall# Manual customization of this file is not recommended.*filter:INPUT ACCEPT [0:0]:FORWARD ACCEPT [0:0]:OUTPUT ACCEPT [0:0]-A INPUT -m state --state ESTABLISHED,RELATED -j ACCEPT-A INPUT -p icmp -j ACCEPT-A INPUT -i lo -j ACCEPT-A INPUT -m state --state NEW -m tcp -p tcp --dport 22 -j ACCEPT # （ssh端口）-A INPUT -m state --state NEW -m tcp -p tcp --dport 80 -j ACCEPT # （web端口）-A INPUT -m state --state NEW -m tcp -p tcp --dport 21 -j ACCEPT # （ftp端口）-A INPUT -j REJECT --reject-with icmp-host-prohibited-A FORWARD -j REJECT --reject-with icmp-host-prohibitedCOMMIT 修改完防火墙iptables后，需要重新启动： 123/etc/init.d/iptables restart# 或者service iptables restart 注意：iptables配置文件存放位置是：/etc/sysconfig/iptables保存命令：service iptables save使用命令：iptables -L -n 可以查看当前iptables的开放端口情况。","categories":[],"tags":[{"name":"linux","slug":"linux","permalink":"https://ydstudios.gitee.io/tags/linux/"}]},{"title":"迅搜(xunsearch) 多值属性的处理","slug":"迅搜-xunsearch-多值属性的处理","date":"2017-12-04T23:18:36.000Z","updated":"2019-12-09T15:03:26.000Z","comments":true,"path":"post/10379547.html","link":"","permalink":"https://ydstudios.gitee.io/post/10379547.html","excerpt":"我之前使用Yii2框架做了一个电影网站dy360.net,网站的搜索是通过MySQL的like实现，搜索的条件很多，担心以后数据越来越多，会拖垮网站的速度，于是想通过搜索引擎来解决！ 开始准备使用sphinx，研究发现不太好使。后发现迅搜(xunsearch)，文档比较清楚，上手比较简单，并且有Yii2的composer包，于是决定使用迅搜。","text":"我之前使用Yii2框架做了一个电影网站dy360.net,网站的搜索是通过MySQL的like实现，搜索的条件很多，担心以后数据越来越多，会拖垮网站的速度，于是想通过搜索引擎来解决！ 开始准备使用sphinx，研究发现不太好使。后发现迅搜(xunsearch)，文档比较清楚，上手比较简单，并且有Yii2的composer包，于是决定使用迅搜。 使用的过程中遇到这样一个问题：一部电影有多个演员，出品国家或地区，其实就是文档的多值属性的新建、搜索的问题。我研究了一下，最终结局方案如下： 123456789101112131415161718192021222324252627282930313233343536#项目的配置文件project.name = yddyproject.default_charset = utf-8server.index = 8383server.search = 8384[id]type = idphrase = yes[summary]type = bodycutlen=900[year]type = numericphrase = yesindex = self [actors_text]type = stringindex = selftokenizer = split(/)[category_id]type = stringindex = selftokenizer = split(,)[country_id]type = stringindex = selftokenizer = split(,)phrase = yes 其实就是使用迅搜的tokenizer分词器，文档上具体的语法如下： 1234567891011tokenizer 分词器默认为 default 采用内置的功能强大的 scws 分词，适合绝大多数字符串字段。也可以指定自定义分词器， 格式为 name 或 name(arg) 两种形式，其中 name 是分词器名称，arg 则是传递给分词器构造函数的参数。 自定义分词器需要在 lib/ 目录下编写名为 XSTokenizerName 的分词类并实现接口 XSTokenizer， 内置支持的分词器有以下几种：full 表示本字段的值整体作为一个检索词，像各种 ID 都适合这种情况none 表示本字段没有任何词汇用于索引split([ ]) 表示根据参数分割内容，默认参数为空格，若参数以 / 开头并以 / 结尾则 内部调用 preg_split(arg, ..) 来分割取词，以支持正则或其它特殊字符分割xlen([2]) 表示根据指定参数长度分段取词，如 ABCDEF =&gt; AB + CD + EFxstep([2]) 表示根据指定参数步长逐段取词，如 ABCDEF =&gt; AB + ABCD + ABCDEFscws([3]) 表示采用指定参数为复合等级的 scws 分词，（若无特殊复合需求，无需指定）tokenizer = default 然后，根据不同情况使用不同的分割参数，譬如，有的使用’/‘，有的使用’,’。最后，搜索代码如下： 12345678910111213141516171819202122232425262728293031323334353637public static function search($params)&#123; $db = Yii::$app-&gt;xunsearch-&gt;getDatabase('yddy'); $xs = $db-&gt;xs; $search = $xs-&gt;getSearch(); $keywords = $params['word']; if(empty($keywords))&#123; $keywords =''; &#125; $search = $search-&gt;setQuery($keywords); if($category=$params['category'])&#123; $search-&gt;addQueryTerm('category_id',$category); &#125; if($country=$params['country'])&#123; $search-&gt;addQueryTerm('country_id',$country); &#125; if( (empty($params['other']) ) &amp;&amp; ($year = intval($params['year'])) &amp;&amp; is_numeric($params['year']))&#123; $search-&gt;addQueryTerm('year',$year); &#125;elseif(($params['other'] == 'ago') &amp;&amp; ($year = intval($params['year'])) )&#123; //$search-&gt;addRange('year',1900,1978); $search-&gt;addRange('year',1900,$year); &#125; if( $rating=intval($params['rating']))&#123; $search-&gt;addQueryTerm('rate', $rating); &#125; $search-&gt;addQueryTerm('subtype',$params['subtype']); //$search-&gt;addQueryTerm('status',1); //echo $search-&gt;getQuery(); return $search; &#125;","categories":[],"tags":[{"name":"linux","slug":"linux","permalink":"https://ydstudios.gitee.io/tags/linux/"},{"name":"xunsearch","slug":"xunsearch","permalink":"https://ydstudios.gitee.io/tags/xunsearch/"}]},{"title":"Centos6.9上搭建lnmp环境","slug":"Centos6-9上搭建lnmp环境","date":"2017-11-29T23:17:13.000Z","updated":"2020-12-06T14:01:31.000Z","comments":true,"path":"post/123335f3.html","link":"","permalink":"https://ydstudios.gitee.io/post/123335f3.html","excerpt":"我准备把之前做的www.dy360.net重新搞起来，就在vultr上买了vps，于是乎有了下面这篇文章。在vps上系统是Centos6.9，至于为什么使用Centos，那是因为我个人比较熟悉Centos！","text":"我准备把之前做的www.dy360.net重新搞起来，就在vultr上买了vps，于是乎有了下面这篇文章。在vps上系统是Centos6.9，至于为什么使用Centos，那是因为我个人比较熟悉Centos！ yum安装Nginx1.执行下面的yum命令安装Nginx 1yum install nginx 如果上面命令没有执行成功，说明系统中yum源中没有Nginx的源，因此我们需要手动添加Nginx的源，步骤如下： 123456789101112vi /etc/yum.repos.d/nginx.repo输入一下内容：[nginx]name=nginx repobaseurl=http://nginx.org/packages/mainline/centos/6/$basearch/gpgcheck=0enabled=1保存之后执行下面的命令更新源yum clean allyum update 2.启动Nginx 1service nginx start 现在Nginx已经启动了，直接访问服务器就能看到Nginx欢迎页面了的。如果还无法访问，则需配置一下Linux防火墙。3.配置防火墙,开放80端口 12345678vim /etc/sysconfig/iptables-A INPUT -p tcp -m state --state NEW -m tcp --dport 22 -j ACCEPT#复制上面一行，将22改成80,保存即可-A INPUT -p tcp -m state --state NEW -m tcp --dport 80 -j ACCEPT#最好重启一下防火墙service iptables restart 注意：如果是阿里云的ecs,你可能需要在阿里云的主机管理后台，添加安全组规则，方可开启80端口 4.设置开机启动 12chkconfig --list | grep nginxchkconfig nginx on END：Nginx安装配置完成 yum 安装MySQL5.6参考: https://www.cnblogs.com/007sx/p/7083143.html1.检查系统是否安装其他版本的MYSQL数据 12yum list installed | grep mysqlyum -y remove mysql-libs.x86_64 2.配置MySQL源 123wget http://repo.mysql.com/mysql-community-release-el6-5.noarch.rpmrpm -ivh mysql-community-release-el6-5.noarch.rpmyum repolist all | grep mysql 3.安装MYSQL数据库 1yum install mysql-community-server -y 4.设置为开机启动(2、3、4都是on代表开机自动启动) 12chkconfig --list | grep mysqldchkconfig mysqld on5.启动mysql 1234567891011121314151617181920212223242526272829303132333435363738 service mysqld start # 显示如图（部分） PLEASE REMEMBER TO SET A PASSWORD FOR THE MySQL root USER !To do so, start the server, then issue the following commands: /usr/bin/mysqladmin -u root password 'new-password' /usr/bin/mysqladmin -u root -h vultr password 'new-password'Alternatively you can run: /usr/bin/mysql_secure_installationwhich will also give you the option of removing the testdatabases and anonymous user created by default. This isstrongly recommended for production servers.See the manual for more instructions.Please report any problems at http://bugs.mysql.com/The latest information about MySQL is available on the web at http://www.mysql.comSupport MySQL by buying support/licenses at http://shop.mysql.comNote: new default config file not created.Please make sure your config file is currentWARNING: Default config file /etc/my.cnf exists on the systemThis file will be read by default by the MySQL serverIf you do not want to use this, either remove it, or use the--defaults-file argument to mysqld_safe when starting the server [ OK ]Starting mysqld: [ OK ]6.设置mysql密码 1/usr/bin/mysqladmin -u root password 'xxxxx' 安全处理,看不懂英文直接一路yes就行 1/usr/bin/mysql_secure_installation[注意]2017/12/24更新： 12345678910111213141516171819202122今日安装MySQL执行上面的命令的时候出现下面的话In order to log into MySQL to secure it, we'll need the currentpassword for the root user. If you've just installed MySQL, andyou haven't set the root password yet, the password will be blank,so you should just press enter here.Enter current password for root (enter for none): OK, successfully used password, moving on...Setting the root password ensures that nobody can log into the MySQLroot user without the proper authorisation.You already have a root password set, so you can safely answer 'n'.Change the root password? [Y/n] YNew password: Re-enter new password: Password updated successfully!Reloading privilege tables.. ... Success!意思是root用户设置密码，之前我们已经设置过了，这个地方输入“no”即可，不然你还要重新设置root用户密码。 7.设置utf-8编码 1234567891011121314查看mysql原本编码：mysql&gt; show variables like 'character%';+--------------------------+----------------------------+| Variable_name | Value |+--------------------------+----------------------------+| character_set_client | utf8 || character_set_connection | utf8 || character_set_database | latin1 || character_set_filesystem | binary || character_set_results | utf8 || character_set_server | latin1 || character_set_system | utf8 || character_sets_dir | /usr/share/mysql/charsets/ |+--------------------------+----------------------------+ 编辑mysql的配置文件/etc/my.cnf 123456789101112131415161718192021222324252627282930313233343536373839vi /etc/my.cnf[mysqld]character-set-server=utf8 collation-server=utf8_general_ci sql_mode='NO_ENGINE_SUBSTITUTION'[mysql]default-character-set = utf8[mysql.server]default-character-set = utf8[mysqld_safe]default-character-set = utf8[client]default-character-set = utf8#重启mysqlservice mysqld restart#再次查看mysql&gt; show variables like 'character%';+--------------------------+----------------------------+| Variable_name | Value |+--------------------------+----------------------------+| character_set_client | utf8 || character_set_connection | utf8 || character_set_database | utf8 || character_set_filesystem | binary || character_set_results | utf8 || character_set_server | utf8 || character_set_system | utf8 || character_sets_dir | /usr/share/mysql/charsets/ |+--------------------------+----------------------------+ END:到此MySQL安装配置成功 编译安装PHP7.0.26参考： https://segmentfault.com/a/1190000005005068 http://blog.csdn.net/tomspcc/article/details/71080991 1.安装PHP支持库 1yum install gcc gcc-c++ libmcrypt-devel mhash-devel libxslt-devel libjpeg libjpeg-devel libpng libpng-devel freetype freetype-devel libxml2 libxml2-devel zlib zlib-devel glibc glibc-devel glib2 glib2-devel bzip2 bzip2-devel ncurses ncurses-devel curl curl-devel e2fsprogs e2fsprogs-devel krb5 krb5-devel libidn libidn-devel openssl openssl-devel php-mcrypt ####2.下载PHP7.0.26源码包 1wget http://sg2.php.net/distributions/php-7.0.26.tar.gz 3.编译安装PHP 12345678910111213141516171819202122232425262728293031323334353637383940414243444546./configure \\--prefix=/usr/local/php \\--exec-prefix=/usr/local/php \\--bindir=/usr/local/php/bin \\--with-fpm-user=www \\--with-fpm-group=www \\--sbindir=/usr/local/php/sbin \\--includedir=/usr/local/php/include \\--libdir=/usr/local/php/lib/php \\--mandir=/usr/local/php/php/man \\--with-config-file-path=/usr/local/php/etc \\--with-mcrypt=/usr/include \\--with-mhash \\--with-openssl \\--with-mysqli=shared,mysqlnd \\--with-pdo-mysql=shared,mysqlnd \\--with-gd \\--with-iconv \\--with-zlib \\--enable-zip \\--enable-inline-optimization \\--disable-debug \\--disable-rpath \\--enable-shared \\--enable-xml \\--enable-bcmath \\--enable-shmop \\--enable-sysvsem \\--enable-mbregex \\--enable-mbstring \\--enable-ftp \\--enable-gd-native-ttf \\--enable-pcntl \\--enable-sockets \\--with-xmlrpc \\--enable-soap \\--without-pear \\--with-gettext \\--enable-session \\--with-curl \\--with-jpeg-dir \\--with-freetype-dir \\--enable-opcache \\--enable-fpm \\--without-gdbm \\--disable-fileinfo 编译报下面的错误 123checking for stdarg.h... (cached) yeschecking for mcrypt support... yesconfigure: error: mcrypt.h not found. Please reinstall libmcrypt. 执行下面的命令安装libmcrypet 1yum install libmcrypt-devel -y 若上面的方法解决不了，可以编译安装libmcrypet 1234567cd /usr/local/srcwget http://softlayer.dl.sourceforge.net/sourceforge/mcrypt/libmcrypt-2.5.8.tar.gztar -zxvf libmcrypt-2.5.8.tar.gzcd /usr/local/src/libmcrypt-2.5.8./configure --prefix=/usr/localmakemake install 上面的方法要是还是不行，那用下面的方法： 12345安装第三方yum源wget http://www.atomicorp.com/installers/atomicsh ./atomicyum install libmcrypt-devel -y 4.最后执行下面的命令，完成PHP的安装 12make make install 5.配置PHP-FPM 增加用户&amp;用户组 12groupadd wwwuseradd -g www www 拷贝php-fpm.conf文件 1cp /usr/local/php/etc/php-fpm.conf.default /usr/local/php/etc/php-fpm.conf 拷贝php-fpm include文件 1cp usr/local/php/etc/php-fpm.d/www.conf.default www.conf 从源码包里拷贝php.ini 1cp /usr/local/src/php-7.0.26/php.ini-production /usr/local/php/etc/php.ini 6.将PHP加入环境变量 1234vi /etc/profileexport PATH=$PATH:/usr/local/php/binsource /etc/profile 7.php-fpm自启动 从源码包里拷贝启动脚本1cp sapi/fpm/init.d.php-fpm /etc/init.d/php-fpm 加入系统启动项1chkconfig php-fpm on 8.重启PHP 1service php-fpm restart END:PHP安装配置完成 编译安装git参考:http://www.cnblogs.com/fazo/p/5578644.html 1.安装依赖的包 1yum install curl-devel expat-devel gettext-devel openssl-devel zlib-devel gcc perl-ExtUtils-MakeMaker 2.下载源码包 123wget https://github.com/git/git/archive/v2.9.0.zipunzip v2.9.0.zipcd git-2.9.0 3.编译安装 12make prefix=/usr/local/git allmake prefix=/usr/local/git install 4.加入系统变量 12vi /etc/profile export PATH=/usr/local/git/bin:$PATH 5.配置git 123git config --global user.name 'vultr'git config --global user.email 'vultr@dy360.net' 'vultr@dy360.net'git config --global credential.helper store #保存密码 6.生成ssh密钥,执行下面的命令，一路回车即可，即可用密钥克隆项目。 1ssh-keygen -t rsa -C 'vultr@dy360.net' END:git安装配置成功 最后，终于环境搭建好了！自己搭建lnmp环境少说也有十几次了，可是每次都有新感觉！因为每次出现的问题，都不太一样！最近两次都出现了Nginx安装后，通过ip不能访问的问题，结果发现都是防火墙的问题，没有开启80端口。坑的要命！","categories":[],"tags":[{"name":"linux","slug":"linux","permalink":"https://ydstudios.gitee.io/tags/linux/"},{"name":"git","slug":"git","permalink":"https://ydstudios.gitee.io/tags/git/"},{"name":"php","slug":"php","permalink":"https://ydstudios.gitee.io/tags/php/"},{"name":"nginx","slug":"nginx","permalink":"https://ydstudios.gitee.io/tags/nginx/"},{"name":"mysql","slug":"mysql","permalink":"https://ydstudios.gitee.io/tags/mysql/"}]},{"title":"tar 分卷压缩和解压","slug":"tar-分卷压缩和解压","date":"2017-11-27T23:16:14.000Z","updated":"2019-12-09T15:03:26.000Z","comments":true,"path":"post/4aac2ff2.html","link":"","permalink":"https://ydstudios.gitee.io/post/4aac2ff2.html","excerpt":"tar 本身为一个打包工具，可以把目录打包成一个文件，它的好处是它把所有文件整合成一个大文件整体，方便拷贝或者移动。","text":"tar 本身为一个打包工具，可以把目录打包成一个文件，它的好处是它把所有文件整合成一个大文件整体，方便拷贝或者移动。 123456789101112131415161718192021语法：tar [-zjxcvfpP] filename,tar命令有多个选项，其中不常用的做了标注。“-z” : 同时用gzip压缩“-j” : 同时用bzip2压缩“-x” : 解包或者解压缩“-t” : 查看tar包里面的文件“-c” : 建立一个tar包或者压缩文件包“-v” : 可视化“-f” : 后面跟文件名，压缩时跟 “-f 文件名”，意思是压缩后的文件名为filename, 解压时跟 “-f 文件名”，意思是解压filename. 请注意，如果是多个参数组合的情况下带有 “-f”，请把 “-f” 写到最后面。“-p” : 使用原文件的属性，压缩前什么属性压缩后还什么属性。（不常用）“-P” : 可以使用绝对路径。（不常用）--exclude filename : 在打包或者压缩时，不要将filename文件包括在内。（不常用） 那如何分卷解压缩呢？ 假如要将目录logs打包压缩并分割成多个1M的文件，可以用下面的命令： 12345678910111213141516tar cjf - logs/ |split -b 1m - logs.tar.bz2完成后会产生下列文件：logs.tar.bz2.aa, logs.tar.bz2.ab, logs.tar.bz2.ac要解压的时候只要执行下面的命令就可以了：cat logs.tar.bz2.a* | tar xj再举例：要将文件test.pdf分包压缩成500 bytes的文件：tar czf - test.pdf | split -b 500 - test.tar.gz最后要提醒但是那两个\"-\"不要漏了，那是tar的ouput和split的input的参数。tar cjf - logs/ |split -b 1m - logs.tar.bz2.完成后会产生下列文件：logs.tar.bz2.aa, logs.tar.bz2.ab, logs.tar.bz2.ac要解压的时候只要执行下面的命令就可以了：cat logs.tar.bz2.a* | tar xj","categories":[],"tags":[{"name":"linux","slug":"linux","permalink":"https://ydstudios.gitee.io/tags/linux/"}]},{"title":"如何强制全站https访问","slug":"如何强制全站https访问","date":"2017-11-23T23:12:52.000Z","updated":"2019-12-09T15:03:26.000Z","comments":true,"path":"post/773d918c.html","link":"","permalink":"https://ydstudios.gitee.io/post/773d918c.html","excerpt":"今天在阿里云上搞了一个免费的https证书，于是就在自己的服务器上安装了一下，配置好证书之后，如何强制全站https访问呢？其实方法有很多，这里只给出两个比较简单的方法：","text":"今天在阿里云上搞了一个免费的https证书，于是就在自己的服务器上安装了一下，配置好证书之后，如何强制全站https访问呢？其实方法有很多，这里只给出两个比较简单的方法： 12345#在自己的虚拟主机配置server节点中添加下面的代码，使用其一即可。#方法一rewrite ^ https://$server_name$request_uri? permanent;#方法二return 301 https://$server_name$request_uri; 我用的是第一个方法，大家可以根据自己的喜好，来决定使用哪中方法。下面贴出我自己网站的配置： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253server &#123; listen 443; listen 80; server_name www.ydstudio.net; set $my_server_name $scheme://$server_name; #防止ip访问，如http://xxx.xxx.xxx.xxx或者https://xxx.xxx.xxx.xxx if ( $host ~* \"\\d+\\.\\d+\\.\\d+\\.\\d+\" ) &#123; rewrite ^ https://$server_name; &#125; if ( $my_server_name != https://$server_name ) &#123; rewrite ^ https://$server_name$request_uri? permanent; &#125; location / &#123; root /usr/share/nginx/html/typecho/; try_files $uri $uri/ /index.php$is_args$args; index index.php index.html index.htm; &#125; #error_page 404 /404.html; # redirect server error pages to the static page /50x.html error_page 500 502 503 504 /50x.html; location = /50x.html &#123; root /usr/share/nginx/html; &#125; # proxy the PHP scripts to Apache listening on 127.0.0.1:80 #location ~ \\.php$ &#123; # proxy_pass http://127.0.0.1; #&#125; # pass the PHP scripts to FastCGI server listening on 127.0.0.1:9000 location ~ \\.php$ &#123; #root html; fastcgi_pass 127.0.0.1:9000; fastcgi_index index.php; fastcgi_param SCRIPT_FILENAME /usr/share/nginx/html/typecho/$fastcgi_script_name; include fastcgi_params; &#125; # deny access to .htaccess files, if Apache's document root # concurs with nginx's one #location ~ /\\.ht &#123; # deny all; #&#125; ssl on; ssl_certificate /etc/nginx/cert/214353452860792.pem; ssl_certificate_key /etc/nginx/cert/214353452860792.key; ssl_session_timeout 5m; ssl_ciphers ECDHE-RSA-AES128-GCM-SHA256:ECDHE:ECDH:AES:HIGH:!NULL:!aNULL:!MD5:!ADH:!RC4; ssl_protocols TLSv1 TLSv1.1 TLSv1.2; ssl_prefer_server_ciphers on;&#125;","categories":[],"tags":[{"name":"linux","slug":"linux","permalink":"https://ydstudios.gitee.io/tags/linux/"},{"name":"nginx","slug":"nginx","permalink":"https://ydstudios.gitee.io/tags/nginx/"}]},{"title":"Linux下chkconfig命令详解","slug":"Linux下chkconfig命令详解","date":"2017-11-19T23:11:31.000Z","updated":"2019-12-09T15:03:26.000Z","comments":true,"path":"post/a5b69b88.html","link":"","permalink":"https://ydstudios.gitee.io/post/a5b69b88.html","excerpt":"简介chkconfig命令检查、设置系统的各种服务。这是Red Hat公司遵循GPL规则所开发的程序，它可查询操作系统在每一个执行等级中会执行哪些系统服务，其中包括各类常驻服务。谨记chkconfig不是立即自动禁止或激活一个服务，它只是简单的改变了符号连接。","text":"简介chkconfig命令检查、设置系统的各种服务。这是Red Hat公司遵循GPL规则所开发的程序，它可查询操作系统在每一个执行等级中会执行哪些系统服务，其中包括各类常驻服务。谨记chkconfig不是立即自动禁止或激活一个服务，它只是简单的改变了符号连接。 使用方法1234567891011121314[root@iZwz99xkrnh5xy0cqp8aofZ ~]# chkconfig --helpchkconfig version 1.3.49.5 - Copyright (C) 1997-2000 Red Hat, Inc.This may be freely redistributed under the terms of the GNU Public License.usage: chkconfig [--list] [--type &lt;type&gt;] [name] chkconfig --add &lt;name&gt; chkconfig --del &lt;name&gt; chkconfig --override &lt;name&gt; chkconfig [--level &lt;levels&gt;] [--type &lt;type&gt;] &lt;name&gt; &lt;on|off|reset|resetpriorities&gt;[root@iZwz99xkrnh5xy0cqp8aofZ ~]# --add：增加所指定的系统服务，让chkconfig指令得以管理它，并同时在系统启动的叙述文件内增加相关数据； --del：删除所指定的系统服务，不再由chkconfig指令管理，并同时在系统启动的叙述文件内删除相关数据； --level&lt;等级代号&gt;：指定读系统服务要在哪一个执行等级中开启或关毕。 level等级代号解释如下： 等级0表示：表示关机 等级1表示：单用户模式 等级2表示：无网络连接的多用户命令行模式 等级3表示：有网络连接的多用户命令行模式 等级4表示：不可用 等级5表示：带图形界面的多用户模式 等级6表示：重新启动 需要说明的是，level选项可以指定要查看的运行级而不一定是当前运行级。对于每个运行级，只能有一个启动脚本或者停止脚本。当切换运行级时，init不会重新启动已经启动的服务，也不会再次去停止已经停止的服务。 运行级文件：每个被chkconfig管理的服务需要在对应的init.d下的脚本加上两行或者更多行的注释。第一行告诉chkconfig缺省启动的运行级以及启动和停止的优先级。如果某服务缺省不在任何运行级启动，那么使用-代替运行级。第二行对服务进行描述，可以用\\跨行注释。如下： 123# chkconfig: 2345 20 80# description: Saves and restores system entropy pool for \\# higher quality random number generation. 实例： 12345678chkconfig --list #列出所有的系统服务。 chkconfig --add httpd #增加httpd服务。 chkconfig --del httpd #删除httpd服务。 chkconfig --level httpd 2345 on #设置httpd在运行级别为2、3、4、5的情况下都是on（开启）的状态。 chkconfig --list #列出系统所有的服务启动情况。 chkconfig --list mysqld #列出mysqld服务设置情况。 chkconfig --level 35 mysqld on #设定mysqld在等级3和5为开机运行服务，--level 35表示操作只在等级3和5执行，on表示启动，off表示关闭。 chkconfig mysqld on #设定mysqld在各等级为on，“各等级”包括2、3、4、5等级。 如何增加一个服务： 123服务脚本必须存放在/etc/ini.d/目录下；chkconfig --add servicename在chkconfig工具服务列表中增加此服务，此时服务会被在/etc/rc.d/rcN.d中赋予K/S入口了；chkconfig --level 35 mysqld on修改服务的默认启动等级。 注意 此命令只能在Centos7以下使用，因为Centos7中已由systemctl替代chkconfig命令","categories":[],"tags":[{"name":"linux","slug":"linux","permalink":"https://ydstudios.gitee.io/tags/linux/"}]},{"title":"Mycat 数据库分库分表中的安装与使用","slug":"Mycat-数据库分库分表中的安装与使用","date":"2017-11-19T23:02:47.000Z","updated":"2019-12-09T15:03:26.000Z","comments":true,"path":"post/56117af9.html","link":"","permalink":"https://ydstudios.gitee.io/post/56117af9.html","excerpt":"Mycat需要jdk的支持，而我的Mac中的jdk已经安装好了，所以此步骤省略，需要安装jdk的童鞋请自行安装jdk。","text":"Mycat需要jdk的支持，而我的Mac中的jdk已经安装好了，所以此步骤省略，需要安装jdk的童鞋请自行安装jdk。 安装Mycat:直接下载解压到/usr/local/mycat目录即可，我下载的是最新版1.6。 运行linux： ./mycat start 启动 ./mycat stop 停止 ./mycat console 前台运行 ./mycat install 添加到系统自动启动（暂未实现） ./mycat remove 取消随系统自动启动（暂未实现） ./mycat restart 重启服务 ./mycat pause 暂停 ./mycat status 查看启动状态 win： 直接运行startup_nowrap.bat，如果出现闪退，在cmd 命令行运行，查看出错原因 内存配置：启动前，一般需要修改JVM配置参数，打开conf/wrapper.conf文件，如下行的内容为2G和2048，可根据本机配置情况修改为512M或其它值。 以下配置跟jvm参数完全一致，可以根据自己的jvm参数调整。 1234567891011121314151617181920212223242526272829303132333435363738394041Java Additional Parameterswrapper.java.additional.1=wrapper.java.additional.1=-DMYCAT_HOME=.wrapper.java.additional.2=-serverwrapper.java.additional.3=-XX:MaxPermSize=64Mwrapper.java.additional.4=-XX:+AggressiveOptswrapper.java.additional.5=-XX:MaxDirectMemorySize=100mwrapper.java.additional.6=-Dcom.sun.management.jmxremotewrapper.java.additional.7=-Dcom.sun.management.jmxremote.port=1984wrapper.java.additional.8=-Dcom.sun.management.jmxremote.authenticate=falsewrapper.java.additional.9=-Dcom.sun.management.jmxremote.ssl=falsewrapper.java.additional.10=-Xmx100mwrapper.java.additional.11=-Xms100mwrapper.java.additional.12=-XX:+UseParNewGCwrapper.java.additional.13=-XX:+UseConcMarkSweepGCwrapper.java.additional.14=-XX:+UseCMSCompactAtFullCollectionwrapper.java.additional.15=-XX:CMSFullGCsBeforeCompaction=0wrapper.java.additional.16=-XX:CMSInitiatingOccupancyFraction=70以下配置作废：wrapper.java.initmemory=3wrapper.java.maxmemory=64 Mycat连接测试：测试mycat与测试mysql完全一致，mysql怎么连接，mycat就怎么连接。 推荐先采用命令行测试： 123456789101112131415sammac:local Sam$ mysql -uroot -p -P8066 Enter password: Welcome to the MySQL monitor. Commands end with ; or \\g.Your MySQL connection id is 339Server version: 5.5.48-log MySQL Community Server (GPL)Copyright (c) 2000, 2016, Oracle and/or its affiliates. All rights reserved.Oracle is a registered trademark of Oracle Corporation and/or itsaffiliates. Other names may be trademarks of their respectiveowners.Type 'help;' or '\\h' for help. Type '\\c' to clear the current input statement.mysql&gt; 如果采用工具连接，1.4,1.3目前部分工具无法连接，会提示database not selected，建议采用高版本，navicat测试。1.5已经修复了部分工具连接 注 文章部分内容转自Mycat官网","categories":[],"tags":[{"name":"mycat","slug":"mycat","permalink":"https://ydstudios.gitee.io/tags/mycat/"},{"name":"database","slug":"database","permalink":"https://ydstudios.gitee.io/tags/database/"}]},{"title":"实现MySQL主从复制","slug":"实现MySQL主从复制","date":"2017-11-17T22:28:06.000Z","updated":"2020-03-15T06:10:35.000Z","comments":true,"path":"post/34c3deb2.html","link":"","permalink":"https://ydstudios.gitee.io/post/34c3deb2.html","excerpt":"实验准备：在Pc上安装两个MySQL，分别监听3307和3308端口，在centos7上也安装一个MySQL，这样我们就有三个slave！首先我们要保证4个MySQL上的数据一致，并且配置好各个的日志，这样即使出错了，也容易找出错误。","text":"实验准备：在Pc上安装两个MySQL，分别监听3307和3308端口，在centos7上也安装一个MySQL，这样我们就有三个slave！首先我们要保证4个MySQL上的数据一致，并且配置好各个的日志，这样即使出错了，也容易找出错误。 Slave1：MySQL3307 Slave2：MySQL3308 Slave3：Centos7上的MySQL Master: 运行在3306端口的MySQL 修改Master的配置1234567891011[mysqld]server-id=1 //[必须]服务器唯一ID，默认是1，可以随便配置，只要不重复即可binlog-do-db = xxx // 同步的数据库名，如有多个写多行binlog-ignore-db =mysql // 不需要备份的数据库，如有多个写多行read-only =0 // 主机，读写都可以log-bin=mysql-bin //[必须]启用二进制日志 binlog-format =mixed // bin的格式relay-log=mysql-relay // 中继日志文件名expire-logs-days=20 // 日志最长保存时间max_binlog_size = 100M // 日志最大容量slave-skip-errors=all // 跳过所有错误 修改Slave的配置1234567891011121314151617181920#MySQL3307 [mysqld] log-bin=mysql-bin //[不是必须]启用二进制日志 server-id=3307 //[必须]服务器唯一ID，默认是1，可以随便配置，只要不重复即可 replicate-do-db =test // 复制某个库,如有多个写多行 replicate-ignore-db=mysql // 不复制某个库,如有多个写多行#MySQL3308[mysqld] log-bin=mysql-bin //[不是必须]启用二进制日志 server-id=3308 //[必须]服务器唯一ID，默认是1，可以随便配置，只要不重复即可 replicate-do-db =test // 复制某个库,如有多个写多行 replicate-ignore-db=mysql // 不复制某个库,如有多个写多行#Centos7[mysqld] log-bin=mysql-bin //[不是必须]启用二进制日志 server-id=3 //[必须]服务器唯一ID，默认是1，可以随便配置，只要不重复即可 replicate-do-db =test // 复制某个库,如有多个写多行 replicate-ignore-db=mysql // 不复制某个库,如有多个写多行 保存修改后重启4个MySQL服务在Master创建3个MySQL用户1234567grant replication client,replication slave on *.* to slave3307@'%' identified by 'slave3307';grant replication client,replication slave on *.* to slave3308@'%' identified by 'slave3308';grant replication client,replication slave on *.* to centos7@'%' identified by 'centos7'; flush privileges; 登录Master执行命令查看log_pos和log_file123456show master status; +------------------+----------+--------------+------------------+ | File | Position | Binlog_Do_DB | Binlog_Ignore_DB | +------------------+----------+--------------+------------------+ | mysql-bin.000062 | 8233| | | +------------------+----------+--------------+------------------+ 注意 执行完此步骤后不要再操作主服务器MYSQL，防止主服务器状态值变化 配置从服务器Slave，下面的命令分别在3个MySQL上执行：1234567891011121314change master to master_host=&apos;192.168.19.254&apos;,master_port=3306,master_user=&apos;slave3307&apos;,master_password=&apos;slave3307&apos;,master_log_file=&apos;mysql-bin.000062&apos;,master_log_pos=8233;start slave #启动从服务器复制功能change master to master_host=&apos;192.168.19.254&apos;,master_port=3306,master_user=&apos;slave3308&apos;,master_password=&apos;slave3308&apos;,master_log_file=&apos;mysql-bin.000062&apos;,master_log_pos=8233;start slave #启动从服务器复制功能change master to master_host=&apos;192.168.19.254&apos;,master_port=3306,master_user=&apos;centos7&apos;,master_password=&apos;centos7&apos;,master_log_file=&apos;mysql-bin.000087&apos;,master_log_pos=8233;start slave #启动从服务器复制功能 检查Slave复制功能的状态1show slave status\\G 注意 Slave_IO及Slave_SQL进程必须正常运行，即YES状态，否则都是错误的状态(如：其中一个NO或者出现Connection)。有时候是server-id配置错误、防火墙拦截或账户没有权限，仔细检查各个的错误日志，就能很快找到问题所在。 END 以上操作过程，主从服务器配置完成附1234#在Windows上注册MySQL添加服务C:\\phpStudy\\MySQL3307\\bin\\mysqld install mysql3307 --defaults-file=\"C:\\phpStudy\\MySQL3307\\my.ini\"C:\\phpStudy\\MySQL3308\\bin\\mysqld install mysql3308 --defaults-file=\"C:\\phpStudy\\MySQL3308\\my.ini\" MySQL主从复制跳过错误两种方式： 12345678910111.跳过指定数量的事务：mysql&gt;stop slave ;mysql&gt;SET GLOBAL SQL_SLAVE_SKIP_COUNTER = 1; #跳过一个事务mysql&gt;start slave;2.修改mysql的配置文件，通过slave_skip_errors参数来跳所有错误或指定类型的错误vi /etc/my.cnf[mysqld]slave-skip-errors=1062,1053,1146 #跳过指定error no类型的错误slave-skip-errors=all #跳过所有错误 2018年10月3号更新：注意：如果你是直接复制MySQL的安装目录，而你的MySQL的版本又是5.6及以后的版本，那你有可能出现下面的问题： 1Fatal error: The slave I/O thread stops because master and slave have equal MySQL server UUIDs; these UUIDs must be different for replication to work. mysql 5.6的复制引入了uuid的概念，各个复制结构中的server_uuid得保证不一样，但是查看到直接复制data文件夹后server_uuid是相同的 1show variables like &apos;%server_uuid%&apos;; 解决方法：找到data文件夹下的auto.cnf文件，修改里面的uuid值，保证各个db的uuid不一样，重启db即可 附一个正确slave status的信息 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758mysql&gt; show slave status\\G*************************** 1. row *************************** Slave_IO_State: Waiting for master to send event Master_Host: 127.0.0.1 Master_User: slave3307 Master_Port: 3306 Connect_Retry: 60 Master_Log_File: mysql-bin.000007 Read_Master_Log_Pos: 602 Relay_Log_File: mysql-relay.000005 Relay_Log_Pos: 765 Relay_Master_Log_File: mysql-bin.000007 Slave_IO_Running: Yes Slave_SQL_Running: Yes Replicate_Do_DB: Replicate_Ignore_DB: Replicate_Do_Table: Replicate_Ignore_Table: Replicate_Wild_Do_Table: Replicate_Wild_Ignore_Table: Last_Errno: 0 Last_Error: Skip_Counter: 0 Exec_Master_Log_Pos: 602 Relay_Log_Space: 1384 Until_Condition: None Until_Log_File: Until_Log_Pos: 0 Master_SSL_Allowed: No Master_SSL_CA_File: Master_SSL_CA_Path: Master_SSL_Cert: Master_SSL_Cipher: Master_SSL_Key: Seconds_Behind_Master: 0Master_SSL_Verify_Server_Cert: No Last_IO_Errno: 0 Last_IO_Error: Last_SQL_Errno: 0 Last_SQL_Error: Replicate_Ignore_Server_Ids: Master_Server_Id: 1 Master_UUID: fe1b230d-6ee9-11e8-b0c6-9c5c8e103115 Master_Info_File: C:\\phpStudy\\MySQL3307\\data\\master.info SQL_Delay: 0 SQL_Remaining_Delay: NULL Slave_SQL_Running_State: Slave has read all relay log; waiting for the slave I/O thread to update it Master_Retry_Count: 86400 Master_Bind: Last_IO_Error_Timestamp: Last_SQL_Error_Timestamp: Master_SSL_Crl: Master_SSL_Crlpath: Retrieved_Gtid_Set: Executed_Gtid_Set: Auto_Position: 01 row in set (0.00 sec) 2020年03月15号更新: binlog_do_db是指定binlog日志记录那些库的二进制日志。replicate_do_db则在slave库中指定同步那些库的binlog日志。在主从互备环境中，有没有必要每个服务器都同时配置binlog_do_db和replicate_do_db?理由是什么？ binlog-do-db：指定mysql的binlog日志记录哪个dbreplicate_do_db：参数是在slave上配置，指定slave要复制哪个库 在master上设置binlog_do_弊端：1、过滤操作带来的负载都在master上2、无法做基于时间点的复制（利用binlog） 如何删除日志?利用 RESET MASTER 或者 purge reset master ：删除所有的binglog日志文件，并将日志索引文件清空，重新开始所有新的日志文件。用于第一次进行搭建主从库时，进行主库binlog初始化工作 注意reset master 不同于 purge binary log的两处地方 reset master 将删除日志索引文件中记录的所有binlog文件，创建一个新的日志文件起始值从000001 开始，然而purge binary log 命令并不会修改记录binlog的顺序的数值 reset master 不能用于有任何slave 正在运行的主从关系的主库。因为在slave 运行时刻 reset master 命令不被支持，reset master 将master 的binlog从000001 开始记录,slave 记录的master log 则是reset master 时主库的最新的binlog,从库会报错无法找的指定的binlog文件 purge使用事例 123456# 清除mysql-bin.010日志purge master logs to 'mysql-bin.010’; # 清除2016-02-28 13:00:00前的日志purge master logs before '2016-02-28 13:00:00'; # 清除3天前的bin日志purge master logs before date_sub(now(), interval 3 day);","categories":[],"tags":[{"name":"linux","slug":"linux","permalink":"https://ydstudios.gitee.io/tags/linux/"},{"name":"mysql","slug":"mysql","permalink":"https://ydstudios.gitee.io/tags/mysql/"}]},{"title":"Linux Crontab 安装使用详细说明","slug":"Linux-Crontab-安装使用详细说明","date":"2017-11-15T23:28:38.000Z","updated":"2019-12-09T15:03:26.000Z","comments":true,"path":"post/6933ee4e.html","link":"","permalink":"https://ydstudios.gitee.io/post/6933ee4e.html","excerpt":"crontab命令常见于Unix和Linux的操作系统之中，用于设置周期性被执行的指令。该命令从标准输入设备读取指令，并将其存放于“crontab”文件中，以供之后读取和执行。通常，crontab储存的指令被守护进程激活。crond 常常在后台运行，每一分钟检查是否有预定的作业需要执行。这类作业一般称为cron jobs。","text":"crontab命令常见于Unix和Linux的操作系统之中，用于设置周期性被执行的指令。该命令从标准输入设备读取指令，并将其存放于“crontab”文件中，以供之后读取和执行。通常，crontab储存的指令被守护进程激活。crond 常常在后台运行，每一分钟检查是否有预定的作业需要执行。这类作业一般称为cron jobs。 安装12yum -y install vixie-cronyum -y install crontabs 说明：vixie-cron 软件包是 cron 的主程序；crontabs 软件包是用来安装、卸装、或列举用来驱动 cron 守护进程的表格的程序。 配置cron是linux的内置服务，但它不自动起来，可以用以下的方法启动、关闭这个服务：12345service crond start //启动服务service crond stop //关闭服务service crond restart //重启服务service crond reload //重新载入配置service crond status //查看crontab服务状态 在CentOS系统中加入开机自动启动:1chkconfig --level 345 crond on crontab 命令 功能：设置计时器。 语法：crontab[-u &lt;用户名称&gt;][配置文件] 或 crontab [-u &lt;用户名称&gt;][-elr]解释：cron 是一个常驻服务，它提供计时器的功能，让用户在特定的时间得以执行预设的指令或程序。只要用户会编辑计时器的配置文件，就可以使 用计时器的功能。其配置文件格式如下：Minute Hour Day Month DayOFWeek Command 参数：-e 编辑该用户的计时器设置。-l 列出该用户的计时器设置。-r 删除该用户的计时器设置。-u&lt;用户名称&gt; 指定要设定计时器的用户名称。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849格式:* * * * * command分 时 日 月 周 命令第1列表示分钟1～59 每分钟用*或者 */1表示第2列表示小时1～23（0表示0点）第3列表示日期1～31第4列表示月份1～12第5列标识号星期0～6（0表示星期天）第6列要运行的命令``` 例子：```bash*/5 * * * * root ab -n 2000 http://60.217.229.252/250k.jpg上面例子表示每5分钟模拟用户访问http://60.217.229.252/250k.jpg 2000次 30 21 * * * /usr/local/etc/rc.d/lighttpd restart上面的例子表示每晚的21:30重启apache。45 4 1,10,22 * * /usr/local/etc/rc.d/lighttpd restart上面的例子表示每月1、10、22日的4 : 45重启apache。10 1 * * 6,0 /usr/local/etc/rc.d/lighttpd restart上面的例子表示每周六、周日的1 : 10重启apache。0,30 18-23 * * * /usr/local/etc/rc.d/lighttpd restart上面的例子表示在每天18 : 00至23 : 00之间每隔30分钟重启apache。0 23 * * 6 /usr/local/etc/rc.d/lighttpd restart上面的例子表示每星期六的11 : 00 pm重启apache。* */1 * * * /usr/local/etc/rc.d/lighttpd restart每一小时重启apache* 23-7/1 * * * /usr/local/etc/rc.d/lighttpd restart晚上11点到早上7点之间，每隔一小时重启apache0 11 4 * mon-wed /usr/local/etc/rc.d/lighttpd restart每月的4号与每周一到周三的11点重启apache0 4 1 jan * /usr/local/etc/rc.d/lighttpd restart一月一号的4点重启apache*/30 * * * * /usr/sbin/ntpdate 210.72.145.44每半小时同步一下时间","categories":[],"tags":[{"name":"linux","slug":"linux","permalink":"https://ydstudios.gitee.io/tags/linux/"},{"name":"crontab","slug":"crontab","permalink":"https://ydstudios.gitee.io/tags/crontab/"}]},{"title":"PHP目前比较常见的五大运行模式","slug":"PHP目前比较常见的五大运行模式","date":"2017-11-15T23:10:03.000Z","updated":"2019-12-09T15:03:26.000Z","comments":true,"path":"post/fa0692b8.html","link":"","permalink":"https://ydstudios.gitee.io/post/fa0692b8.html","excerpt":"关于PHP目前比较常见的五大运行模式： CGI（通用网关接口 / Common Gateway Interface） FastCGI（常驻型CGI / Long-Live CGI） CLI（命令行运行 / Command Line Interface） Web模块模式（Apache等Web服务器运行的模式） ISAPI（Internet Server Application Program Interface）","text":"关于PHP目前比较常见的五大运行模式： CGI（通用网关接口 / Common Gateway Interface） FastCGI（常驻型CGI / Long-Live CGI） CLI（命令行运行 / Command Line Interface） Web模块模式（Apache等Web服务器运行的模式） ISAPI（Internet Server Application Program Interface） 备注：在PHP5.3以后，PHP不再有ISAPI模式，安装后也不再有php5isapi.dll这个文件。要在IIS6上使用高版本PHP，必须安装FastCGI 扩展，然后使IIS6支持FastCGI。 CGI模式 CGI即通用网关接口（Common Gateway Interface），它是一段程序，通俗的讲CGI就象是一座桥，把网页和Web服务器中的执行程序连接起来，它把HTML接收的指令传递给服务器的执行程序，再把服务器执行程序的结果返还给HTML页。CGI 的跨平台性能极佳，几乎可以在任何操作系统上实现。 CGI已经是比较老的模式了，这几年都很少用了。 每有一个用户请求，都会先要创建CGI的子进程，然后处理请求，处理完后结束这个子进程，这就是Fork-And-Execute模式。 当用户请求数量非常多时，会大量挤占系统的资源如内存，CPU时间等，造成效能低下。所以用CGI方式的服务器有多少连接请求就会有多少CGI子进程，子进程反复加载是CGI性能低下的主要原因。 如果不想把 PHP 嵌入到服务器端软件（如 Apache）作为一个模块安装的话，可以选择以 CGI 的模式安装。或者把 PHP 用于不同的 CGI 封装以便为代码创建安全的 chroot 和 setuid 环境。这样每个客户机请求一个PHP文件，Web服务器就调用php.exe（win下是php.exe,linux是php）去解释这个文件，然后再把解释的结果以网页的形式返回给客户机。 这种安装方式通常会把 PHP 的可执行文件安装到 web 服务器的 cgi-bin 目录。CERT 建议书 CA-96.11 建议不要把任何的解释器放到 cgi-bin 目录。 这种方式的好处是把Web Server和具体的程序处理独立开来，结构清晰，可控性强，同时缺点就是如果在高访问需求的情况下，CGI的进程Fork就会成为很大的服务器负担，想 象一下数百个并发请求导致服务器Fork出数百个进程就明白了。这也是为什么CGI一直背负性能低下，高资源消耗的恶名的原因。 FastCGI模式 FastCGI是CGI的升级版本，FastCGI像是一个常驻 (long-live)型的 CGI，它可以一直执行着，只要激活后，不会每次都要花费时间去 Fork 一次 （这是 CGI 最为人诟病的 fork-and-execute 模式）。 FastCGI是一个可伸缩地、高速地在HTTP server和动态脚本语言间通信的接口。多数流行的HTTP server都支持FastCGI，包括Apache、Nginx和lighttpd等，同时，FastCGI也被许多脚本语言所支持，其中就有PHP。 FastCGI接口方式采用C/S结构，可以将HTTP服务器和脚本解析服务器分开，同时在脚本解析服务器上启动一个或者多个脚本解析守护进程。当HTTP服务器每次遇到动态程序时，可以将其直接交付给FastCGI进程来执行，然后将得到的结果返回给浏览器。这种方式可以让HTTP服务器专一地处理静态请求或者将动态脚本服务器的结果返回给客户端，这在很大程度上提高了整个应用系统的性能。 原理 Web Server启动时载入FastCGI进程管理器（IIS ISAPI或Apache Module)； FastCGI进程管理器自身初始化，启动多个CGI解释器进程 (可见多个php-cgi.exe或php-cig)并等待来自Web Server的连接； 当客户端请求到达Web Server时，FastCGI进程管理器选择并连接到一个CGI解释器。Web server将CGI环境变量和标准输入发送到FastCGI子进程php-cgi； FastCGI子进程完成处理后将标准输出和错误信息从同一连接返回WebServer。当FastCGI子进程关闭连接时，请求便告处理完成。FastCGI子进程接着等待并处理来自FastCGI进程管理器（运行在WebServer中）的下一个连接。在正常的CGI模式中，php-cgi.exe在此便退出了。在CGI模式中，你可以想象CGI通常有多慢。每一个Web请求PHP都必须重新解析php.ini、重新载入全部dll扩展并重初始化全部数据结构。使用FastCGI，所有这些都只在进程启动时发生一次。一个额外的好处是，持续数据库连接（Persistent database connection）可以工作。 备注：PHP的FastCGI进程管理器是PHP-FPM（PHP-FastCGI Process Manager） 优点 从稳定性上看，FastCGI是以独立的进程池来运行CGI，单独一个进程死掉，系统可以很轻易的丢弃，然后重新分配新的进程来运行逻辑； 从安全性上看，FastCGI支持分布式运算。FastCGI和宿主的Server完全独立，FastCGI怎么down也不会把Server搞垮； 从性能上看，FastCGI把动态逻辑的处理从Server中分离出来，大负荷的IO处理还是留给宿主Server，这样宿主Server可以一心一意作IO，对于一个普通的动态网页来说, 逻辑处理可能只有一小部分，大量的是图片等静态。 缺点 说完了好处，也来说说缺点。从我的实际使用来看，用FastCGI模式更适合生产环境的服务器。但对于开发用机器来说就不太合适。因为当使用ZendStudio调试程序时，由于FastCGI会认为PHP进程超时，从而在页面返回500错误。这一点让人非常恼火，所以我在开发机器上还是换回了ISAPI模式。对某些服务器的新版本支持不好，对分布式负载均衡没要求的模块化安装是否是更好的选择。目前的FastCGI和Server沟通还不够智能，一个FastCGI进程如果执行时间过长会被当成是死进程杀掉重起，这样在处理长时间任务的时候很麻烦，这样做也使得FastCGI无法允许联机调试。因为是多进程，所以比CGI多线程消耗更多的服务器内存，PHP-CGI解释器每进程消耗7至25兆内存，将这个数字乘以50或100就是很大的内存数。 CLI模式 PHP-CLI是PHP Command Line Interface的简称，如同它名字的意思，就是PHP在命令行运行的接口，区别于在Web服务器上运行的PHP环境（PHP-CGI，ISAPI等）。 也就是说，PHP不单可以写前台网页，它还可以用来写后台的程序。PHP的CLIShell脚本适用于所有的PHP优势，使创建要么支持脚本或系统甚至与GUI应用程序的服务端，在Windows和Linux下都是支持PHP-CLI模式的。 优点 使用多进程，子进程结束以后，内核会负责回收资源； 使用多进程，子进程异常退出不会导致整个进程Thread退出，父进程还有机会重建流程； 一个常驻主进程，只负责任务分发，逻辑更清楚。我们在Linux下经常使用”php–m”查找PHP安装了那些扩展就是PHP命令行运行模式；有兴趣的同学可以输入”php –h”去深入研究该运行模式。 模块模式 模块模式是以mod_php5模块的形式集成，此时mod_php5模块的作用是接收Apache传递过来的PHP文件请求，并处理这些请求，然后将处理后的结果返回给Apache。如果我们在Apache启动前在其配置文件中配置好了PHP模块（mod_php5）， PHP模块通过注册apache2的ap_hook_post_config挂钩，在Apache启动的时候启动此模块以接受PHP文件的请求。 除了这种启动时的加载方式，Apache的模块可以在运行的时候动态装载，这意味着对服务器可以进行功能扩展而不需要重新对源代码进行编译，甚至根本不需要停止服务器。我们所需要做的仅仅是给服务器发送信号HUP或者AP_SIG_GRACEFUL通知服务器重新载入模块。但是在动态加载之前，我们需要将模块编译成为动态链接库。此时的动态加载就是加载动态链接库。 Apache中对动态链接库的处理是通过模块mod_so来完成的，因此mod_so模块不能被动态加载，它只能被静态编译进Apache的核心。这意味着它是随着Apache一起启动的。 Apache是如何加载模块的呢？我们以前面提到的mod_php5模块为例。首先我们需要在Apache的配置文件httpd.conf中添加一行：LoadModule php5_module modules/mod_php5.so 这里我们使用了LoadModule命令，该命令的第一个参数是模块的名称，名称可以在模块实现的源码中找到。第二个选项是该模块所处的路径。如果需要在服务器运行时加载模块，可以通过发送信号HUP或者AP_SIG_GRACEFUL给服务器，一旦接受到该信号，Apache将重新装载模块，而不需要重新启动服务器。 该运行模式是我们以前在windows环境下使用apache服务器经常使用的，而在模块化（DLL）中，PHP是与Web服务器一起启动并运行的。（它是apache在CGI的基础上进行的一种扩展，加快PHP的运行效率）。 ISAPI模式 ISAPI（Internet Server Application Program Interface）是微软提供的一套面向Internet服务的API接口，一个ISAPI的DLL，可以在被用户请求激活后长驻内存，等待用户的另一个请求，还可以在一个DLL里设置多个用户请求处理函数，此外，ISAPI的DLL应用程序和WWW服务器处于同一个进程中，效率要显著高于CGI。（由于微软的排他性，只能运行于windows环境）PHP作为Apache模块，Apache服务器在系统启动后，预先生成多个进程副本驻留在内存中，一旦有请求出现，就立即使用这些空余的子进程进行处理，这样就不存在生成子进程造成的延迟了。这些服务器副本在处理完一次HTTP请求之后并不立即退出，而是停留在计算机中等待下次请求。对于客户浏览器的请求反应更快，性能较高。","categories":[],"tags":[{"name":"php","slug":"php","permalink":"https://ydstudios.gitee.io/tags/php/"}]},{"title":"解决git默认不区分文件名大小写的问题","slug":"解决git默认不区分文件名大小写的问题","date":"2017-11-13T22:46:50.000Z","updated":"2019-12-09T15:03:26.000Z","comments":true,"path":"post/8598d574.html","link":"","permalink":"https://ydstudios.gitee.io/post/8598d574.html","excerpt":"当你创建一个文件后,叫 readme.md 写入内容后 提交到线上代码仓库.然后你在本地修改文件名为 Readme.md 接着你去提交,发现代码没有变化.","text":"当你创建一个文件后,叫 readme.md 写入内容后 提交到线上代码仓库.然后你在本地修改文件名为 Readme.md 接着你去提交,发现代码没有变化. 1git status 你会无任何提示信息,其实 git 默认对于文件名大小写是不敏感的,所以上面你修改了首字母大写,但是git 并没有发现代码任何改动.那么如何才能让 git 识别文件名大小写变化. 一 配置git 使其对文件名大小写敏感 1git config core.ignorecase false 二 从git 本地仓库删除此文件,然后添加再提交(1) 删除 1git rm readme.md (2) 重新添加 1git add Readme.md (3)提交 1git commit -m 'Readme.md' 这样git不区分文件名大小写的问题就解决了！","categories":[],"tags":[]},{"title":"隐藏Nginx或Apache以及PHP的版本号","slug":"隐藏Nginx或Apache以及PHP的版本号","date":"2017-11-13T22:40:48.000Z","updated":"2019-12-09T15:03:26.000Z","comments":true,"path":"post/adb014f8.html","link":"","permalink":"https://ydstudios.gitee.io/post/adb014f8.html","excerpt":"当你配置完一台服务器后，并不是就可以高枕无忧了，由于软件难免会有漏洞，防止因为漏洞导致一些损失，所以说经常关注安全公告并及时升级服务器也是必要的。一般来说，黑客攻击服务器的首要步骤就是收集信息，比如说你的软件版本，这些将成为下一步有针对性攻击的依据。所以说一定程度的隐藏这些信息就显得非常有必要了，本文将简单介绍如何在网络上隐藏Nginx版本号以及PHP的版本号。","text":"当你配置完一台服务器后，并不是就可以高枕无忧了，由于软件难免会有漏洞，防止因为漏洞导致一些损失，所以说经常关注安全公告并及时升级服务器也是必要的。一般来说，黑客攻击服务器的首要步骤就是收集信息，比如说你的软件版本，这些将成为下一步有针对性攻击的依据。所以说一定程度的隐藏这些信息就显得非常有必要了，本文将简单介绍如何在网络上隐藏Nginx版本号以及PHP的版本号。 我们用命令测试一下，显示服务器上版本信息，命令如下： 1curl -I http://127.0.0.1/phpinfo.php 结果显示了如下内容： 1234567HTTP/1.1 200 OKServer: nginx/1.5.0Date: Thu, 18 Jun 2015 02:39:32 GMTContent-Type: text/htmlConnection: keep-aliveVary: Accept-EncodingX-Powered-By: PHP/5.3.1 可以看到我们的服务器nginx和php版本都暴露了。那该如何隐藏版本号呢？ 1.先说隐藏Nginx版本号，Nginx的版本号主要在两个地方会有，一个是HTTP header，有个Server:nginx/1.x.x类似会暴露Web服务器所用软件名称以及版本号，这个也是大多数Web服务器最容易暴露版本号的地方，第二个地方是Nginx出错页面，比如404页面没有找到等，这是如果用户没有指定页面的话，那么Nginx自己的页面会有版本戳记。 在nginx配置文件中http节的配置,加入以下代码： 1234http &#123; # ...省略一些配置 server_tokens off; &#125; 最后别忘了使用命令nginx -s reload刷新当前配置。完成后你可以查看所有页面的响应头或者错误页，看看是不是只看到nginx字样而看不到版本号？什么？你想连nginx也改掉？呵呵，这个恐怕就麻烦了，需要改动Nginx源代码然后重新编译，感兴趣的童鞋可以研究下！在apache配置文件httpd.conf中,加入以下代码： 12ServerTokens ProdServerSignature Off 记得保存后重启一下apache! 2.隐藏PHP的版本号，PHP容易暴露的版本号在什么地方呢？其实也是在HTTP头，以类似X-Powered-By: PHP/5.2.11这种形式存在。 其实这个是在PHP的配置文件php.ini里改动，打开php.ini，找到下面叙述： 123456; Decides whether PHP may expose the fact that it is installed on the server; (e.g. by adding its signature to the Web server header). It is no security; threat in any way, but it makes it possible to determine whether you use PHP; on your server or not.; http://php.net/expose-phpexpose_php = On expose_php = On改为expose_php = Off就搞定了，当然，对于apache服务器还有另外一个方法可以直接尝试在.htaccess文件中Header unset X-Powered-By，删除X-Powered-By节，不过我还是建议改动php.ini的expose_php。","categories":[],"tags":[]},{"title":"如何正确的修改Python的pip自带的源","slug":"如何正确的修改Python的pip自带的源","date":"2017-11-13T22:06:56.000Z","updated":"2019-12-09T15:03:26.000Z","comments":true,"path":"post/cf179e51.html","link":"","permalink":"https://ydstudios.gitee.io/post/cf179e51.html","excerpt":"Python中的pip安装依赖包时默认访问https://pypi.python.org/simple/，但是经常出现不稳定以及访问速度非常慢的情况，国内厂商提供的pipy镜像目前可用的有： 1.http://pypi.douban.com/ 豆瓣2.http://pypi.mirrors.ustc.edu.cn/simple/ 中国科学技术大学","text":"Python中的pip安装依赖包时默认访问https://pypi.python.org/simple/，但是经常出现不稳定以及访问速度非常慢的情况，国内厂商提供的pipy镜像目前可用的有： 1.http://pypi.douban.com/ 豆瓣2.http://pypi.mirrors.ustc.edu.cn/simple/ 中国科学技术大学 有两种方式使用我们自己指定的镜像源，第一种是手动指定： 1pip -i http://pypi.douban.com/simple install Flask -- trusted-host pypi.douban.com 不过这种方式在每次安装时都要手动指定，所以第二种方法更加方便，在当前用户目录下创建.pip文件夹中，代码如下： 1234567891011121314import osini=\"\"\"[global]index-url = https://pypi.doubanio.com/simple/[install]trusted-host=pypi.doubanio.com\"\"\"pippath=os.environ[\"USERPROFILE\"]+\"\\\\pip\\\\\"if not os.path.exists(pippath): os.mkdir(pippath)with open(pippath+\"pip.ini\",\"w+\") as f: f.write(ini) 个人觉得第二种方法最佳，一次执行终身无忧！","categories":[],"tags":[]},{"title":"Linux文件时间详解 ctime、mtime和atime","slug":"Linux文件时间详解-ctime、mtime和atime","date":"2017-11-10T23:01:09.000Z","updated":"2019-12-09T15:03:26.000Z","comments":true,"path":"post/6652f017.html","link":"","permalink":"https://ydstudios.gitee.io/post/6652f017.html","excerpt":"Linux系统文件有三个主要的时间属性，分别是 ctime(change time), atime(access time), mtime(modify time)。这三个时间很容易混淆，准备深入了解Linux的童鞋请区分这三者的区别。","text":"Linux系统文件有三个主要的时间属性，分别是 ctime(change time), atime(access time), mtime(modify time)。这三个时间很容易混淆，准备深入了解Linux的童鞋请区分这三者的区别。 atime：Access time，是在读取文件或者执行文件时更改，即文件最后一次被读取的时间。说明： st_atime 12Time when file data was last accessed. Changed by the following functions: creat(), mknod(), pipe(), utime(2), and read(2). mtime：Modified time，是在写入文件时随文件内容的更改而更改，是指文件内容最后一次被修改的时间。说明： st_mtime 12Time when data was last modified. Changed by the following functions: creat(), mknod(), pipe(), utime(), and write(2). ctime：Change time，是在写入文件、更改所有者、权限或链接设置时随 Inode 的内容更改而更改，即文件状态最后一次被改变的时间。说明： st_ctime 12Time when file status was last changed. Changed by the following functions: chmod(), chown(), creat(), link(2), mknod(), pipe(), unlink(2), utime(), and write(). 很多人把它理解成create time，包括很多误导人的书籍也是这么写。实际上ctime是指change time。注意：1、修改是文本本身的内容发生了变化（mtime）改变是文件的索引节点发生了改变（ctime）2、如果修改了文件内容，则同时更新ctime和mtime3、如果只改变了文件索引节点，比如修改权限，则只是改变了ctime4、如果使用ext3文件系统的时候，在mount的时候使用了noatime参数则不会更新atime的信息，即访问文件之后atime不会被修改，而这个不代表真实情况小知识：这三个 time stamp 都放在 inode 中。若mtime,atime修改, inode 就一定会改,相应的inode改了,那ctime 也就跟着要改了，之所以在mount option中使用 noatime, 就是不想 file system 做太多的修改, 从而改善读取性能.查看文件的 atime、ctime 和 mtime。 12345678910# ls -lc filename 列出文件的 ctime# ls -lu filename 列出文件的 atime# ls -l filename 列出文件的 mtime#例子1： # echo \"Hello World\" &gt;&gt; myfile atime不变,同时改变了ctime和mtime2： # cat myfile ctime和mtime不变，只改变atime# ls myfilectime和mtime以及atime都不改变3： # chmod u+x myfile mtime和atime不变，只改变ctime4： # mv myfile ../ mtime和atime不变，只改变ctime 其他扩展：relatime属性从kernel2.6.29开始，默认集成了一个 relatime的属性。使用这个特性来挂装文件系统后，只有当mtime比atime更新的时候，才会更新atime。使用场景：在文件读操作很频繁的系统中，atime更新所带来的开销很大，所以在挂装文件系统的时候使用noatime属性来停止更新atime。但是有些程序需要根据atime进行一些判断和操作，这个时候relatime特性就派上用场了。其实在事实上，这个时候atime和mtime已经是同一个time，所以可以理解这个选项就是为了实现对atime的兼容才推出的，并不是一个新的时间属性。使用方法： 1# mount -o relatime /dir##挂载目录的时候加relatime参数","categories":[],"tags":[{"name":"linux","slug":"linux","permalink":"https://ydstudios.gitee.io/tags/linux/"}]},{"title":"apache出现no input file specified.的完美解决方案","slug":"apache出现no-input-file-specified-的完美解决方案","date":"2017-11-10T22:53:58.000Z","updated":"2019-12-09T15:03:26.000Z","comments":true,"path":"post/ff626742.html","link":"","permalink":"https://ydstudios.gitee.io/post/ff626742.html","excerpt":"启用rewrite的伪静态功能的时候，首页可以访问，而访问内页的时候，就提示：“No input file specified.”原因在于使用的PHP是fast_cgi模式，而在某些情况下，不能正确识别path_info所造成的错误，Wordpress的伪静态也有一样的问题。","text":"启用rewrite的伪静态功能的时候，首页可以访问，而访问内页的时候，就提示：“No input file specified.”原因在于使用的PHP是fast_cgi模式，而在某些情况下，不能正确识别path_info所造成的错误，Wordpress的伪静态也有一样的问题。 Wordpress程序默认的.htaccess里面的规则： 12345RewriteEngine OnRewriteBase /RewriteCond %&#123;REQUEST_FILENAME&#125; !-fRewriteCond %&#123;REQUEST_FILENAME&#125; !-dRewriteRule ^(.*)$ /index.php/$1 [L] 其实“No input file specified.”，是没有得到有效的文件路径造成的。修改伪静态规则，如下： 12345RewriteEngine OnRewriteBase /RewriteCond %&#123;REQUEST_FILENAME&#125; !-fRewriteCond %&#123;REQUEST_FILENAME&#125; !-dRewriteRule ^(.*)$ /index.php?/$1 [L] 有没有发现不同？其实就是在正则结果“/$1”前面多加了一个“?”号，问题也就随之解决了。","categories":[],"tags":[{"name":"nginx","slug":"nginx","permalink":"https://ydstudios.gitee.io/tags/nginx/"}]},{"title":"git pull操作失败的问题","slug":"git-pull操作失败的问题","date":"2017-11-10T22:53:13.000Z","updated":"2019-12-09T15:03:26.000Z","comments":true,"path":"post/a774f745.html","link":"","permalink":"https://ydstudios.gitee.io/post/a774f745.html","excerpt":"使用git版本控制，提交代码到测试环境，发现代码没有更新，直接在服务器上用命令行操作的时候出现下面的错误：","text":"使用git版本控制，提交代码到测试环境，发现代码没有更新，直接在服务器上用命令行操作的时候出现下面的错误： 123456789101112www@iZ23oqentptZ:~/develop.dameiweb.com/webroot$ git pullerror: Cannot update the ref 'refs/remotes/origin/develop': unable to append to .git/logs/refs/remotes/origin/develop: Permission deniedFrom git.dameiweb.com:dm/www.dameiweb.com ! 3fa3fb3..297aa33 develop -&gt; origin/develop (unable to update local ref)error: Cannot update the ref 'refs/remotes/origin/master': unable to append to .git/logs/refs/remotes/origin/master: Permission denied ! 0eadc3f..d8ad757 master -&gt; origin/master (unable to update local ref) 通过查看文件的权限，发现.git文件夹下文件的权限不对，有的文件不属于git账号（git的用户是www），出现这个情况可能是用不同的账号去执行git命令，譬如我们这就是使用root账号操作过。 解决办法就是用root账户修改.git文件的权限,.git文件的所有者和所属组都是www,于是执行命令： 12#文件的所有者和所属组修改成wwwchown -R www:www .git 执行完命令之后，查看文件的权限修改成功后，再切换成www用户，执行git pull，更新成功！ 大家切记，linux下权限很重要，不用乱用不用的用户，这样会导致文件的所有者和所属组发生改变！","categories":[],"tags":[]},{"title":"git的.gitignore规则不生效的解决办法","slug":"git的-gitignore规则不生效的解决办法","date":"2017-11-10T22:51:42.000Z","updated":"2019-12-09T15:03:26.000Z","comments":true,"path":"post/8b084705.html","link":"","permalink":"https://ydstudios.gitee.io/post/8b084705.html","excerpt":"在git中如果想忽略掉某个文件，不让这个文件提交到版本库中，可以使用修改根目录中 .gitignore 文件的方法（如无，则需自己手工建立此文件）。这个文件每一行保存了一个匹配的规则例如：","text":"在git中如果想忽略掉某个文件，不让这个文件提交到版本库中，可以使用修改根目录中 .gitignore 文件的方法（如无，则需自己手工建立此文件）。这个文件每一行保存了一个匹配的规则例如： 123456# 此为注释 – 将被 Git 忽略*.a # 忽略所有 .a 结尾的文件!lib.a # 但 lib.a 除外/TODO # 仅仅忽略项目根目录下的 TODO 文件，不包括 subdir/TODObuild/ # 忽略 build/ 目录下的所有文件doc/*.txt # 会忽略 doc/notes.txt 但不包括 doc/server/arch.txt 规则很简单，不做过多解释，但是有时候在项目开发过程中，突然心血来潮想把某些目录或文件加入忽略规则，按照上述方法定义后发现并未生效，原因是.gitignore只能忽略那些原来没有被track的文件，如果某些文件已经被纳入了版本管理中，则修改.gitignore是无效的。那么解决方法就是先把本地缓存删除（改变成未track状态），然后再提交： 123git rm -r --cached .git add .git commit -m 'update .gitignore' 这样git的忽略规则不生效的问题就解决了！","categories":[],"tags":[]}]}